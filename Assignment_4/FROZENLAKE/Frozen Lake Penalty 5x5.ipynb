{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "from tqdm import tqdm\n",
    "from bettermdptools.algorithms.planner import Planner\n",
    "from bettermdptools.utils.plots import Plots\n",
    "from bettermdptools.algorithms.rl import RL\n",
    "from bettermdptools.utils.test_env import TestEnv\n",
    "from bettermdptools.utils.grid_search import GridSearch\n",
    "from bettermdptools.utils.blackjack_wrapper import BlackjackWrapper\n",
    "import time\n",
    "import csv\n",
    "import pygame\n",
    "import itertools\n",
    "import gc\n",
    "from scipy import stats\n",
    "\n",
    "gc.collect();\n",
    "#from A4_Utils import mode_policy_dict,visualize_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting a large frozen lake for an MDP\n",
    "size_val=5\n",
    "seed=42\n",
    "frozen_lake = gym.make('FrozenLake-v1', desc=generate_random_map(size=size_val) ,is_slippery=True, render_mode=\"rgb_array\", negative_reward_hole=True)\n",
    "frozen_lake.reset(seed=seed);\n",
    "plt.imshow(frozen_lake.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('saved_data/frozenlake_5.npy',frozen_lake.desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_policy_dict(policies,gammas):\n",
    "    '''Function to compute optimal policy for each gamma value using mode'''\n",
    "    mode_policies=np.zeros(shape=(gammas.shape[0],len(policies[list(policies.keys())[0]])))\n",
    "    for i,g in enumerate(gammas):\n",
    "        master_key='{:.2f}_'.format(g)\n",
    "        big_array=None\n",
    "        short_listed_keys=[]\n",
    "        for k in policies.keys():\n",
    "            if master_key in k:\n",
    "                short_listed_keys.append(k)\n",
    "            else:\n",
    "                pass\n",
    "        for ii,s_k in enumerate(short_listed_keys):\n",
    "            if ii==0:\n",
    "                big_array=np.array(list(policies[s_k].values()))\n",
    "            else:\n",
    "                big_array=np.c_[big_array,np.array(list(policies[s_k].values()))]\n",
    "            \n",
    "        #Getting Mode\n",
    "        mode_policies[i]=stats.mode(big_array,axis=1)[0].ravel()\n",
    "    return mode_policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(policies,env,gammas,title_header='None',row=3,cols=5, axes_in=None):\n",
    "    #env=[v.decode('ascii') for v in env]\n",
    "    env=env.ravel()\n",
    "    arrow_colors = {\n",
    "    0: 'blue',\n",
    "    1: 'black',\n",
    "    2: 'red',\n",
    "    3: 'green'\n",
    "    }\n",
    "\n",
    "    grid_colors={\n",
    "        b'H': 'red',\n",
    "        b'G': 'green',\n",
    "        b'F': 'grey',\n",
    "        b'S': 'grey'\n",
    "    }\n",
    "\n",
    "    if axes_in is None:\n",
    "        fig, axes = plt.subplots(row,cols)\n",
    "        fig.set_size_inches(10*cols,5*row)\n",
    "        plt.suptitle(f'{title_header} Policies Plotted',fontsize=18)\n",
    "        for i,ax in enumerate(axes.flatten()):\n",
    "            policy=policies[i]\n",
    "            ax.set_title('Gamma: {:.2f}'.format(gammas[i]))\n",
    "            ax.set_xticks(np.arange(policy.shape[0]**(0.5)))\n",
    "            ax.set_yticks(-1*np.arange(policy.shape[0]**(0.5)))\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.grid(True)\n",
    "\n",
    "            for ii in range(len(policy)):\n",
    "                if policy[ii]==2:\n",
    "                    arrow = '→'\n",
    "                elif policy[ii]==0:\n",
    "                    arrow = '←'\n",
    "                elif policy[ii]==3:\n",
    "                    arrow = '↑'\n",
    "                elif policy[ii]==1:\n",
    "                    arrow = '↓'\n",
    "                else:\n",
    "                    continue\n",
    "                x=int(ii%policy.shape[0]**(0.5))\n",
    "                y=-int(ii/policy.shape[0]**(0.5))\n",
    "                rect = plt.Rectangle((x, y), 1, 1, color=grid_colors[env[ii]], alpha=0.1)\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x,y,arrow, ha='center', va='center', color=arrow_colors[policy[ii]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        for i,ax in enumerate(axes_in.flatten()):\n",
    "            policy=policies[i]\n",
    "            ax.set_title('Policy Visualized')\n",
    "            ax.set_xticks(np.arange(policy.shape[0]**(0.5)))\n",
    "            ax.set_yticks(-1*np.arange(policy.shape[0]**(0.5)))\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.grid(True)\n",
    "\n",
    "            for ii in range(len(policy)):\n",
    "                if policy[ii]==2:\n",
    "                    arrow = '→'\n",
    "                elif policy[ii]==0:\n",
    "                    arrow = '←'\n",
    "                elif policy[ii]==3:\n",
    "                    arrow = '↑'\n",
    "                elif policy[ii]==1:\n",
    "                    arrow = '↓'\n",
    "                else:\n",
    "                    continue\n",
    "                x=int(ii%policy.shape[0]**(0.5))\n",
    "                y=-int(ii/policy.shape[0]**(0.5))\n",
    "                rect = plt.Rectangle((x, y), 1, 1, color=grid_colors[env[ii]], alpha=0.1)\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x,y,arrow, ha='center', va='center', color=arrow_colors[policy[ii]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_heatmap_states(values,gammas,title_header='None',row=3,cols=5):\n",
    "    #env=[v.decode('ascii') for v in env]\n",
    "    fig, axes = plt.subplots(row,cols)\n",
    "    fig.set_size_inches(10*cols,5*row)\n",
    "    plt.suptitle(f'{title_header} Policies Plotted',fontsize=18)\n",
    "\n",
    "    for i,ax in enumerate(axes.flatten()):\n",
    "        value=values[i,:]\n",
    "        ax.set_title('Gamma: {:.2f}'.format(gammas[i]))\n",
    "        ax.set_xticks(np.arange(values.shape[0]**(0.5)))\n",
    "        ax.set_yticks(-1*np.arange(values.shape[0]**(0.5)))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        heatmap=ax.imshow(value.reshape(int(len(value)**(0.5)),int(len(value)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "        plt.colorbar(heatmap, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing gamms\n",
    "gammas=np.arange(0.57,1.0,0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the effect of convergence rate for the same convergence value for frozen lakes\n",
    "REPEATS=5\n",
    "iters=5000\n",
    "convergence_policy_iterations=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "times_policy_iterations=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "v_policy_iterations=np.zeros(shape=(len(gammas),REPEATS,size_val**2))\n",
    "vi_policy_all_iterations=np.zeros(shape=(len(gammas),REPEATS,iters,size_val**2))\n",
    "pi_policy_iteraetions={}\n",
    "\n",
    "for big_i,gamma in tqdm(enumerate(gammas)):\n",
    "    for i in range(REPEATS):\n",
    "        frozen_lake.reset()\n",
    "        \n",
    "        #Solving\n",
    "        start=time.time()\n",
    "        V, V_track, pi, convergence_i = Planner(frozen_lake.P).policy_iteration(gamma=gamma,n_iters=iters)\n",
    "        end=time.time()\n",
    "\n",
    "        #Storing Time\n",
    "        times_policy_iterations[big_i,i]=end-start\n",
    "\n",
    "        #Getting iterations\n",
    "        convergence_policy_iterations[big_i,i]=convergence_i\n",
    "\n",
    "        #Storing policy\n",
    "        pi_policy_iteraetions['{:.2f}_{}'.format(gamma,i)]=pi\n",
    "\n",
    "        #Stroing ideal V\n",
    "        v_policy_iterations[big_i,i]=V\n",
    "\n",
    "        #Storing V historic\n",
    "        V_track[np.argwhere(V_track.sum(axis=-1)==0)[1][0]:]=V_track[np.argwhere(V_track.sum(axis=-1)==0)[1][0]-1]\n",
    "        vi_policy_all_iterations[big_i,i]=V_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the effect of convergence rate for the same convergence value for frozen lakes\n",
    "REPEATS=5\n",
    "iters=5000\n",
    "convergence_value_iterations=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "times_value_iterations=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "v_value_iterations=np.zeros(shape=(len(gammas),REPEATS,size_val**2))\n",
    "vi_value_all_iterations=np.zeros(shape=(len(gammas),REPEATS,iters,size_val**2))\n",
    "pi_value_iteraetions={}\n",
    "\n",
    "for big_i,gamma in tqdm(enumerate(gammas)):\n",
    "    for i in range(REPEATS):\n",
    "        frozen_lake.reset()\n",
    "        \n",
    "        #Solving\n",
    "        start=time.time()\n",
    "        V, V_track, pi, convergence_i = Planner(frozen_lake.P).value_iteration(gamma=gamma,n_iters=iters)\n",
    "        end=time.time()\n",
    "\n",
    "        #Storing Time\n",
    "        times_value_iterations[big_i,i]=end-start\n",
    "\n",
    "        #Getting iterations\n",
    "        convergence_value_iterations[big_i,i]=convergence_i\n",
    "\n",
    "        #Storing policy\n",
    "        pi_value_iteraetions['{:.2f}_{}'.format(gamma,i)]=pi\n",
    "\n",
    "        #Stroing ideal V\n",
    "        v_value_iterations[big_i,i]=V\n",
    "\n",
    "        #Storing V historic\n",
    "        V_track[np.argwhere(V_track.sum(axis=-1)==0)[1][0]:]=V_track[np.argwhere(V_track.sum(axis=-1)==0)[1][0]-1]\n",
    "        vi_value_all_iterations[big_i,i]=V_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot number of iterations for policy iteration\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Gamma Value & Iterations Until Convergence')\n",
    "#plt.scatter(gammas,convergence_policy_iterations.mean(axis=1),label='Policy Iteration',color='blue')\n",
    "plt.plot(gammas,convergence_policy_iterations.mean(axis=1),label='Policy Iteration',color='blue')\n",
    "plt.fill_between(gammas, convergence_policy_iterations.mean(axis=1) - convergence_policy_iterations.std(axis=1), convergence_policy_iterations.mean(axis=1) + convergence_policy_iterations.std(axis=1), color='blue', alpha=0.2)\n",
    "#plt.scatter(gammas,convergence_value_iterations.mean(axis=1),label='Value Iteration',color='orange')\n",
    "plt.plot(gammas,convergence_value_iterations.mean(axis=1),label='Value Iteration',color='orange')\n",
    "plt.fill_between(gammas, convergence_value_iterations.mean(axis=1) - convergence_value_iterations.std(axis=1), convergence_value_iterations.mean(axis=1) + convergence_value_iterations.std(axis=1), color='orange', alpha=0.2)\n",
    "#plt.plot(gammas,convergence_policy_iterations.mean(axis=1))\n",
    "#plt.plot(gammas,convergence_value_iterations.mean(axis=1))\n",
    "plt.legend()\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Number of Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot number Time for Value iteration\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Gamma Value & Convergence Time')\n",
    "#plt.scatter(gammas,times_policy_iterations.mean(axis=1),label='Policy Iteration',color='blue')\n",
    "plt.plot(gammas,times_policy_iterations.mean(axis=1),label='Policy Iteration',color='blue')\n",
    "plt.fill_between(gammas, times_policy_iterations.mean(axis=1) - times_policy_iterations.std(axis=1), times_policy_iterations.mean(axis=1) + times_policy_iterations.std(axis=1), color='blue', alpha=0.2)\n",
    "#plt.scatter(gammas,times_value_iterations.mean(axis=1),label='Value Iteration',color='orange')\n",
    "plt.plot(gammas,times_value_iterations.mean(axis=1),label='Value Iteration',color='orange')\n",
    "plt.fill_between(gammas, times_value_iterations.mean(axis=1) - times_value_iterations.std(axis=1), times_value_iterations.mean(axis=1) + times_value_iterations.std(axis=1), color='orange', alpha=0.2)\n",
    "#plt.plot(gammas,times_policy_iterations.mean(axis=1),color='blue')\n",
    "#plt.plot(gammas,times_value_iterations.mean(axis=1),color='orange')\n",
    "plt.legend()\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ting V_mean by gamma for policy iteration\n",
    "cufoff=150\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Number of Iteration & V_mean for Policy Iteration by Gamma')\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    #plt.scatter(np.arange(cufoff),vi_policy_all_iterations.mean(axis=1).mean(axis=-1)[i,:cufoff],label='{:.2f}'.format(gammas[i]))\n",
    "    plt.plot(np.arange(cufoff),vi_policy_all_iterations.mean(axis=1).mean(axis=-1)[i,:cufoff],label='{:.2f}'.format(gammas[i]))\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Value Matrix Mean Value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ting V_mean by gamma for value iteration\n",
    "cufoff=500\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Number of Iteration & V_mean for Value Iteration by Gamma')\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    #plt.scatter(np.arange(cufoff),vi_policy_all_iterations.mean(axis=1).mean(axis=-1)[i,:cufoff],label='{:.2f}'.format(gammas[i]))\n",
    "    plt.plot(np.arange(cufoff),vi_value_all_iterations.mean(axis=1).mean(axis=-1)[i,:cufoff],label='{:.2f}'.format(gammas[i]))\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Value Matrix Mean Value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sucess Rate for each ideal policy for each gamma policy iteration\n",
    "attempts=20\n",
    "win_rate_policy=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "reward_rate_policy=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "num_steps_won_policy=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "parent_dict=pi_policy_iteraetions\n",
    "\n",
    "for ig,gamma in tqdm(enumerate(gammas)):\n",
    "    for ir,repeat in enumerate(range(REPEATS)):\n",
    "        key='{:.2f}_{}'.format(gamma,repeat)\n",
    "        chosen_dict=parent_dict[key]\n",
    "        won_counter=0\n",
    "        reward=0\n",
    "        steps=0\n",
    "        for i in range(attempts):\n",
    "            #Resetting the lake\n",
    "            frozen_lake.reset()\n",
    "            checker=0\n",
    "            ended=False\n",
    "            inner_steps=0\n",
    "            while not ended:\n",
    "                if checker==0:\n",
    "                    checker+=1\n",
    "                    state,r,ended,won,_=frozen_lake.step(chosen_dict[0])\n",
    "                else:\n",
    "                    state,r,ended,won,_=frozen_lake.step(chosen_dict[state])\n",
    "                reward+=r\n",
    "                inner_steps+=1\n",
    "            if float(r>1):\n",
    "                steps+=inner_steps    \n",
    "            won_counter+=float(r>1)\n",
    "        if won_counter==0:\n",
    "            num_steps_won_policy[ig,ir]=np.inf\n",
    "        else:\n",
    "            num_steps_won_policy[ig,ir]=steps/won_counter\n",
    "        win_rate_policy[ig,ir]=won_counter/attempts\n",
    "        reward_rate_policy[ig,ir]=reward/attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sucess Rate for each ideal policy for each gamma value iteration\n",
    "attempts=20\n",
    "win_rate_value=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "reward_rate_value=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "parenta_dict=pi_value_iteraetions\n",
    "num_steps_won_value=np.zeros(shape=(gammas.shape[0],REPEATS))\n",
    "\n",
    "for ig,gamma in tqdm(enumerate(gammas)):\n",
    "    for ir,repeat in enumerate(range(REPEATS)):\n",
    "        key='{:.2f}_{}'.format(gamma,repeat)\n",
    "        chosen_dict=parent_dict[key]\n",
    "        won_counter=0\n",
    "        reward=0\n",
    "        steps=0\n",
    "        for i in range(attempts):\n",
    "            #Resetting the lake\n",
    "            frozen_lake.reset()\n",
    "            checker=0\n",
    "            ended=False\n",
    "            inner_steps=0\n",
    "            while not ended:\n",
    "                if checker==0:\n",
    "                    checker+=1\n",
    "                    state,r,ended,won,_=frozen_lake.step(chosen_dict[0])\n",
    "                else:\n",
    "                    state,r,ended,won,_=frozen_lake.step(chosen_dict[state])\n",
    "                reward+=r\n",
    "                inner_steps+=1\n",
    "            if float(r>1):\n",
    "                steps+=inner_steps    \n",
    "            won_counter+=float(r>1)\n",
    "        if won_counter==0:\n",
    "            num_steps_won_value[ig,ir]=np.inf\n",
    "        else:\n",
    "            num_steps_won_value[ig,ir]=steps/won_counter\n",
    "        win_rate_value[ig,ir]=won_counter/attempts\n",
    "        reward_rate_value[ig,ir]=reward/attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting win rate, average score and number of steps\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Reward Rate & Gamma Value')\n",
    "#plt.scatter(gammas,reward_rate_policy.mean(axis=1),label='Policy Iteration')\n",
    "#plt.scatter(gammas,reward_rate_value.mean(axis=1),label='Value Iteration')\n",
    "plt.plot(gammas,reward_rate_policy.mean(axis=1),label='Policy Iteration',color='blue')\n",
    "plt.fill_between(gammas, reward_rate_policy.mean(axis=1) - reward_rate_policy.std(axis=1), reward_rate_policy.mean(axis=1) + reward_rate_policy.std(axis=1), color='blue', alpha=0.2)\n",
    "plt.plot(gammas,reward_rate_value.mean(axis=1),label='Value Iteration',color='orange')\n",
    "plt.fill_between(gammas, reward_rate_value.mean(axis=1) - reward_rate_value.std(axis=1), reward_rate_value.mean(axis=1) + reward_rate_value.std(axis=1), color='orange', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Win Rate & Gamma Value')\n",
    "#plt.scatter(gammas,win_rate_policy.mean(axis=1),label='Policy Iteration')\n",
    "#plt.scatter(gammas,win_rate_value.mean(axis=1),label='Value Iteration')\n",
    "plt.plot(gammas,win_rate_policy.mean(axis=1),label='Policy Iteration',color='blue')\n",
    "plt.fill_between(gammas, win_rate_policy.mean(axis=1) - win_rate_policy.std(axis=1), win_rate_policy.mean(axis=1) + win_rate_policy.std(axis=1), color='blue', alpha=0.2)\n",
    "plt.plot(gammas,win_rate_value.mean(axis=1),label='Value Iteration',color='orange')\n",
    "plt.fill_between(gammas, win_rate_value.mean(axis=1) - win_rate_value.std(axis=1), win_rate_value.mean(axis=1) + win_rate_value.std(axis=1), color='orange', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Win Rate')\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Average Num of Steps for Win & Gamma Value')\n",
    "# plt.scatter(gammas,num_steps_won_policy.mean(axis=1),label='Policy Iteration')\n",
    "# plt.scatter(gammas,num_steps_won_value.mean(axis=1),label='Value Iteration')\n",
    "plt.plot(gammas,num_steps_won_policy.mean(axis=1),label='Policy Iteration',color='blue')\n",
    "plt.fill_between(gammas, num_steps_won_policy.mean(axis=1) - num_steps_won_policy.std(axis=1), num_steps_won_policy.mean(axis=1) + num_steps_won_policy.std(axis=1), color='blue', alpha=0.2)\n",
    "plt.plot(gammas,num_steps_won_value.mean(axis=1),label='Value Iteration',color='orange')\n",
    "plt.fill_between(gammas, num_steps_won_value.mean(axis=1) - num_steps_won_value.std(axis=1), num_steps_won_value.mean(axis=1) + num_steps_won_value.std(axis=1), color='orange', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Average Number of Steps for Win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_policy(policy,env):\n",
    "    # Visualize policy\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((512, 512))\n",
    "    clock = pygame.time.Clock()\n",
    "    env.reset()\n",
    "    # Main loop\n",
    "    running = True\n",
    "    i=0\n",
    "    while running:\n",
    "        # Handle events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "        # Perform a random action\n",
    "        if i==0:\n",
    "            state,_,ended,won,_=env.step(policy[0])\n",
    "            i+=1\n",
    "        else:\n",
    "            state,_,ended,won,_=env.step(policy[state])\n",
    "\n",
    "        # Render the environment\n",
    "        if hasattr(env, 'render'):\n",
    "            # Use the Gym's built-in rendering method\n",
    "            env.render()\n",
    "            img = env.render()\n",
    "            img = np.transpose(img, (1, 0, 2))  # Transpose to match Pygame's display format\n",
    "            pygame.surfarray.blit_array(screen, img)\n",
    "            pygame.display.flip()\n",
    "        \n",
    "        if ended:\n",
    "            time.sleep(0.5)\n",
    "            running = False\n",
    "\n",
    "        clock.tick(60)\n",
    "\n",
    "    # Clean up\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing magnitude of differences between value iteration and policy iteration\n",
    "print(f'The normalized difference between both policies for all gammas and all runs is {np.linalg.norm(v_policy_iterations-v_value_iterations)} indicating convergence point was the same/similar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing policies for different gamms for Policy and Value Iteration\n",
    "mod_policy_pi=mode_policy_dict(pi_policy_iteraetions,gammas)\n",
    "mod_policy_val=mode_policy_dict(pi_value_iteraetions,gammas)\n",
    "\n",
    "#Plotting\n",
    "visualize_policy(mod_policy_pi,frozen_lake.desc,gammas,title_header='Policy Iteration')\n",
    "visualize_policy(mod_policy_val,frozen_lake.desc,gammas,title_header='Value Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_heatmap_states(v_policy_iterations.mean(axis=1),gammas,'Policy Iteration')\n",
    "visualize_heatmap_states(v_value_iterations.mean(axis=1),gammas,'Value Iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Q Learning grid search\n",
    "gammas_grid=[0.99]\n",
    "gammas_grid=np.r_[gammas_grid,np.array([0.999])]\n",
    "\n",
    "epsilon_decays=[0.99] \n",
    "iters=[int(1e5)]\n",
    "#iters=[int(100000)]\n",
    "env=frozen_lake\n",
    "average_eval_rewards=[]\n",
    "average_win_steps_results=[]\n",
    "keys=[]\n",
    "FILE_SAVE='saved_data/frozen_lake_gd_run_big/'\n",
    "\n",
    "for i in itertools.product(gammas_grid, epsilon_decays, iters):\n",
    "\n",
    "    print(\"running q_learning with gamma:\", i[0],  \"epsilon decay:\", i[1],  \" iterations:\", i[2])\n",
    "    Q, V, pi, Q_track, pi_track, total_reward, states_counter = RL(env).q_learning(gamma=i[0], epsilon_decay_ratio=i[1], n_episodes=i[2], min_alpha=0.1)\n",
    "\n",
    "    #summed_Q=Q_track.sum(axis=1).sum(axis=-1)\n",
    "    #ind_cross=np.argwhere(summed_Q==0)[0][0]\n",
    "    #Q_track[ind_cross:]=Q_track[np.argwhere(summed_Q==0)[0][0]-1]\n",
    "    #total_reward[ind_cross:]=total_reward[np.argwhere(total_reward==0)[0][0]-1]\n",
    "\n",
    "    episode_rewards, average_win_steps, states_counter_app = TestEnv.test_env(env=env, n_iters=100, pi=pi)\n",
    "\n",
    "    #Plotting rewards and Q learning visualizations + Value heat map per run\n",
    "    row=1\n",
    "    cols=6\n",
    "    fig, axes = plt.subplots(row,cols)\n",
    "    fig.set_size_inches(10*cols,5*row)\n",
    "    name='Gamma  {:.2f}, Episolon_Decay_Ratio {:.2f}'.format(i[0],i[1])\n",
    "    plt.suptitle(name)\n",
    "    \n",
    "    mode_Q=np.max(Q_track,axis=-1)\n",
    "    plot_Q=mode_Q.mean(axis=-1)\n",
    "    axes[0].set_title('Mean V & Episodes')\n",
    "    axes[0].plot(np.arange(plot_Q.shape[0]),plot_Q)\n",
    "    axes[0].set_xlabel('Episodes')\n",
    "    axes[0].set_ylabel('Mean V')\n",
    "\n",
    "\n",
    "    cumulative_sum = np.cumsum(total_reward)\n",
    "    windowed_sum = cumulative_sum[10 - 1:] - np.concatenate([[0], cumulative_sum[:-10]])\n",
    "    windowed_sum=np.r_[-10*np.ones(cumulative_sum.shape[0]-windowed_sum.shape[0]),windowed_sum]\n",
    "    axes[1].set_title('Reward & Episodes (10 Episodes Averaged)')\n",
    "    axes[1].plot(np.arange(windowed_sum.shape[0]),windowed_sum)\n",
    "    axes[1].set_xlabel('Episodes')\n",
    "    axes[1].set_ylabel('Reward & Episodes')\n",
    "\n",
    "    axes[2].set_title('Value Heatmap')\n",
    "    axes[2].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_xticklabels([])\n",
    "    axes[2].set_yticklabels([])\n",
    "    heatmap=axes[2].imshow(V.reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[2])\n",
    "    \n",
    "    visualize_policy(np.array([list(pi.values())]),frozen_lake.desc,None,None,axes_in=np.array([axes[3]]))\n",
    "\n",
    "    axes[4].set_title('Most Common Training Steps Heatmap (Log)')\n",
    "    axes[4].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_xticklabels([])\n",
    "    axes[4].set_yticklabels([])\n",
    "    states_counter_arr=np.array(list(states_counter.values()))+1\n",
    "    heatmap=axes[4].imshow(np.log(states_counter_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[4])\n",
    "\n",
    "    axes[5].set_title('Most Common Ideal Policy Steps Heatmap (Log)')\n",
    "    axes[5].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_xticklabels([])\n",
    "    axes[5].set_yticklabels([])\n",
    "    states_counter_app_arr=np.array(list(states_counter_app.values()))+1\n",
    "    heatmap=axes[5].imshow(np.log(states_counter_app_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_plots/frozen_lake_gd_small/{}.png'.format(name))\n",
    "    plt.close()\n",
    "    print(\"Avg. Number Of Winning Steps: \", average_win_steps)\n",
    "    print(\"Avg. episode reward: \", np.mean(episode_rewards))\n",
    "\n",
    "    #Saving policy, Q and V\n",
    "    # pd.DataFrame(Q,columns=['0','1','2','3']).to_csv(FILE_SAVE+name+'_Q.csv')\n",
    "    # pd.DataFrame(V.reshape(size_val,size_val)).to_csv(FILE_SAVE+name+'_V.csv')\n",
    "    # pd.DataFrame(list(pi.values()),columns=['Action']).to_csv(FILE_SAVE+name+'_pi.csv')\n",
    "\n",
    "    average_eval_rewards.append(np.mean(episode_rewards))\n",
    "    average_win_steps_results.append(average_win_steps)\n",
    "    keys.append(name)\n",
    "    print(\"###################\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Q Learning grid search\n",
    "gammas_grid_2=[0.999]\n",
    "#gammas_grid=np.r_[gammas_grid,np.array([0.999])]\n",
    "\n",
    "#epsilon_decays_2=[0.01,0.99]\n",
    "epsilon_decays_2\n",
    "iters=[int(2.5e6)]\n",
    "#iters=[int(100000)]\n",
    "env=frozen_lake\n",
    "average_eval_rewards_bad=[]\n",
    "average_win_steps_results_bad=[]\n",
    "keys=[]\n",
    "FILE_SAVE='saved_data/frozen_lake_gd_run/'\n",
    "\n",
    "for i in itertools.product(gammas_grid_2, epsilon_decays_2, iters):\n",
    "\n",
    "    print(\"running q_learning with gamma:\", i[0],  \"epsilon decay:\", i[1],  \" iterations:\", i[2])\n",
    "    Q, V, pi, Q_track, pi_track, total_reward, states_counter = RL(env).q_learning(gamma=i[0], epsilon_decay_ratio=i[1], n_episodes=i[2], min_alpha=0.1)\n",
    "\n",
    "    #summed_Q=Q_track.sum(axis=1).sum(axis=-1)\n",
    "    #ind_cross=np.argwhere(summed_Q==0)[0][0]\n",
    "    #Q_track[ind_cross:]=Q_track[np.argwhere(summed_Q==0)[0][0]-1]\n",
    "    #total_reward[ind_cross:]=total_reward[np.argwhere(total_reward==0)[0][0]-1]\n",
    "\n",
    "    episode_rewards, average_win_steps, states_counter_app = TestEnv.test_env(env=env, n_iters=100, pi=pi)\n",
    "\n",
    "    #Plotting rewards and Q learning visualizations + Value heat map per run\n",
    "    row=1\n",
    "    cols=6\n",
    "    fig, axes = plt.subplots(row,cols)\n",
    "    fig.set_size_inches(10*cols,5*row)\n",
    "    name='Gamma  {:.2f}, Episolon_Decay_Ratio {:.2f}'.format(i[0],i[1])\n",
    "    plt.suptitle(name)\n",
    "    \n",
    "    mode_Q=np.max(Q_track,axis=-1)\n",
    "    plot_Q=mode_Q.mean(axis=-1)\n",
    "    axes[0].set_title('Mean V & Episodes')\n",
    "    axes[0].plot(np.arange(plot_Q.shape[0]),plot_Q)\n",
    "    axes[0].set_xlabel('Episodes')\n",
    "    axes[0].set_ylabel('Mean V')\n",
    "\n",
    "\n",
    "    cumulative_sum = np.cumsum(total_reward)\n",
    "    windowed_sum = cumulative_sum[10 - 1:] - np.concatenate([[0], cumulative_sum[:-10]])\n",
    "    windowed_sum=np.r_[-10*np.ones(cumulative_sum.shape[0]-windowed_sum.shape[0]),windowed_sum]\n",
    "    axes[1].set_title('Reward & Episodes (10 Episodes Averaged)')\n",
    "    axes[1].plot(np.arange(windowed_sum.shape[0]),windowed_sum)\n",
    "    axes[1].set_xlabel('Episodes')\n",
    "    axes[1].set_ylabel('Reward & Episodes')\n",
    "\n",
    "    axes[2].set_title('Value Heatmap')\n",
    "    axes[2].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_xticklabels([])\n",
    "    axes[2].set_yticklabels([])\n",
    "    heatmap=axes[2].imshow(V.reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[2])\n",
    "    \n",
    "    visualize_policy(np.array([list(pi.values())]),frozen_lake.desc,None,None,axes_in=np.array([axes[3]]))\n",
    "\n",
    "    axes[4].set_title('Most Common Training Steps Heatmap (Log)')\n",
    "    axes[4].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_xticklabels([])\n",
    "    axes[4].set_yticklabels([])\n",
    "    states_counter_arr=np.array(list(states_counter.values()))+1\n",
    "    heatmap=axes[4].imshow(np.log(states_counter_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[4])\n",
    "\n",
    "    axes[5].set_title('Most Common Ideal Policy Steps Heatmap (Log)')\n",
    "    axes[5].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_xticklabels([])\n",
    "    axes[5].set_yticklabels([])\n",
    "    states_counter_app_arr=np.array(list(states_counter_app.values()))+1\n",
    "    heatmap=axes[5].imshow(np.log(states_counter_app_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_plots/frozen_lake_gd/{}.png'.format(name))\n",
    "    plt.close()\n",
    "    print(\"Avg. Number Of Winning Steps: \", average_win_steps)\n",
    "    print(\"Avg. episode reward: \", np.mean(episode_rewards))\n",
    "\n",
    "    #Saving policy, Q and V\n",
    "    pd.DataFrame(Q,columns=['0','1','2','3']).to_csv(FILE_SAVE+name+'_Q.csv')\n",
    "    pd.DataFrame(V.reshape(size_val,size_val)).to_csv(FILE_SAVE+name+'_V.csv')\n",
    "    pd.DataFrame(list(pi.values()),columns=['Action']).to_csv(FILE_SAVE+name+'_pi.csv')\n",
    "\n",
    "    average_eval_rewards_bad.append(np.mean(episode_rewards))\n",
    "    average_win_steps_results_bad.append(average_win_steps)\n",
    "    keys.append(name)\n",
    "    print(\"###################\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Q Learning grid search\n",
    "gammas_grid_3=[0.999]\n",
    "epsilon_decays_2=[0.99]\n",
    "\n",
    "init_epsilon=0.01\n",
    "iters=[int(2.5e6)]\n",
    "\n",
    "env=frozen_lake\n",
    "average_eval_rewards_bad=[]\n",
    "average_win_steps_results_bad=[]\n",
    "keys=[]\n",
    "FILE_SAVE='saved_data/frozen_lake_gd_run/'\n",
    "name='0.999_g_0.1_init_epsilon'\n",
    "\n",
    "for i in itertools.product(gammas_grid_3, epsilon_decays_2, iters):\n",
    "\n",
    "    print(\"running q_learning with gamma:\", i[0],  \"epsilon decay:\", i[1],  \" iterations:\", i[2])\n",
    "    Q, V, pi, Q_track, pi_track, total_reward, states_counter = RL(env).q_learning(gamma=i[0], epsilon_decay_ratio=i[1], n_episodes=i[2], min_alpha=0.1,init_epsilon=init_epsilon)\n",
    "\n",
    "    #summed_Q=Q_track.sum(axis=1).sum(axis=-1)\n",
    "    #ind_cross=np.argwhere(summed_Q==0)[0][0]\n",
    "    #Q_track[ind_cross:]=Q_track[np.argwhere(summed_Q==0)[0][0]-1]\n",
    "    #total_reward[ind_cross:]=total_reward[np.argwhere(total_reward==0)[0][0]-1]\n",
    "\n",
    "    episode_rewards, average_win_steps, states_counter_app = TestEnv.test_env(env=env, n_iters=100, pi=pi)\n",
    "\n",
    "    #Plotting rewards and Q learning visualizations + Value heat map per run\n",
    "    row=1\n",
    "    cols=6\n",
    "    fig, axes = plt.subplots(row,cols)\n",
    "    fig.set_size_inches(10*cols,5*row)\n",
    "    #name='Gamma  {:.2f}, Episolon_Decay_Ratio {:.2f}'.format(i[0],i[1])\n",
    "    plt.suptitle(name)\n",
    "    \n",
    "    mode_Q=np.max(Q_track,axis=-1)\n",
    "    plot_Q=mode_Q.mean(axis=-1)\n",
    "    axes[0].set_title('Mean V & Episodes')\n",
    "    axes[0].plot(np.arange(plot_Q.shape[0]),plot_Q)\n",
    "    axes[0].set_xlabel('Episodes')\n",
    "    axes[0].set_ylabel('Mean V')\n",
    "\n",
    "\n",
    "    cumulative_sum = np.cumsum(total_reward)\n",
    "    windowed_sum = cumulative_sum[10 - 1:] - np.concatenate([[0], cumulative_sum[:-10]])\n",
    "    windowed_sum=np.r_[-10*np.ones(cumulative_sum.shape[0]-windowed_sum.shape[0]),windowed_sum]\n",
    "    axes[1].set_title('Reward & Episodes (10 Episodes Averaged)')\n",
    "    axes[1].plot(np.arange(windowed_sum.shape[0]),windowed_sum)\n",
    "    axes[1].set_xlabel('Episodes')\n",
    "    axes[1].set_ylabel('Reward & Episodes')\n",
    "\n",
    "    axes[2].set_title('Value Heatmap')\n",
    "    axes[2].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_xticklabels([])\n",
    "    axes[2].set_yticklabels([])\n",
    "    heatmap=axes[2].imshow(V.reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[2])\n",
    "    \n",
    "    visualize_policy(np.array([list(pi.values())]),frozen_lake.desc,None,None,axes_in=np.array([axes[3]]))\n",
    "\n",
    "    axes[4].set_title('Most Common Training Steps Heatmap (Log)')\n",
    "    axes[4].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_xticklabels([])\n",
    "    axes[4].set_yticklabels([])\n",
    "    states_counter_arr=np.array(list(states_counter.values()))+1\n",
    "    heatmap=axes[4].imshow(np.log(states_counter_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[4])\n",
    "\n",
    "    axes[5].set_title('Most Common Ideal Policy Steps Heatmap (Log)')\n",
    "    axes[5].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_xticklabels([])\n",
    "    axes[5].set_yticklabels([])\n",
    "    states_counter_app_arr=np.array(list(states_counter_app.values()))+1\n",
    "    heatmap=axes[5].imshow(np.log(states_counter_app_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_plots/frozen_lake_gd/{}.png'.format(name))\n",
    "    plt.close()\n",
    "    print(\"Avg. Number Of Winning Steps: \", average_win_steps)\n",
    "    print(\"Avg. episode reward: \", np.mean(episode_rewards))\n",
    "\n",
    "    #Saving policy, Q and V\n",
    "    pd.DataFrame(Q,columns=['0','1','2','3']).to_csv(FILE_SAVE+name+'_Q.csv')\n",
    "    pd.DataFrame(V.reshape(size_val,size_val)).to_csv(FILE_SAVE+name+'_V.csv')\n",
    "    pd.DataFrame(list(pi.values()),columns=['Action']).to_csv(FILE_SAVE+name+'_pi.csv')\n",
    "\n",
    "    average_eval_rewards_bad.append(np.mean(episode_rewards))\n",
    "    average_win_steps_results_bad.append(average_win_steps)\n",
    "    keys.append(name)\n",
    "    print(\"###################\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Q Learning grid search\n",
    "gammas_grid_3=[0.999]\n",
    "epsilon_decays_2=[0.99]\n",
    "\n",
    "init_epsilon=min_epsilon=0.001\n",
    "iters=[int(2.5e6)]\n",
    "\n",
    "env=frozen_lake\n",
    "average_eval_rewards_bad=[]\n",
    "average_win_steps_results_bad=[]\n",
    "keys=[]\n",
    "FILE_SAVE='saved_data/frozen_lake_gd_run/'\n",
    "name='0.999_g_0.001_epsilon'\n",
    "\n",
    "for i in itertools.product(gammas_grid_3, epsilon_decays_2, iters):\n",
    "\n",
    "    print(\"running q_learning with gamma:\", i[0],  \"epsilon decay:\", i[1],  \" iterations:\", i[2])\n",
    "    Q, V, pi, Q_track, pi_track, total_reward, states_counter = RL(env).q_learning(gamma=i[0], epsilon_decay_ratio=i[1], n_episodes=i[2], min_alpha=0.1,init_epsilon=init_epsilon,min_epsilon=min_epsilon)\n",
    "\n",
    "    #summed_Q=Q_track.sum(axis=1).sum(axis=-1)\n",
    "    #ind_cross=np.argwhere(summed_Q==0)[0][0]\n",
    "    #Q_track[ind_cross:]=Q_track[np.argwhere(summed_Q==0)[0][0]-1]\n",
    "    #total_reward[ind_cross:]=total_reward[np.argwhere(total_reward==0)[0][0]-1]\n",
    "\n",
    "    episode_rewards, average_win_steps, states_counter_app = TestEnv.test_env(env=env, n_iters=100, pi=pi)\n",
    "\n",
    "    #Plotting rewards and Q learning visualizations + Value heat map per run\n",
    "    row=1\n",
    "    cols=6\n",
    "    fig, axes = plt.subplots(row,cols)\n",
    "    fig.set_size_inches(10*cols,5*row)\n",
    "    #name='Gamma  {:.2f}, Episolon_Decay_Ratio {:.2f}'.format(i[0],i[1])\n",
    "    plt.suptitle(name)\n",
    "    \n",
    "    mode_Q=np.max(Q_track,axis=-1)\n",
    "    plot_Q=mode_Q.mean(axis=-1)\n",
    "    axes[0].set_title('Mean V & Episodes')\n",
    "    axes[0].plot(np.arange(plot_Q.shape[0]),plot_Q)\n",
    "    axes[0].set_xlabel('Episodes')\n",
    "    axes[0].set_ylabel('Mean V')\n",
    "\n",
    "\n",
    "    cumulative_sum = np.cumsum(total_reward)\n",
    "    windowed_sum = cumulative_sum[10 - 1:] - np.concatenate([[0], cumulative_sum[:-10]])\n",
    "    windowed_sum=np.r_[-10*np.ones(cumulative_sum.shape[0]-windowed_sum.shape[0]),windowed_sum]\n",
    "    axes[1].set_title('Reward & Episodes (10 Episodes Averaged)')\n",
    "    axes[1].plot(np.arange(windowed_sum.shape[0]),windowed_sum)\n",
    "    axes[1].set_xlabel('Episodes')\n",
    "    axes[1].set_ylabel('Reward & Episodes')\n",
    "\n",
    "    axes[2].set_title('Value Heatmap')\n",
    "    axes[2].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_xticklabels([])\n",
    "    axes[2].set_yticklabels([])\n",
    "    heatmap=axes[2].imshow(V.reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[2])\n",
    "    \n",
    "    visualize_policy(np.array([list(pi.values())]),frozen_lake.desc,None,None,axes_in=np.array([axes[3]]))\n",
    "\n",
    "    axes[4].set_title('Most Common Training Steps Heatmap (Log)')\n",
    "    axes[4].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_xticklabels([])\n",
    "    axes[4].set_yticklabels([])\n",
    "    states_counter_arr=np.array(list(states_counter.values()))+1\n",
    "    heatmap=axes[4].imshow(np.log(states_counter_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[4])\n",
    "\n",
    "    axes[5].set_title('Most Common Ideal Policy Steps Heatmap (Log)')\n",
    "    axes[5].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_xticklabels([])\n",
    "    axes[5].set_yticklabels([])\n",
    "    states_counter_app_arr=np.array(list(states_counter_app.values()))+1\n",
    "    heatmap=axes[5].imshow(np.log(states_counter_app_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_plots/frozen_lake_gd/{}.png'.format(name))\n",
    "    plt.close()\n",
    "    print(\"Avg. Number Of Winning Steps: \", average_win_steps)\n",
    "    print(\"Avg. episode reward: \", np.mean(episode_rewards))\n",
    "\n",
    "    #Saving policy, Q and V\n",
    "    pd.DataFrame(Q,columns=['0','1','2','3']).to_csv(FILE_SAVE+name+'_Q.csv')\n",
    "    pd.DataFrame(V.reshape(size_val,size_val)).to_csv(FILE_SAVE+name+'_V.csv')\n",
    "    pd.DataFrame(list(pi.values()),columns=['Action']).to_csv(FILE_SAVE+name+'_pi.csv')\n",
    "\n",
    "    average_eval_rewards_bad.append(np.mean(episode_rewards))\n",
    "    average_win_steps_results_bad.append(average_win_steps)\n",
    "    keys.append(name)\n",
    "    print(\"###################\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Q Learning grid search\n",
    "gammas_grid_3=[0.81]\n",
    "epsilon_decays_2=[0.99]\n",
    "\n",
    "init_epsilon=min_epsilon=0.001\n",
    "iters=[int(2.5e6)]\n",
    "\n",
    "env=frozen_lake\n",
    "average_eval_rewards_bad=[]\n",
    "average_win_steps_results_bad=[]\n",
    "keys=[]\n",
    "FILE_SAVE='saved_data/frozen_lake_gd_run/'\n",
    "name='0.81_g_0.001_epsilon'\n",
    "\n",
    "for i in itertools.product(gammas_grid_3, epsilon_decays_2, iters):\n",
    "\n",
    "    print(\"running q_learning with gamma:\", i[0],  \"epsilon decay:\", i[1],  \" iterations:\", i[2])\n",
    "    Q, V, pi, Q_track, pi_track, total_reward, states_counter = RL(env).q_learning(gamma=i[0], epsilon_decay_ratio=i[1], n_episodes=i[2], min_alpha=0.1,init_epsilon=init_epsilon,min_epsilon=min_epsilon)\n",
    "\n",
    "    #summed_Q=Q_track.sum(axis=1).sum(axis=-1)\n",
    "    #ind_cross=np.argwhere(summed_Q==0)[0][0]\n",
    "    #Q_track[ind_cross:]=Q_track[np.argwhere(summed_Q==0)[0][0]-1]\n",
    "    #total_reward[ind_cross:]=total_reward[np.argwhere(total_reward==0)[0][0]-1]\n",
    "\n",
    "    episode_rewards, average_win_steps, states_counter_app = TestEnv.test_env(env=env, n_iters=100, pi=pi)\n",
    "\n",
    "    #Plotting rewards and Q learning visualizations + Value heat map per run\n",
    "    row=1\n",
    "    cols=6\n",
    "    fig, axes = plt.subplots(row,cols)\n",
    "    fig.set_size_inches(10*cols,5*row)\n",
    "    #name='Gamma  {:.2f}, Episolon_Decay_Ratio {:.2f}'.format(i[0],i[1])\n",
    "    plt.suptitle(name)\n",
    "    \n",
    "    mode_Q=np.max(Q_track,axis=-1)\n",
    "    plot_Q=mode_Q.mean(axis=-1)\n",
    "    axes[0].set_title('Mean V & Episodes')\n",
    "    axes[0].plot(np.arange(plot_Q.shape[0]),plot_Q)\n",
    "    axes[0].set_xlabel('Episodes')\n",
    "    axes[0].set_ylabel('Mean V')\n",
    "\n",
    "\n",
    "    cumulative_sum = np.cumsum(total_reward)\n",
    "    windowed_sum = cumulative_sum[10 - 1:] - np.concatenate([[0], cumulative_sum[:-10]])\n",
    "    windowed_sum=np.r_[-10*np.ones(cumulative_sum.shape[0]-windowed_sum.shape[0]),windowed_sum]\n",
    "    axes[1].set_title('Reward & Episodes (10 Episodes Averaged)')\n",
    "    axes[1].plot(np.arange(windowed_sum.shape[0]),windowed_sum)\n",
    "    axes[1].set_xlabel('Episodes')\n",
    "    axes[1].set_ylabel('Reward & Episodes')\n",
    "\n",
    "    axes[2].set_title('Value Heatmap')\n",
    "    axes[2].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_xticklabels([])\n",
    "    axes[2].set_yticklabels([])\n",
    "    heatmap=axes[2].imshow(V.reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[2])\n",
    "    \n",
    "    visualize_policy(np.array([list(pi.values())]),frozen_lake.desc,None,None,axes_in=np.array([axes[3]]))\n",
    "\n",
    "    axes[4].set_title('Most Common Training Steps Heatmap (Log)')\n",
    "    axes[4].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_xticklabels([])\n",
    "    axes[4].set_yticklabels([])\n",
    "    states_counter_arr=np.array(list(states_counter.values()))+1\n",
    "    heatmap=axes[4].imshow(np.log(states_counter_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[4])\n",
    "\n",
    "    axes[5].set_title('Most Common Ideal Policy Steps Heatmap (Log)')\n",
    "    axes[5].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_xticklabels([])\n",
    "    axes[5].set_yticklabels([])\n",
    "    states_counter_app_arr=np.array(list(states_counter_app.values()))+1\n",
    "    heatmap=axes[5].imshow(np.log(states_counter_app_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_plots/frozen_lake_gd/{}.png'.format(name))\n",
    "    plt.close()\n",
    "    print(\"Avg. Number Of Winning Steps: \", average_win_steps)\n",
    "    print(\"Avg. episode reward: \", np.mean(episode_rewards))\n",
    "\n",
    "    #Saving policy, Q and V\n",
    "    pd.DataFrame(Q,columns=['0','1','2','3']).to_csv(FILE_SAVE+name+'_Q.csv')\n",
    "    pd.DataFrame(V.reshape(size_val,size_val)).to_csv(FILE_SAVE+name+'_V.csv')\n",
    "    pd.DataFrame(list(pi.values()),columns=['Action']).to_csv(FILE_SAVE+name+'_pi.csv')\n",
    "\n",
    "    average_eval_rewards_bad.append(np.mean(episode_rewards))\n",
    "    average_win_steps_results_bad.append(average_win_steps)\n",
    "    keys.append(name)\n",
    "    print(\"###################\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Q Learning grid search\n",
    "gammas_grid_3=[0.999]\n",
    "epsilon_decays_2=[0.99]\n",
    "\n",
    "init_alpha=min_alpha=0.9\n",
    "iters=[int(2.5e6)]\n",
    "\n",
    "env=frozen_lake\n",
    "average_eval_rewards_bad=[]\n",
    "average_win_steps_results_bad=[]\n",
    "keys=[]\n",
    "FILE_SAVE='saved_data/frozen_lake_gd_run/'\n",
    "name='0.999g_0.99ed_0.9alpha'\n",
    "\n",
    "for i in itertools.product(gammas_grid_3, epsilon_decays_2, iters):\n",
    "\n",
    "    print(\"running q_learning with gamma:\", i[0],  \"epsilon decay:\", i[1],  \" iterations:\", i[2])\n",
    "    Q, V, pi, Q_track, pi_track, total_reward, states_counter = RL(env).q_learning(gamma=i[0], epsilon_decay_ratio=i[1], n_episodes=i[2], min_alpha=min_alpha,init_alpha=init_alpha,init_epsilon=init_epsilon,min_epsilon=min_epsilon)\n",
    "\n",
    "    #summed_Q=Q_track.sum(axis=1).sum(axis=-1)\n",
    "    #ind_cross=np.argwhere(summed_Q==0)[0][0]\n",
    "    #Q_track[ind_cross:]=Q_track[np.argwhere(summed_Q==0)[0][0]-1]\n",
    "    #total_reward[ind_cross:]=total_reward[np.argwhere(total_reward==0)[0][0]-1]\n",
    "\n",
    "    episode_rewards, average_win_steps, states_counter_app = TestEnv.test_env(env=env, n_iters=100, pi=pi)\n",
    "\n",
    "    #Plotting rewards and Q learning visualizations + Value heat map per run\n",
    "    row=1\n",
    "    cols=6\n",
    "    fig, axes = plt.subplots(row,cols)\n",
    "    fig.set_size_inches(10*cols,5*row)\n",
    "    #name='Gamma  {:.2f}, Episolon_Decay_Ratio {:.2f}'.format(i[0],i[1])\n",
    "    plt.suptitle(name)\n",
    "    \n",
    "    mode_Q=np.max(Q_track,axis=-1)\n",
    "    plot_Q=mode_Q.mean(axis=-1)\n",
    "    axes[0].set_title('Mean V & Episodes')\n",
    "    axes[0].plot(np.arange(plot_Q.shape[0]),plot_Q)\n",
    "    axes[0].set_xlabel('Episodes')\n",
    "    axes[0].set_ylabel('Mean V')\n",
    "\n",
    "\n",
    "    cumulative_sum = np.cumsum(total_reward)\n",
    "    windowed_sum = cumulative_sum[10 - 1:] - np.concatenate([[0], cumulative_sum[:-10]])\n",
    "    windowed_sum=np.r_[-10*np.ones(cumulative_sum.shape[0]-windowed_sum.shape[0]),windowed_sum]\n",
    "    axes[1].set_title('Reward & Episodes (10 Episodes Averaged)')\n",
    "    axes[1].plot(np.arange(windowed_sum.shape[0]),windowed_sum)\n",
    "    axes[1].set_xlabel('Episodes')\n",
    "    axes[1].set_ylabel('Reward & Episodes')\n",
    "\n",
    "    axes[2].set_title('Value Heatmap')\n",
    "    axes[2].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_xticklabels([])\n",
    "    axes[2].set_yticklabels([])\n",
    "    heatmap=axes[2].imshow(V.reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[2])\n",
    "    \n",
    "    visualize_policy(np.array([list(pi.values())]),frozen_lake.desc,None,None,axes_in=np.array([axes[3]]))\n",
    "\n",
    "    axes[4].set_title('Most Common Training Steps Heatmap (Log)')\n",
    "    axes[4].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_xticklabels([])\n",
    "    axes[4].set_yticklabels([])\n",
    "    states_counter_arr=np.array(list(states_counter.values()))+1\n",
    "    heatmap=axes[4].imshow(np.log(states_counter_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[4])\n",
    "\n",
    "    axes[5].set_title('Most Common Ideal Policy Steps Heatmap (Log)')\n",
    "    axes[5].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_xticklabels([])\n",
    "    axes[5].set_yticklabels([])\n",
    "    states_counter_app_arr=np.array(list(states_counter_app.values()))+1\n",
    "    heatmap=axes[5].imshow(np.log(states_counter_app_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_plots/frozen_lake_gd/{}.png'.format(name))\n",
    "    plt.close()\n",
    "    print(\"Avg. Number Of Winning Steps: \", average_win_steps)\n",
    "    print(\"Avg. episode reward: \", np.mean(episode_rewards))\n",
    "\n",
    "    #Saving policy, Q and V\n",
    "    pd.DataFrame(Q,columns=['0','1','2','3']).to_csv(FILE_SAVE+name+'_Q.csv')\n",
    "    pd.DataFrame(V.reshape(size_val,size_val)).to_csv(FILE_SAVE+name+'_V.csv')\n",
    "    pd.DataFrame(list(pi.values()),columns=['Action']).to_csv(FILE_SAVE+name+'_pi.csv')\n",
    "\n",
    "    average_eval_rewards_bad.append(np.mean(episode_rewards))\n",
    "    average_win_steps_results_bad.append(average_win_steps)\n",
    "    keys.append(name)\n",
    "    print(\"###################\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Q Learning grid search\n",
    "gammas_grid_3=[0.999]\n",
    "epsilon_decays_2=[0.99]\n",
    "\n",
    "init_alpha=min_alpha=0.01\n",
    "iters=[int(2.5e6)]\n",
    "\n",
    "env=frozen_lake\n",
    "average_eval_rewards_bad=[]\n",
    "average_win_steps_results_bad=[]\n",
    "keys=[]\n",
    "FILE_SAVE='saved_data/frozen_lake_gd_run/'\n",
    "name='0.999g_0.99ed_0.01alpha'\n",
    "\n",
    "for i in itertools.product(gammas_grid_3, epsilon_decays_2, iters):\n",
    "\n",
    "    print(\"running q_learning with gamma:\", i[0],  \"epsilon decay:\", i[1],  \" iterations:\", i[2])\n",
    "    Q, V, pi, Q_track, pi_track, total_reward, states_counter = RL(env).q_learning(gamma=i[0], epsilon_decay_ratio=i[1], n_episodes=i[2], min_alpha=min_alpha,init_alpha=init_alpha,init_epsilon=init_epsilon,min_epsilon=min_epsilon)\n",
    "\n",
    "    #summed_Q=Q_track.sum(axis=1).sum(axis=-1)\n",
    "    #ind_cross=np.argwhere(summed_Q==0)[0][0]\n",
    "    #Q_track[ind_cross:]=Q_track[np.argwhere(summed_Q==0)[0][0]-1]\n",
    "    #total_reward[ind_cross:]=total_reward[np.argwhere(total_reward==0)[0][0]-1]\n",
    "\n",
    "    episode_rewards, average_win_steps, states_counter_app = TestEnv.test_env(env=env, n_iters=100, pi=pi)\n",
    "\n",
    "    #Plotting rewards and Q learning visualizations + Value heat map per run\n",
    "    row=1\n",
    "    cols=6\n",
    "    fig, axes = plt.subplots(row,cols)\n",
    "    fig.set_size_inches(10*cols,5*row)\n",
    "    #name='Gamma  {:.2f}, Episolon_Decay_Ratio {:.2f}'.format(i[0],i[1])\n",
    "    plt.suptitle(name)\n",
    "    \n",
    "    mode_Q=np.max(Q_track,axis=-1)\n",
    "    plot_Q=mode_Q.mean(axis=-1)\n",
    "    axes[0].set_title('Mean V & Episodes')\n",
    "    axes[0].plot(np.arange(plot_Q.shape[0]),plot_Q)\n",
    "    axes[0].set_xlabel('Episodes')\n",
    "    axes[0].set_ylabel('Mean V')\n",
    "\n",
    "\n",
    "    cumulative_sum = np.cumsum(total_reward)\n",
    "    windowed_sum = cumulative_sum[10 - 1:] - np.concatenate([[0], cumulative_sum[:-10]])\n",
    "    windowed_sum=np.r_[-10*np.ones(cumulative_sum.shape[0]-windowed_sum.shape[0]),windowed_sum]\n",
    "    axes[1].set_title('Reward & Episodes (10 Episodes Averaged)')\n",
    "    axes[1].plot(np.arange(windowed_sum.shape[0]),windowed_sum)\n",
    "    axes[1].set_xlabel('Episodes')\n",
    "    axes[1].set_ylabel('Reward & Episodes')\n",
    "\n",
    "    axes[2].set_title('Value Heatmap')\n",
    "    axes[2].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[2].set_xticklabels([])\n",
    "    axes[2].set_yticklabels([])\n",
    "    heatmap=axes[2].imshow(V.reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[2])\n",
    "    \n",
    "    visualize_policy(np.array([list(pi.values())]),frozen_lake.desc,None,None,axes_in=np.array([axes[3]]))\n",
    "\n",
    "    axes[4].set_title('Most Common Training Steps Heatmap (Log)')\n",
    "    axes[4].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[4].set_xticklabels([])\n",
    "    axes[4].set_yticklabels([])\n",
    "    states_counter_arr=np.array(list(states_counter.values()))+1\n",
    "    heatmap=axes[4].imshow(np.log(states_counter_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[4])\n",
    "\n",
    "    axes[5].set_title('Most Common Ideal Policy Steps Heatmap (Log)')\n",
    "    axes[5].set_xticks(np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_yticks(-1*np.arange(V.shape[0]**(0.5)))\n",
    "    axes[5].set_xticklabels([])\n",
    "    axes[5].set_yticklabels([])\n",
    "    states_counter_app_arr=np.array(list(states_counter_app.values()))+1\n",
    "    heatmap=axes[5].imshow(np.log(states_counter_app_arr).reshape(int(len(V)**(0.5)),int(len(V)**(0.5))), cmap='jet', interpolation='lanczos')\n",
    "    plt.colorbar(heatmap, ax=axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_plots/frozen_lake_gd/{}.png'.format(name))\n",
    "    plt.close()\n",
    "    print(\"Avg. Number Of Winning Steps: \", average_win_steps)\n",
    "    print(\"Avg. episode reward: \", np.mean(episode_rewards))\n",
    "\n",
    "    #Saving policy, Q and V\n",
    "    pd.DataFrame(Q,columns=['0','1','2','3']).to_csv(FILE_SAVE+name+'_Q.csv')\n",
    "    pd.DataFrame(V.reshape(size_val,size_val)).to_csv(FILE_SAVE+name+'_V.csv')\n",
    "    pd.DataFrame(list(pi.values()),columns=['Action']).to_csv(FILE_SAVE+name+'_pi.csv')\n",
    "\n",
    "    average_eval_rewards_bad.append(np.mean(episode_rewards))\n",
    "    average_win_steps_results_bad.append(average_win_steps)\n",
    "    keys.append(name)\n",
    "    print(\"###################\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas_grid=np.arange(0.72,1.00,0.09)\n",
    "gammas_grid=np.r_[gammas_grid,np.array([0.999])]\n",
    "epsilon_decays=np.arange(0.72,1.0,0.09) \n",
    "\n",
    "average_reward=np.zeros((len(gammas_grid),len(epsilon_decays)))\n",
    "average_win_steps_array=np.zeros((len(gammas_grid),len(epsilon_decays)))\n",
    "\n",
    "for i,g in enumerate(gammas_grid):\n",
    "    for j,e in enumerate(epsilon_decays):\n",
    "        name='Gamma  {:.2f}, Episolon_Decay_Ratio {:.2f}'.format(g,e)\n",
    "\n",
    "        ideal_pi='saved_data/frozen_lake_gd_run/'+name+'_pi.csv'\n",
    "        pi_saved=pd.read_csv(ideal_pi).drop(columns=['Unnamed: 0']).to_dict()['Action']\n",
    "\n",
    "        episode_rewards, average_win_steps, states_counter_app = TestEnv.test_env(env=env, n_iters=100, pi=pi_saved)\n",
    "\n",
    "        average_reward[i,j]=np.mean(episode_rewards)\n",
    "        average_win_steps_array[i,j]=average_win_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Heat Map\n",
    "#Plotting into heatmaps\n",
    "row=1\n",
    "cols=2\n",
    "fig, axes = plt.subplots(row,cols)\n",
    "fig.set_size_inches(10*cols,5*row)\n",
    "name='Q Learning Results'\n",
    "plt.suptitle('                          Q Learning Results',fontsize=18)\n",
    "\n",
    "x_labels=['{:.3f}'.format(i) for i in epsilon_decays]\n",
    "y_labels=['{:.3f}'.format(i) for i in gammas_grid]\n",
    "\n",
    "axes[0].set_title('Average Reward Heatmap')\n",
    "axes[0].set_xticks(np.arange(epsilon_decays.shape[0]))\n",
    "axes[0].set_yticks(np.arange(gammas_grid.shape[0]))\n",
    "axes[0].set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "axes[0].set_yticklabels(y_labels)\n",
    "axes[0].set_xlabel('Epsilon Delta Ratio')\n",
    "axes[0].set_ylabel('Gamma')\n",
    "heatmap=axes[0].imshow(average_reward, cmap='jet', interpolation='lanczos')\n",
    "plt.colorbar(heatmap, ax=axes[0])\n",
    "\n",
    "axes[1].set_title('Average Steps Taken for Ideal Policy (Win)')\n",
    "axes[1].set_xticks(np.arange(epsilon_decays.shape[0]))\n",
    "axes[1].set_yticks(np.arange(gammas_grid.shape[0]))\n",
    "axes[1].set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "axes[1].set_yticklabels(y_labels)\n",
    "axes[1].set_xlabel('Epsilon Delta Ratio')\n",
    "axes[1].set_ylabel('Gamma')\n",
    "heatmap=axes[1].imshow(average_win_steps_array, cmap='jet', interpolation='lanczos')\n",
    "plt.colorbar(heatmap, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

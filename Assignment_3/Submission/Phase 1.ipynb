{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:07.913679Z",
     "start_time": "2024-03-23T08:29:59.291447Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import operator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,fbeta_score,make_scorer,mutual_info_score,silhouette_score,normalized_mutual_info_score,classification_report, confusion_matrix,f1_score, mean_squared_error, adjusted_rand_score\n",
    "import time \n",
    "from NNnet_class import NNnet\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA,FastICA\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim,SparseRandomProjection,GaussianRandomProjection\n",
    "from sklearn.manifold import TSNE,trustworthiness,Isomap\n",
    "from A3_utils import calculate_wcss,plot_gallery,categorizer,create_elbow_plot,experiment_em_clusters,experiment_km_clusters\n",
    "from kmodes.kmodes import KModes\n",
    "import umap\n",
    "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
    "                                  TextArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:08.870904Z",
     "start_time": "2024-03-23T08:30:08.859901Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "# Ignore SparseEfficiencyWarning\n",
    "warnings.simplefilter('ignore', category=SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:10.004170Z",
     "start_time": "2024-03-23T08:30:09.994167Z"
    }
   },
   "outputs": [],
   "source": [
    "#Seeting seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Making accuracy scorrer\n",
    "accuracy_scorer=make_scorer(accuracy_score)\n",
    "\n",
    "#Global Beta (Used for Ftwo score - not used in assignment)\n",
    "BETA=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:10.686329Z",
     "start_time": "2024-03-23T08:30:10.663324Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_param_curve(data,param,param_vals,ax,algorithim_name,algorithim,metric=accuracy_scorer,metric_name='Accuracy',graph=True,beta=BETA,cv=3):\n",
    "\n",
    "    '''Function to create parameter curves\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of np.arrays containing the features and labels  \n",
    "    param (str): Parameter to vary\n",
    "    param_vals (list): Parameter values\n",
    "    ax (matplotlib.axes): Axis for graph\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim to vary parameter in\n",
    "    metric (sklearn.metric): Metric to score algorotihim on\n",
    "    metric_name (str): Metric Name\n",
    "    graph (bool): Bool for validation graph\n",
    "    beta (int): Beta value for the Ftwo scored \n",
    "    cv (int): The number of cross validations folds\n",
    "    \n",
    "    Returns:\n",
    "    None'''\n",
    "\n",
    "\n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "    \n",
    "    for i in param_vals:\n",
    "        #Crossvalidating each parameter value\n",
    "        kf=KFold(n_splits=cv,shuffle=True)\n",
    "\n",
    "        internal_train_accuracy=0\n",
    "        internal_test_accuracy=0\n",
    "        \n",
    "        for train,test in kf.split(X=data[0]):\n",
    "            if i!='Default':\n",
    "                if 'net' in algorithim_name.lower():\n",
    "                    kwargs={param:i,'input_dims':data[0].shape[-1]}\n",
    "                    clf=algorithim(**kwargs)\n",
    "                else:\n",
    "                    clf=algorithim(**{param:i})\n",
    "            else:\n",
    "                clf=algorithim()\n",
    "\n",
    "            clf.fit(data[0][train],data[1][train])\n",
    "            internal_train_accuracy+=metric(y_pred=clf.predict(data[0][train]),y_true=data[1][train])\n",
    "            internal_test_accuracy+=metric(y_pred=clf.predict(data[0][test]),y_true=data[1][test])\n",
    "\n",
    "        train_acc.append(internal_train_accuracy/cv)\n",
    "        test_acc.append(internal_test_accuracy/cv)\n",
    "\n",
    "    best_val=param_vals[np.argmax(test_acc)]\n",
    "\n",
    "    if type(param_vals[0])==str:\n",
    "        ax.scatter(param_vals,train_acc,label='Training {}'.format(metric_name))\n",
    "        ax.scatter(param_vals,test_acc,label='Validation {}'.format(metric_name))\n",
    "    else:\n",
    "        ax.plot(param_vals,train_acc,label='Training {}'.format(metric_name))\n",
    "        ax.plot(param_vals,test_acc,label='Validation {}'.format(metric_name))\n",
    "    #plt.xscale('log')\n",
    "    ax.axvline(best_val,label='Best {} Value'.format(param),color='red',linestyle = '--')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('{}'.format(metric_name));\n",
    "    ax.set_xlabel(param);\n",
    "    ax.set_title('{} vs {}'.format(param,metric_name));\n",
    "    if type(best_val)!=str:\n",
    "        if best_val==max(param_vals):\n",
    "            ax.text(y=(max(test_acc)+min(test_acc))/2+np.abs(np.std(test_acc)),x=best_val-np.std(param_vals)/12,s=best_val,color='green',weight='bold')\n",
    "        else:\n",
    "            ax.text(y=(max(test_acc)+min(test_acc))/2+np.abs(np.std(test_acc)),x=best_val+np.std(param_vals)/12,s=best_val,color='green',weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:11.800591Z",
     "start_time": "2024-03-23T08:30:11.785587Z"
    }
   },
   "outputs": [],
   "source": [
    "def deal_algorithim(data,param_dicts,dataset,algorithim_name,algorithim,metric=accuracy_score,metric_name='Accuracy',cv=3):\n",
    "    \n",
    "    '''Function to deal with algorithim\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of np.arrays containing the features and labels\n",
    "    param_dict (dict): Parameter dictionary to vary\n",
    "    dataset (str): Dataset name\n",
    "    algorithim_name (str): Algorithim name\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim to vary parameter in\n",
    "    metric (sklearn.metric): Metric to score algorotihim on\n",
    "    metric_name (str): Metric Name\n",
    "    cv (int): The number of cross validations folds \n",
    "\n",
    "    Returns:\n",
    "    None'''\n",
    "    \n",
    "    num_classes=len(param_dicts.keys())\n",
    "\n",
    "    #Getting Fig Size\n",
    "    fig,axes=plt.subplots(2,int(np.ceil(num_classes/2)))\n",
    "    fig.set_size_inches(15,15)\n",
    "    i=-1\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "        i+=1\n",
    "        plt.suptitle('Results for Algorithim: \"{}\" for \"{}\" Dataset'.format(algorithim_name,dataset),fontsize=18)\n",
    "        param=list(param_dicts.keys())[i]\n",
    "        param_vals=param_dicts[param]\n",
    "        create_param_curve(data,param,param_vals,ax,algorithim_name,algorithim,metric,metric_name)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:12.966865Z",
     "start_time": "2024-03-23T08:30:12.949861Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load Heart Disease Data\n",
    "#Load Heart Disease Data\n",
    "def load_heart_data():\n",
    "\n",
    "    '''Load Heart Disease Dataset\n",
    "    \n",
    "    Returns:\n",
    "    X (np.array): X array\n",
    "    Y (np.array): Y array\n",
    "    col_index (dict): Dictionary containing the pairing for the column location and it's name'''\n",
    "\n",
    "    #PLEASE CHANGE TO LOCATION OF YOUR HEART DATA\n",
    "    df=pd.read_csv('Data/Heart_2/heart.csv')\n",
    "    Y=np.array(df['HeartDisease'])\n",
    "    df.drop('HeartDisease',axis=1,inplace=True)\n",
    "    \n",
    "    label_columns=['HeartDisease']\n",
    "    categorical_columns=['Sex', 'ChestPainType', 'RestingECG','ExerciseAngina','ST_Slope','FastingBS']\n",
    "\n",
    "    non_categorical_variables=list(set(df.columns).difference(set(categorical_columns+label_columns)))\n",
    "    X=np.array(df[non_categorical_variables])\n",
    "    columns_categorized=non_categorical_variables\n",
    "\n",
    "    #Now we need to one hot vectorize the type_of_meal_plan, room_type_reserved and market_segment_type\n",
    "    label_dict={}\n",
    "    for i in categorical_columns:\n",
    "        label_dict[i]=OneHotEncoder()\n",
    "        res=label_dict[i].fit_transform(np.array(df[i]).reshape(-1,1)).toarray()\n",
    "        X=np.c_[X,res]\n",
    "        columns_categorized=columns_categorized+[i+'%'+j for j in ['1','2','3','4','5','6','7'][:res.shape[-1]]]\n",
    "\n",
    "        col_index={}\n",
    "        results_corr={}\n",
    "        for label,col in zip(columns_categorized,range(X.shape[-1])):\n",
    "            corr=scipy.stats.pearsonr(X[:,col],Y)[0]\n",
    "            results_corr[label]=corr\n",
    "            col_index[label]=col\n",
    "    return X,Y,col_index\n",
    "\n",
    "#Fashion MNIST\n",
    "def load_fmnist():\n",
    "    #Loading dataset\n",
    "    data=pd.concat([pd.read_csv('Data/FMNIST/fashion-mnist_train.csv'),pd.read_csv('Data/FMNIST/fashion-mnist_test.csv')],axis=0)\n",
    "\n",
    "    X=data.iloc[:,1:].to_numpy()\n",
    "    Y=data.iloc[:,:1].to_numpy().ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1,random_state=42,stratify=Y)\n",
    "\n",
    "    return X_test,y_test\n",
    "\n",
    "\n",
    "#Load Pokemon Data\n",
    "def load_pokemon():\n",
    "    directory='Data/Pokemon/'\n",
    "    image_files = glob(os.path.join(directory, '**', '*.*'), recursive=True)\n",
    "\n",
    "    # Initialize a list to store the images as NumPy arrays\n",
    "    features=[]\n",
    "    labels=[]\n",
    "\n",
    "    # Iterate over the Image files\n",
    "    for file in image_files:\n",
    "\n",
    "        #Name of pokemon\n",
    "        label=file.split('\\\\')[1]\n",
    "\n",
    "        #Appending name\n",
    "        labels.append(label)\n",
    "\n",
    "        #Reading images\n",
    "        image = Image.open(file)\n",
    "\n",
    "        #Converting to RGB\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        #Removing color\n",
    "        image=image.convert('L')\n",
    "        \n",
    "        #Resizing images\n",
    "        image=image.resize((48, 48))\n",
    "\n",
    "        #Appending images\n",
    "        features.append(np.array(image).flatten())\n",
    "\n",
    "    #Converting to Numpy\n",
    "    labels=np.array(labels)\n",
    "    features=np.array(features)\n",
    "\n",
    "    pokemon_names=LabelEncoder()\n",
    "    labels_encoded=pokemon_names.fit_transform(labels)\n",
    "\n",
    "    return features,labels_encoded,pokemon_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:14.161144Z",
     "start_time": "2024-03-23T08:30:14.153144Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_algorithim(X,Y,dataset_name,algorithim_name,algorithim,params,predictor_metric=accuracy_score,grid_search_metric=accuracy_scorer,predictor_metric_name='accuracy',return_needed=False):\n",
    "\n",
    "    '''Function to score algorithim\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.array): All features\n",
    "    Y (np.array): All labels\n",
    "    dataset_name (str): Dataset name\n",
    "    algorithim_name (str): Algorithim name\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim being experimented on\n",
    "    params (dict): Parameter dictionary for GridSearch\n",
    "    predictor_metric (sklearn.metric): Metric to score algorotihim on\n",
    "    grid_search_metric (sklearn.metric): Metric used by GridSearchCV\n",
    "    predictor_metric_name (str): Metric Name for predictor metric\n",
    "    return_needed (bool): Bool if fit model needed to be returned    \n",
    "\n",
    "    Returns:\n",
    "    glf (sklearn.model/NNnet): Fit model'''\n",
    "\n",
    "    standardize=False\n",
    "    if 'KNN' in algorithim_name.upper() or 'SVM' in algorithim_name.upper() or 'NET' in algorithim_name.upper():\n",
    "        standardize=True\n",
    "    \n",
    "    if 'NET' in algorithim_name.upper():\n",
    "        n_jobs=1\n",
    "    else:\n",
    "        n_jobs=-1\n",
    "\n",
    "\n",
    "    train,test=split_data(X,Y,valid=False,standardize=standardize)\n",
    "\n",
    "    glf_cv=GridSearchCV(algorithim,param_grid=params,verbose=0,n_jobs=n_jobs,cv=3,scoring=grid_search_metric,refit=True)\n",
    "\n",
    "    rep=10\n",
    "    start_time=time.time()\n",
    "    glf_cv.fit(train[0],train[1])\n",
    "\n",
    "    glf=glf_cv.best_estimator_\n",
    "    \n",
    "    for i in range(rep):\n",
    "        glf.fit(train[0],train[1])\n",
    "    time_delay_train=(time.time()-start_time)/rep\n",
    "\n",
    "    y_train_pred=glf.predict(train[0])\n",
    "    \n",
    "    if 'F' in predictor_metric_name.upper():\n",
    "        train_score = predictor_metric(y_pred=glf.predict(train[0]),y_true=train[1],beta=BETA)\n",
    "        test_score = predictor_metric(y_pred=glf.predict(test[0]),y_true=test[1],beta=BETA)\n",
    "    else:\n",
    "        train_score = predictor_metric(y_pred=glf.predict(train[0]),y_true=train[1])\n",
    "        test_score = predictor_metric(y_pred=glf.predict(test[0]),y_true=test[1])\n",
    "\n",
    "    start_time=time.time()\n",
    "    for i in range(rep):\n",
    "        y_test_pred=glf.predict(test[0])\n",
    "    time_delay_infer=(time.time()-start_time)/rep\n",
    "\n",
    "    print('{} Final Results'.format(dataset_name))\n",
    "    print('\\n')\n",
    "    print(glf_cv.best_params_)\n",
    "    print('\\n')\n",
    "    print('Average time to train the ideal {} was {:.3f} seconds'.format(algorithim_name,time_delay_train))\n",
    "    print('Average time to infer the ideal {} was {:.3f} seconds'.format(algorithim_name,time_delay_infer))\n",
    "    print('\\n')\n",
    "    print('The result on the training data for the ideal {} algorithim is a {} {} score'.format(algorithim_name,train_score,predictor_metric_name))\n",
    "    print('The result on the test data for the ideal {} algorithim is a {} {} score'.format(algorithim_name,test_score,predictor_metric_name))\n",
    "    \n",
    "    if return_needed:\n",
    "        return glf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:15.312415Z",
     "start_time": "2024-03-23T08:30:15.295411Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(X,Y,valid=True,standardize=False):\n",
    "\n",
    "    '''\n",
    "    Split the data between train, test and optional validation dataset\n",
    "\n",
    "    Parameters:\n",
    "    X (np.array): X features\n",
    "    Y (np.rray): Labels\n",
    "    valid (bool): Split into validation dataset \n",
    "    standardize (bool): Whether to standardize the data (introduces bias as Sklearn Standard Scaler is trained only on the train data)\n",
    "\n",
    "    Returns:\n",
    "    train (list): np.array list of train\n",
    "    valid (list): optional np.array list of validation\n",
    "    test (list): np.array list of test\n",
    "    '''\n",
    "    \n",
    "    #Now let's split the data between test and train, we'll use the standard 80/20 split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=42)\n",
    "    \n",
    "    if valid:\n",
    "        #We'll also split the data between train and validation, we'll again use the standard 80/20 split\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2,random_state=42)\n",
    "        \n",
    "        if standardize:\n",
    "            sklr=StandardScaler()\n",
    "            X_train=sklr.fit_transform(X_train)\n",
    "            X_valid=sklr.transform(X_valid)\n",
    "            X_test=sklr.transform(X_test)\n",
    "        return [X_train,y_train],[X_valid,y_valid],[X_test,y_test]\n",
    "\n",
    "    if standardize:\n",
    "        sklr=StandardScaler()\n",
    "        X_train=sklr.fit_transform(X_train)\n",
    "        X_test=sklr.transform(X_test)\n",
    "    return [X_train,y_train],[X_test,y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:37.994556Z",
     "start_time": "2024-03-23T08:30:16.361660Z"
    }
   },
   "outputs": [],
   "source": [
    "#Heart\n",
    "X_heart,Y_heart,heart_index=load_heart_data()\n",
    "train_heart,test_heart=split_data(X_heart,Y_heart,valid=False)\n",
    "\n",
    "#Fashion MNIST\n",
    "X_fm,Y_fm=load_fmnist()\n",
    "train_fm,test_fm=split_data(X_fm,Y_fm,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:30:38.186601Z",
     "start_time": "2024-03-23T08:30:37.995556Z"
    }
   },
   "outputs": [],
   "source": [
    "#Standardizing\n",
    "sklr_poke=StandardScaler()\n",
    "sklr_heart=StandardScaler()\n",
    "sklr_fm=StandardScaler()\n",
    "\n",
    "X_poke_scaler=StandardScaler()\n",
    "X_heart_scaler=StandardScaler()\n",
    "X_fm_scaler=StandardScaler()\n",
    "\n",
    "#Heart\n",
    "X_heart_scaled=X_heart_scaler.fit_transform(X_heart)\n",
    "\n",
    "train_heart_standardized=train_heart.copy()\n",
    "\n",
    "test_heart_standardized=test_heart.copy()\n",
    "\n",
    "train_heart_standardized[0]=sklr_heart.fit_transform(train_heart[0])\n",
    "test_heart_standardized[0]=sklr_heart.transform(test_heart[0])\n",
    "\n",
    "#FMNIST\n",
    "X_fm_scaled=X_fm_scaler.fit_transform(X_fm)\n",
    "\n",
    "train_fm_standardized=train_fm.copy()\n",
    "train_fm_unstandardized=test_fm.copy()\n",
    "\n",
    "test_fm_standardized=test_fm.copy()\n",
    "\n",
    "train_fm_standardized[0]=sklr_fm.fit_transform(train_fm[0])\n",
    "test_fm_standardized[0]=sklr_fm.transform(test_fm[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FMNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-23T14:41:40.914860Z"
    }
   },
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "n_list=np.arange(1,20)\n",
    "dataset_name='FMNIST'\n",
    "dataset=X_fm_scaled\n",
    "labels_og=Y_fm\n",
    "algorithm_name='EM'\n",
    "cluster_graphs=4\n",
    "repeats=3\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2)\n",
    "dataset_2d=converter.fit_transform(dataset)\n",
    "\n",
    "#Average of 3 runs \n",
    "silhouette_scores=np.zeros(shape=len(n_list))\n",
    "wcss_scores=np.zeros(shape=len(n_list))\n",
    "BIC_scores=np.zeros(shape=len(n_list))\n",
    "AIC_scores=np.zeros(shape=len(n_list))\n",
    "EM_dict={}\n",
    "\n",
    "for r in [11*(1+i) for i in range(repeats)]:\n",
    "\n",
    "    #Will be calculated for each run for plots\n",
    "    all_labels=[]\n",
    "    \n",
    "    #Assigning Clusters\n",
    "    for i,n in enumerate(n_list):\n",
    "        #Creating Gaussian\n",
    "        em=GaussianMixture(n_components=n,random_state=r)\n",
    "\n",
    "        #Fitted labels dataset\n",
    "        em.fit(dataset)\n",
    "\n",
    "        #Predictions\n",
    "        labels=em.predict(dataset)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "        #Calculating Silhouette Score\n",
    "        if n==1: \n",
    "            pass\n",
    "        else:\n",
    "            silhouette_scores[i]=silhouette_score(dataset,labels)/repeats\n",
    "\n",
    "        #WCSS Score\n",
    "        wcss_scores[i]=calculate_wcss(dataset,labels,em.means_)/repeats\n",
    "\n",
    "        #BIC Scores\n",
    "        BIC_scores[i]=em.bic(dataset)/repeats\n",
    "\n",
    "        #AIC\n",
    "        AIC_scores[i]=em.aic(dataset)/repeats\n",
    "\n",
    "        #Saving EM models for final Run \n",
    "        EM_dict[n]=em\n",
    "\n",
    "    #Plotting 2d color coded with labels \n",
    "    fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "    fig.set_size_inches(35,5)\n",
    "\n",
    "    plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "\n",
    "        if c>-1:\n",
    "            #Predictions\n",
    "            labels=all_labels[c+1]\n",
    "\n",
    "            #Plotting\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('{} : COMPONENTS'.format(n_list[c+1]));\n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "        else:\n",
    "\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('Dataset');      \n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Silhouette_scores & Clusters')\n",
    "plt.plot(n_list[1:],silhouette_scores[1:])\n",
    "plt.scatter(n_list[1:], silhouette_scores[1:], s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xticks(n_list[1:])\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Silhouette_scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(n_list)\n",
    "plt.xlabel('N_clusters')\n",
    "ax_2=plt.gca().twinx()\n",
    "create_elbow_plot(n_list,wcss_scores,ax_2)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting BIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('BIC Score & Clusters')\n",
    "plt.plot(n_list,BIC_scores)\n",
    "plt.scatter(n_list, BIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xticks(n_list)\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "\n",
    "#Plotting AIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('AIC Score & Clusters')\n",
    "plt.plot(n_list,AIC_scores)\n",
    "plt.scatter(n_list, AIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.ylabel('AIC Score')\n",
    "plt.xticks(n_list)\n",
    "plt.xlabel('N_clusters')\n",
    "ax_2=plt.gca().twinx()\n",
    "create_elbow_plot(n_list,AIC_scores,ax_2)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Number of Clusters\n",
    "#Choosing 5 as inspecting graphs + domain knowledge indicates 7 is closer to reality \n",
    "best_clusters_d1=8\n",
    "\n",
    "#Plotting the Mean Cluster Image from the ideal number of customers\n",
    "#Best EM model\n",
    "best_em_d1=GaussianMixture(n_components=best_clusters_d1,random_state=21)\n",
    "best_em_d1.fit(X_fm_scaled)\n",
    "#Plotting the best cluster means\n",
    "\n",
    "titles=[f'Centroid {i+1}' for i in range(len(best_em_d1.means_))]\n",
    "centroids_viz =best_em_d1.means_.reshape(best_clusters_d1,28,28)\n",
    "plot_gallery(centroids_viz,titles, 28, 28,n_row=1,n_col=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_dict_em=EM_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    #Evaluation Normalized MI Score\n",
    "    best_em_d1=GaussianMixture(n_components=best_clusters_d1,random_state=21)\n",
    "    mi_score=normalized_mutual_info_score(labels_og,best_em_d1.predict(dataset))\n",
    "    print(f'The Normalized Mutual Info Score for the best clustering is {mi_score}') \n",
    "\n",
    "    #Evaluating Adjusted Rand Score Rand Score\n",
    "    aj_score=adjusted_rand_score(labels_og,best_em_d1.predict(dataset))\n",
    "    print(f'The Adjusted Rand Score for the best clustering is {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(dataset)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(15, 5))\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations EM Clustering Results with Labels (FMINST)',fontsize=18)\n",
    "LABS=[labels_og,best_em_d1.predict(dataset)]\n",
    "\n",
    "for lab in np.unique(labels_og):\n",
    "    x,y=dataset_transformed[labels_og==lab][:,0].mean(),dataset_transformed[labels_og==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(labels_og):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(dataset[labels_og==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heart Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T14:41:40.913859Z",
     "start_time": "2024-03-23T14:41:39.462519Z"
    }
   },
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "n_list=np.arange(1,20)\n",
    "dataset_name='Heart Disease'\n",
    "dataset=X_heart_scaled\n",
    "labels_og=Y_heart\n",
    "algorithm_name='EM'\n",
    "cluster_graphs=4\n",
    "repeats=3\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2)\n",
    "dataset_2d=converter.fit_transform(dataset)\n",
    "\n",
    "#Average of 3 runs \n",
    "silhouette_scores=np.zeros(shape=len(n_list))\n",
    "wcss_scores=np.zeros(shape=len(n_list))\n",
    "BIC_scores=np.zeros(shape=len(n_list))\n",
    "AIC_scores=np.zeros(shape=len(n_list))\n",
    "EM_dict={}\n",
    "\n",
    "for r in [21*(1+i) for i in range(repeats)]:\n",
    "\n",
    "    #Will be calculated for each run for plots\n",
    "    all_labels=[]\n",
    "    \n",
    "    #Assigning Clusters\n",
    "    for i,n in enumerate(n_list):\n",
    "        #Creating Gaussian\n",
    "        em=GaussianMixture(n_components=n,random_state=r)\n",
    "\n",
    "        #Fitted labels dataset\n",
    "        em.fit(dataset)\n",
    "\n",
    "        #Predictions\n",
    "        labels=em.predict(dataset)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "        #Calculating Silhouette Score\n",
    "        if n==1:\n",
    "            pass\n",
    "        else:\n",
    "            silhouette_scores[i]=silhouette_score(dataset,labels)/repeats\n",
    "\n",
    "        #WCSS Score\n",
    "        wcss_scores[i]=calculate_wcss(dataset,labels,em.means_)/repeats\n",
    "\n",
    "        #BIC Scores\n",
    "        BIC_scores[i]=em.bic(dataset)/repeats\n",
    "\n",
    "        #AIC\n",
    "        AIC_scores[i]=em.aic(dataset)/repeats\n",
    "\n",
    "        #Saving EM models for final Run \n",
    "        EM_dict[n]=em\n",
    "\n",
    "    #Plotting 2d color coded with labels \n",
    "    fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "    fig.set_size_inches(35,5)\n",
    "\n",
    "    plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "\n",
    "        if c>-1:\n",
    "            #Predictions\n",
    "            labels=all_labels[c+1]\n",
    "\n",
    "            #Plotting\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('{} : COMPONENTS'.format(n_list[c+1]));\n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "        else:\n",
    "\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('Dataset');      \n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Silhouette_scores & Clusters')\n",
    "plt.plot(n_list[1:],silhouette_scores[1:])\n",
    "plt.scatter(n_list[1:],silhouette_scores[1:], s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xticks(n_list[1:])\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Silhouette_scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(n_list)\n",
    "plt.xlabel('N_clusters')\n",
    "ax_2=plt.gca().twinx()\n",
    "create_elbow_plot(n_list,wcss_scores,ax_2)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting BIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('BIC Score & Clusters')\n",
    "plt.plot(n_list,BIC_scores)\n",
    "plt.scatter(n_list, BIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xticks(n_list)\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "\n",
    "#Plotting AIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('AIC Score & Clusters')\n",
    "plt.plot(n_list,AIC_scores)\n",
    "plt.scatter(n_list, AIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.ylabel('AIC Score')\n",
    "plt.xticks(n_list)\n",
    "plt.xlabel('N_clusters')\n",
    "ax_2=plt.gca().twinx()\n",
    "create_elbow_plot(n_list,AIC_scores,ax_2)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the best number of cluster and plotting pair plots \n",
    "dataset=X_heart_scaled\n",
    "#Best Number of Clusters\n",
    "best_clusters_em_d2=7\n",
    "\n",
    "for i in range(5):\n",
    "    #Best EM model\n",
    "    best_em_d2=GaussianMixture(n_components=best_clusters_em_d2,random_state=63)\n",
    "\n",
    "    best_em_d2.fit(dataset)\n",
    "    #Getting labels for best cluster\n",
    "    labels=best_em_d2.predict(dataset)\n",
    "\n",
    "    #Evaluation Normalized MI Score\n",
    "    mi_score=normalized_mutual_info_score(labels_og,best_em_d2.predict(dataset))\n",
    "    print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "    #Evaluating Adjusted Rand Score Rand Score\n",
    "    aj_score=adjusted_rand_score(labels_og,best_em_d2.predict(dataset))\n",
    "    print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Result\n",
    "#Visualizing\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(dataset)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(15, 5))\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations EM Clustering (HEART)',fontsize=18)\n",
    "LABS=[labels_og,best_em_d2.predict(dataset)]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS=2\n",
    "#Plotting with actual labels\n",
    "\n",
    "TWO_FIXED=['Oldpeak','MaxHR']\n",
    "ind_1=heart_index[TWO_FIXED[0]]\n",
    "ind_2=heart_index[TWO_FIXED[1]]\n",
    "\n",
    "filter_cols=[i for i in range(len(heart_index)) if i not in (ind_1,ind_2)]\n",
    "filtered_heart=X_heart_scaled[:,filter_cols]\n",
    "reversed_dict={i:k for k,i in heart_index.items()}\n",
    "\n",
    "fig,axes=plt.subplots(ROWS,4,subplot_kw={'projection': '3d'})\n",
    "fig.set_size_inches(ROWS*(len(reversed_dict)-4)/ROWS,5*ROWS)\n",
    "plt.suptitle(f'Visualizing Best Clusters By Feature',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c%2==0:\n",
    "        #Plot Actual With Binary\n",
    "        sc=ax.scatter(X_heart_scaled[:,ind_1],X_heart_scaled[:,ind_2],filtered_heart[:,c],c=labels_og,cmap='viridis',alpha=0.75,s=10)\n",
    "        ax.set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        ax.set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        ax.set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        ax.set_title(f'Actual Clusters for {reversed_dict[filter_cols[c]]}')\n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "        \n",
    "        # Remove fill\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "        ax.grid(False)\n",
    "\n",
    "        #Plot Predicted Cluster\n",
    "        sc=fig.axes[c+1].scatter(X_heart_scaled[:,ind_1],X_heart_scaled[:,ind_2],filtered_heart[:,c],c=labels,cmap='viridis',alpha=0.75,s=10)\n",
    "        fig.axes[c+1].set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_title(f'Predicted Clusters for {reversed_dict[filter_cols[c]]}')\n",
    "        fig.axes[c+1].legend(*sc.legend_elements(), title='clusters')\n",
    "        \n",
    "        # Remove fill\n",
    "        fig.axes[c+1].xaxis.pane.fill = False\n",
    "        fig.axes[c+1].yaxis.pane.fill = False\n",
    "        fig.axes[c+1].zaxis.pane.fill = False\n",
    "        fig.axes[c+1].xaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].yaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].zaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].set_xticks([])\n",
    "        fig.axes[c+1].set_yticks([])\n",
    "        fig.axes[c+1].set_zticks([])\n",
    "        fig.axes[c+1].grid(False)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_dict_em=EM_dict.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Modes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FMNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T14:41:39.461518Z",
     "start_time": "2024-03-23T08:54:16.559664Z"
    }
   },
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "dataset_name='FMNIST'\n",
    "dataset=X_fm_scaled\n",
    "labels_og=Y_fm\n",
    "algorithm_name='K-MODES'\n",
    "categorical_cols=None\n",
    "\n",
    "experiment_km_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name,categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "dataset=X_fm_scaled\n",
    "best_clusters=2\n",
    "categorical_cols=None\n",
    "data_categorized=categorizer(dataset,categorical_cols)\n",
    "labels_og=Y_fm\n",
    "labels=labels_og\n",
    "\n",
    "for i in range(5):\n",
    "    #Kmodes\n",
    "    best_km_d1 = KModes(n_clusters=best_clusters, init='Huang', n_init=3, verbose=0)\n",
    "\n",
    "    #Best KM model\n",
    "    best_km_d1.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    labels=best_km_d1.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "    print(\"DATASET 1 KMODES RESULTS \\n\\n\")\n",
    "\n",
    "    #Evaluation Normalized MI Score\n",
    "    mi_score=normalized_mutual_info_score(labels_og,best_km_d1.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1])))\n",
    "    print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "    #Evaluating Adjusted Rand Score Rand Score\n",
    "    aj_score=adjusted_rand_score(labels_og,best_km_d1.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1])))\n",
    "    print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Mean Cluster Image from the ideal number of clusters \n",
    "\n",
    "#Visualizing Results from KFOLDS\n",
    "tsne_vis=TSNE(n_components=2,perplexity=30,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(data_categorized)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (FMINST)',fontsize=18)\n",
    "LABS=[labels_og,best_km_d1.predict(data_categorized,categorical=data_categorized.shape[-1])]\n",
    "\n",
    "for lab in np.unique(labels_og):\n",
    "    x, y = dataset_transformed[labels_og == lab][:, 0].mean(), dataset_transformed[labels_og == lab][:, 1].mean()\n",
    "    position_dict[lab] = [x, y]\n",
    "\n",
    "axis_int = 0\n",
    "for ax, labels_ in zip(axes, LABS):\n",
    "    artists = []\n",
    "\n",
    "    for l in np.unique(labels_):\n",
    "        # Choose color from a colormap\n",
    "        color = plt.cm.viridis(l / len(np.unique(labels_)))\n",
    "        ax.scatter(dataset_transformed[:, 0][labels_ == l], dataset_transformed[:, 1][labels_ == l],\n",
    "                   label=l, zorder=1, color=color)\n",
    "\n",
    "    # Overlay each image within the scatter plot\n",
    "    for lab in np.unique(labels_og):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im = OffsetImage(X_fm_scaled[labels_og == lab][0].reshape(28, 28), cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int == 0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int += 1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Mean Cluster Image from the ideal number of clusters \n",
    "\n",
    "#Visualizing Results from KFOLDS\n",
    "tsne_vis=TSNE(n_components=2,perplexity=30,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(data_categorized)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (FMINST) - CATEGORIZED',fontsize=18)\n",
    "LABS=[labels_og,best_km_d1.predict(data_categorized,categorical=data_categorized.shape[-1])]\n",
    "\n",
    "for lab in np.unique(labels_og):\n",
    "    x, y = dataset_transformed[labels_og == lab][:, 0].mean(), dataset_transformed[labels_og == lab][:, 1].mean()\n",
    "    position_dict[lab] = [x, y]\n",
    "\n",
    "axis_int = 0\n",
    "for ax, labels_ in zip(axes, LABS):\n",
    "    artists = []\n",
    "\n",
    "    for l in np.unique(labels_):\n",
    "        # Choose color from a colormap\n",
    "        color = plt.cm.viridis(l / len(np.unique(labels_)))\n",
    "        ax.scatter(dataset_transformed[:, 0][labels_ == l], dataset_transformed[:, 1][labels_ == l],\n",
    "                   label=l, zorder=1, color=color)\n",
    "\n",
    "    # Overlay each image within the scatter plot\n",
    "    for lab in np.unique(labels_og):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im = OffsetImage(X_fm_scaled[labels_og == lab][0].reshape(28, 28), cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int == 0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int += 1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Mean Cluster Image from the ideal number of clusters \n",
    "#Visualizing Results from KFOLDS\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(X_fm_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(25, 5))\n",
    "\n",
    "position_dict = {}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (FMINST)', fontsize=18)\n",
    "LABS=[labels_og,best_km_d1.predict(data_categorized,categorical=data_categorized.shape[-1])]\n",
    "\n",
    "for lab in np.unique(labels_og):\n",
    "    x, y = dataset_transformed[labels_og == lab][:, 0].mean(), dataset_transformed[labels_og == lab][:, 1].mean()\n",
    "    position_dict[lab] = [x, y]\n",
    "\n",
    "axis_int = 0\n",
    "for ax, labels_ in zip(axes, LABS):\n",
    "    artists = []\n",
    "\n",
    "    for l in np.unique(labels_):\n",
    "        # Choose color from a colormap\n",
    "        color = plt.cm.viridis(l / len(np.unique(labels_)))\n",
    "        ax.scatter(dataset_transformed[:, 0][labels_ == l], dataset_transformed[:, 1][labels_ == l],\n",
    "                   label=l, zorder=1, color=color)\n",
    "\n",
    "    # Overlay each image within the scatter plot\n",
    "    for lab in np.unique(labels_og):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im = OffsetImage(X_fm_scaled[labels_og == lab][0].reshape(28, 28), cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int == 0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int += 1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_dict_km=KM_dict.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heart Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:54:16.557664Z",
     "start_time": "2024-03-23T08:53:41.774242Z"
    }
   },
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "n_list=np.arange(1,20)\n",
    "dataset_name='Heart Disease'\n",
    "dataset=X_heart_scaled\n",
    "labels_og=Y_heart\n",
    "algorithm_name='K-MODES'\n",
    "cluster_graphs=4\n",
    "categorical=[i for i,k in enumerate(heart_index.keys()) if '%' in k]\n",
    "\n",
    "data_categorized=categorizer(dataset,categorical)\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2)\n",
    "dataset_2d=converter.fit_transform(data_categorized)\n",
    "silhouette_scores=[]\n",
    "all_labels=[]\n",
    "wcss_scores=[]\n",
    "KM_cost=[]\n",
    "KM_dict={}\n",
    "\n",
    "\n",
    "#Assigning Clusters\n",
    "for n in tqdm(n_list):\n",
    "    #Creating KMODS\n",
    "    km = KModes(n_clusters=n, init='Huang', n_init=5, verbose=0)\n",
    "\n",
    "    #Fitted labels dataset\n",
    "    km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "    #Predictions\n",
    "    labels=km.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    all_labels.append(labels)\n",
    "    \n",
    "    #silhouette score\n",
    "    #Calculating Silhouette Score\n",
    "    if n==1:\n",
    "        pass\n",
    "    else:\n",
    "        silhouette_scores.append(silhouette_score(data_categorized,labels))\n",
    "        \n",
    "        \n",
    "    #WCSS Score\n",
    "    wcss_scores.append(calculate_wcss(data_categorized,labels,km.cluster_centroids_))\n",
    "\n",
    "    KM_dict[n]=km\n",
    "\n",
    "    KM_cost.append(km.cost_)\n",
    "\n",
    "#Plotting 2d color coded with labels \n",
    "fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "fig.set_size_inches(35,5)\n",
    "\n",
    "plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c>-1:\n",
    "        #Predictions\n",
    "        labels=all_labels[c+1]\n",
    "\n",
    "        #Plotting\n",
    "        sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "        #ax.legend()\n",
    "        ax.set_ylabel('Y');\n",
    "        ax.set_xlabel('X');\n",
    "        ax.set_title('{} : COMPONENTS'.format(n_list[c+1]));\n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    else:\n",
    "\n",
    "        sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "        #ax.legend()\n",
    "        ax.set_ylabel('Y');\n",
    "        ax.set_xlabel('X');\n",
    "        ax.set_title('Dataset');      \n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xticks(n_list)\n",
    "ax_2=plt.gca().twinx()\n",
    "create_elbow_plot(n_list,wcss_scores,ax_2)\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting Cost\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Cost & Clusters')\n",
    "plt.plot(n_list,KM_cost)\n",
    "plt.scatter(n_list, KM_cost, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xticks(n_list)\n",
    "ax_2=plt.gca().twinx()\n",
    "create_elbow_plot(n_list,KM_cost,ax_2)\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Silhouette_scores & Clusters')\n",
    "plt.plot(n_list[1:],silhouette_scores)\n",
    "plt.scatter(n_list[1:], silhouette_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xticks(n_list[1:])\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Silhouette_scores')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Results\n",
    "labels_og=Y_heart\n",
    "\n",
    "categorical=[i for i,k in enumerate(heart_index.keys()) if '%' in k]\n",
    "data_categorized=categorizer(X_heart_scaled,categorical)\n",
    "\n",
    "dataset=data_categorized\n",
    "\n",
    "\n",
    "REPEATS=5\n",
    "\n",
    "#Selecting N based on Plots\n",
    "best_clusters=2\n",
    "best_km_d2=KModes(n_clusters=best_clusters, init='Huang', n_init=5, verbose=0)\n",
    "\n",
    "best_km_d2.fit(dataset,categorical=np.arange(dataset.shape[-1]))\n",
    "#best_em_d1=EM_dict[best_clusters_PCA_d1]\n",
    "\n",
    "print(\"DATASET 2 KMODES RESULTS \\n\\n\")\n",
    "\n",
    "#Evaluation Normalized MI & Adjusted Rand Score Score\n",
    "\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "\n",
    "for i in range(REPEATS):\n",
    "    mi_score+=normalized_mutual_info_score(labels_og,best_km_d2.predict(dataset,categorical=np.arange(dataset.shape[-1])))/REPEATS\n",
    "    aj_score+=adjusted_rand_score(labels_og,best_km_d2.predict(dataset,categorical=np.arange(dataset.shape[-1])))/REPEATS\n",
    "\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWO_FIXED=['Oldpeak','MaxHR']\n",
    "LABELS=labels_og\n",
    "dataset=data_categorized\n",
    "ind_1=heart_index[TWO_FIXED[0]]\n",
    "ind_2=heart_index[TWO_FIXED[1]]\n",
    "pred_labels=best_km_d2.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "\n",
    "filter_cols=[i for i in range(len(heart_index)) if i not in (ind_1,ind_2)]\n",
    "filtered_heart=X_heart_scaled[:,filter_cols]\n",
    "reversed_dict={i:k for k,i in heart_index.items()}\n",
    "\n",
    "fig,axes=plt.subplots(ROWS,4,subplot_kw={'projection': '3d'})\n",
    "fig.set_size_inches(ROWS*(len(reversed_dict)-4)/ROWS,5*ROWS)\n",
    "plt.suptitle(f'Visualizing K-MODES Clustering Results By Features (ISOMAP HEART)',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c%2==0:\n",
    "        #Plot Actual With Binary\n",
    "        for lab in np.unique(LABELS):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(LABELS)))\n",
    "            ax.scatter(dataset[:,ind_1][LABELS==lab],dataset[:,ind_2][LABELS==lab],filtered_heart[:,c][LABELS==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        ax.set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        ax.set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        ax.set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        ax.set_title(f'Actual Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "        ax.grid(False)\n",
    "\n",
    "        #Plot Predicted Cluster\n",
    "        for lab in np.unique(pred_labels):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(pred_labels)))\n",
    "            fig.axes[c+1].scatter(dataset[:,ind_1][pred_labels==lab],dataset[:,ind_2][pred_labels==lab],filtered_heart[:,c][pred_labels==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        fig.axes[c+1].set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_title(f'Predicted Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        fig.axes[c+1].legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        fig.axes[c+1].xaxis.pane.fill = False\n",
    "        fig.axes[c+1].yaxis.pane.fill = False\n",
    "        fig.axes[c+1].zaxis.pane.fill = False\n",
    "        fig.axes[c+1].xaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].yaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].zaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].set_xticks([])\n",
    "        fig.axes[c+1].set_yticks([])\n",
    "        fig.axes[c+1].set_zticks([])\n",
    "        fig.axes[c+1].grid(False)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset, columns=heart_index)\n",
    "df['Cluster']=labels\n",
    "\n",
    "sns.pairplot(df, hue='Cluster', palette='viridis', diag_kind='kde')\n",
    "plt.suptitle('Pair Plot of Best Clusters By Feature', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FMINST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA on the FMINST Dataset\n",
    "\n",
    "#Remembering that the number of features for FMINST are 784\n",
    "dataset=X_fm_scaled\n",
    "chosen_n_pca_d1=None\n",
    "\n",
    "pca_size=range(250)\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "LIMIT=0.875\n",
    "\n",
    "explained_variance=np.zeros(len(pca_size))\n",
    "reconstruction_error=np.zeros(len(pca_size))\n",
    "\n",
    "for r in [21*(1+i) for i in range(repeats)]:\n",
    "\n",
    "    for i_ind,i in enumerate(pca_size):\n",
    "        p_comp=PCA(n_components=i,random_state=r)\n",
    "        p_comp.fit(dataset)\n",
    "        explained_variance[i_ind]+=sum(p_comp.explained_variance_ratio_)/repeats\n",
    "\n",
    "        #Calcualting Reconstruction Error\n",
    "        dataset_transformed=p_comp.transform(dataset)\n",
    "        dataset_reconstructed=p_comp.inverse_transform(dataset_transformed)\n",
    "        reconstruction_error[i_ind]+=mean_squared_error(dataset,dataset_reconstructed)/repeats\n",
    "\n",
    "#Plotting explained variance \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Explained Variance & Number of PCA Components (FMINST)')\n",
    "plt.plot(pca_size,explained_variance)\n",
    "plt.scatter(pca_size, explained_variance, s=10, c='blue', marker='o', edgecolors='black')\n",
    "plt.axhline(y=LIMIT,color='red',linestyle='--')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Explained Variance')\n",
    "\n",
    "#Plotting reconstruction error \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Reconstruction Error & Number of PCA Components (FMINST)')\n",
    "plt.plot(pca_size,reconstruction_error)\n",
    "plt.scatter(pca_size, reconstruction_error, s=10, c='blue', marker='o', edgecolors='black')\n",
    "plt.axvline(np.argwhere(explained_variance>LIMIT)[0][0],color='red',linestyle='--',label=f'chosen Components')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "\n",
    "print('The lowest number of componenets to exceed the limit of {} is {}'.format(LIMIT,np.argwhere(explained_variance>LIMIT)[0][0]))\n",
    "chosen_n_pca_d1=np.argwhere(explained_variance>LIMIT)[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing PCA Results for FMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the images for the componenets chosen\n",
    "chosen_n_pca_d1=75\n",
    "dataset=X_fm_scaled\n",
    "p_comp=PCA(n_components=chosen_n_pca_d1)\n",
    "p_comp.fit(dataset)\n",
    "titles=[f'Eigenvalue {i+1}' for i in range(chosen_n_pca_d1)]\n",
    "eigenfashion = p_comp.components_.reshape(chosen_n_pca_d1,28,28)\n",
    "plot_gallery(eigenfashion,titles, 28, 28,n_row=1,n_col=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heart Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA on the FMINST Dataset\n",
    "\n",
    "#Remembering that the number of features for FMINST are 784\n",
    "dataset=X_heart_scaled\n",
    "chosen_n_pca_d2=None\n",
    "\n",
    "pca_size=range(20)\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "LIMIT=0.85\n",
    "\n",
    "explained_variance=np.zeros(len(pca_size))\n",
    "reconstruction_error=np.zeros(len(pca_size))\n",
    "\n",
    "for r in [21*(1+i) for i in range(repeats)]:\n",
    "\n",
    "    for i_ind,i in enumerate(pca_size):\n",
    "        p_comp=PCA(n_components=i,random_state=r)\n",
    "        p_comp.fit(dataset)\n",
    "        explained_variance[i_ind]+=sum(p_comp.explained_variance_ratio_)/repeats\n",
    "\n",
    "        #Calcualting Reconstruction Error\n",
    "        dataset_transformed=p_comp.transform(dataset)\n",
    "        dataset_reconstructed=p_comp.inverse_transform(dataset_transformed)\n",
    "        reconstruction_error[i_ind]+=mean_squared_error(dataset,dataset_reconstructed)/repeats\n",
    "\n",
    "#Plotting explained variance \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Explained Variance & Number of PCA Components (HEART)')\n",
    "plt.plot(pca_size,explained_variance)\n",
    "plt.scatter(pca_size, explained_variance, s=10, c='blue', marker='o', edgecolors='black')\n",
    "plt.axhline(y=LIMIT,color='red',linestyle='--')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Explained Variance')\n",
    "\n",
    "#Plotting reconstruction error \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Reconstruction Error & Number of PCA Components (HEART)')\n",
    "plt.plot(pca_size,reconstruction_error)\n",
    "plt.scatter(pca_size, reconstruction_error, s=10, c='blue', marker='o', edgecolors='black')\n",
    "plt.axvline(np.argwhere(explained_variance>LIMIT)[0][0],color='red',linestyle='--',label=f'chosen Components')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "\n",
    "print('The lowest number of components to exceed the limit of {} is {}'.format(LIMIT,np.argwhere(explained_variance>LIMIT)[0][0]))\n",
    "chosen_n_pca_d2=np.argwhere(explained_variance>LIMIT)[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing PCA Results for FMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the images for the componenets chosen\n",
    "dataset=X_heart_scaled\n",
    "labels=Y_heart\n",
    "p_comp=PCA(n_components=chosen_n_pca_d2)\n",
    "p_comp.fit(dataset)\n",
    "PLOT_FACTOR=5\n",
    "\n",
    "#Projecting the dataset into the first 2 dimensions to identify most important features\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "dataset_2d=np.dot(dataset,p_comp.components_[:2,:].T)\n",
    "\n",
    "plt.title('Visualizing PCA & Most Important Features for Heart Dataset')\n",
    "sc=plt.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma',s=5)\n",
    "most_important=np.argsort(np.abs(p_comp.components_[:2]).sum(axis=0))[::-1][:4].ravel()\n",
    "\n",
    "for i in range(dataset.shape[-1]):\n",
    "    if i in most_important:\n",
    "        plt.arrow(0,0,p_comp.components_[:2,i].T[0]*PLOT_FACTOR,p_comp.components_[:2,i].T[1]*PLOT_FACTOR,color='r',alpha=0.5)\n",
    "        plt.text(p_comp.components_[:2,i].T[0]*PLOT_FACTOR*1.15,p_comp.components_[:2,i].T[1]*PLOT_FACTOR*1.15,s=list(heart_index.keys())[i],color = 'r', ha = 'center', va = 'center')\n",
    "    else:\n",
    "        plt.arrow(0,0,p_comp.components_[:2,i].T[0]*PLOT_FACTOR,p_comp.components_[:2,i].T[1]*PLOT_FACTOR,color='g',alpha=0.5)\n",
    "        plt.text(p_comp.components_[:2,i].T[0]*PLOT_FACTOR*1.15,p_comp.components_[:2,i].T[1]*PLOT_FACTOR*1.15,s=list(heart_index.keys())[i],color = 'g', ha = 'center', va = 'center')        \n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(*sc.legend_elements(), title='Label');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FMINST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying ICA on the FMINST Dataset\n",
    "\n",
    "#Remembering that the number of features for FMINST are 784\n",
    "dataset=X_fm_scaled\n",
    "chosen_n_ica_d1=None\n",
    "\n",
    "ica_size=range(1,250)\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "\n",
    "kurtosis=np.zeros(len(ica_size))\n",
    "reconstruction_error=np.zeros(len(ica_size))\n",
    "\n",
    "for i_ind,i in tqdm(enumerate(ica_size)):\n",
    "    for r in [21*(1+i) for i in range(repeats)]:\n",
    "        ica_comp=FastICA(n_components=i,random_state=r,max_iter=500)\n",
    "        ica_comp.fit(dataset)\n",
    "        kurtosis[i_ind]+=scipy.stats.kurtosis(np.abs(ica_comp.transform(dataset))).mean()/repeats\n",
    "\n",
    "        #Getting error\n",
    "        dataset_transformed=ica_comp.transform(dataset)\n",
    "        dataset_reconstructed=ica_comp.inverse_transform(dataset_transformed)\n",
    "        error=mean_squared_error(dataset,dataset_reconstructed)\n",
    "        reconstruction_error[i_ind]+=error/3\n",
    "\n",
    "#Plotting explained variance \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Mean Kurtosis & Number of ICA Components (FMINST)')\n",
    "plt.plot(ica_size,kurtosis)\n",
    "plt.scatter(ica_size, kurtosis, s=10, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Mean Kurtosis')\n",
    "\n",
    "#Plotting reconstruction error \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Reconstruction Error & Number of ICA Components (FMINST)')\n",
    "plt.plot(ica_size,reconstruction_error)\n",
    "plt.scatter(ica_size, reconstruction_error, s=10, c='blue', marker='o', edgecolors='black')\n",
    "plt.axvline(ica_size[np.argmax(kurtosis)],color='red',linestyle='--',label=f'chosen Components')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing ICA Results for FMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the images generated by ICA \n",
    "dataset=X_fm_scaled\n",
    "chosen_n_ica_d1=66\n",
    "ica_ideal_d1=FastICA(n_components=chosen_n_ica_d1,random_state=21,max_iter=500)\n",
    "d1_ica=ica_ideal_d1.fit_transform(dataset)\n",
    "titles=[f'ICA Component {i+1}' for i in range(chosen_n_ica_d1)]\n",
    "plot_gallery(ica_ideal_d1.components_,titles, 28, 28,n_row=1,n_col=8)\n",
    "\n",
    "correlation_matrix = np.corrcoef(d1_ica.T)\n",
    "\n",
    "# Plot heatmap of new dataset \n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(correlation_matrix, cmap='magma', fmt=\".2f\", cbar=True)\n",
    "plt.title('Correlation Matrix of Independent Components (FMINST)')\n",
    "plt.xlabel('Component Index')\n",
    "plt.ylabel('Component Index')\n",
    "plt.show()\n",
    "\n",
    "correlation_matrix = np.corrcoef(dataset.T)\n",
    "\n",
    "# Plot heatmap of old dataset \n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(correlation_matrix, cmap='magma', fmt=\".2f\", cbar=True)\n",
    "plt.title('Correlation Matrix of Original Features')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting \n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(d1_ica)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(25, 5))\n",
    "\n",
    "position_dict = {}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (FMINST)', fontsize=18)\n",
    "LABS=[labels_og]\n",
    "\n",
    "for lab in np.unique(labels_og):\n",
    "    x, y = dataset_transformed[labels_og == lab][:, 0].mean(), dataset_transformed[labels_og == lab][:, 1].mean()\n",
    "    position_dict[lab] = [x, y]\n",
    "\n",
    "axis_int = 0\n",
    "for ax, labels_ in zip(axes, LABS):\n",
    "    artists = []\n",
    "\n",
    "    for l in np.unique(labels_):\n",
    "        # Choose color from a colormap\n",
    "        color = plt.cm.viridis(l / len(np.unique(labels_)))\n",
    "        ax.scatter(dataset_transformed[:, 0][labels_ == l], dataset_transformed[:, 1][labels_ == l],\n",
    "                   label=l, zorder=1, color=color)\n",
    "\n",
    "    # Overlay each image within the scatter plot\n",
    "    for lab in np.unique(labels_og):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im = OffsetImage(X_fm_scaled[labels_og == lab][0].reshape(28, 28), cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int == 0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int += 1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heart Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying ICA on the Heart Dataset\n",
    "\n",
    "#Remembering that the number of features for FMINST are 784\n",
    "dataset=X_heart_scaled\n",
    "chosen_n_ica_d1=None\n",
    "\n",
    "ica_size=range(1,21)\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "\n",
    "kurtosis=np.zeros(len(ica_size))\n",
    "reconstruction_error=np.zeros(len(ica_size))\n",
    "\n",
    "for i_ind,i in tqdm(enumerate(ica_size)):\n",
    "    \n",
    "    for r in [21*(1+i) for i in range(repeats)]:\n",
    "\n",
    "        ica_comp=FastICA(n_components=i,random_state=r,max_iter=100000)\n",
    "        ica_comp.fit(dataset)\n",
    "        kurtosis[i_ind]+=scipy.stats.kurtosis(np.abs(ica_comp.transform(dataset))).mean()/repeats\n",
    "\n",
    "        #Getting error\n",
    "        dataset_transformed=ica_comp.transform(dataset)\n",
    "        dataset_reconstructed=ica_comp.inverse_transform(dataset_transformed)\n",
    "        error=mean_squared_error(dataset,dataset_reconstructed)\n",
    "        reconstruction_error[i_ind]+=error/repeats\n",
    "\n",
    "#Plotting explained variance \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Mean Kurtosis & Number of ICA Components (HEART)')\n",
    "plt.plot(ica_size,kurtosis)\n",
    "plt.scatter(ica_size, kurtosis, s=10, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Mean Kurtosis')\n",
    "\n",
    "#Plotting reconstruction error \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Reconstruction Error & Number of ICA Components (HEART)')\n",
    "plt.plot(ica_size,reconstruction_error)\n",
    "plt.scatter(ica_size, reconstruction_error, s=10, c='blue', marker='o', edgecolors='black')\n",
    "plt.axvline(ica_size[np.argmax(kurtosis)],color='red',linestyle='--',label=f'chosen Components')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing ICA Results for Heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=X_heart_scaled\n",
    "#chosen_n_ica_d2=ica_size[np.argmax(kurtosis)]\n",
    "ica_comp=FastICA(n_components=chosen_n_ica_d2,random_state=r,max_iter=100000)\n",
    "d2_ica=ica_comp.fit_transform(dataset)\n",
    "\n",
    "correlation_matrix = np.corrcoef(d2_ica.T)\n",
    "\n",
    "# Plot heatmap of new dataset \n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(correlation_matrix, cmap='magma', fmt=\".2f\", cbar=True)\n",
    "plt.title('Correlation Matrix of Independent Components (HEART)')\n",
    "plt.xlabel('Component Index')\n",
    "plt.ylabel('Component Index')\n",
    "plt.show()\n",
    "\n",
    "correlation_matrix = np.corrcoef(dataset.T)\n",
    "\n",
    "# Plot heatmap of old dataset \n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(correlation_matrix, cmap='magma', fmt=\".2f\", cbar=True)\n",
    "plt.title('Correlation Matrix of Original Features (HEART)')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Index')\n",
    "plt.show()\n",
    "\n",
    "#Projecting the dataset into the first 2 dimensions\n",
    "fig,axes=plt.subplots(1,2)\n",
    "fig.set_size_inches(20,5)\n",
    "plt.suptitle('Visualizing ICA for Selected Components & Features (HEART)')\n",
    "\n",
    "SELECTION=[[5,6],[6,7]]\n",
    "\n",
    "for ii,ica_select in enumerate(SELECTION):\n",
    "    PLOT_FACTOR=3\n",
    "    ICA_COMP_1=ica_select[0]\n",
    "    ICA_COMP_2=ica_select[1]\n",
    "\n",
    "    filtered_components=ica_comp.components_[[ICA_COMP_1,ICA_COMP_2]]\n",
    "    dataset_2d=np.dot(dataset,filtered_components.T)\n",
    "\n",
    "\n",
    "    sc=axes[ii].scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma',s=5)\n",
    "    most_important=np.argsort(np.abs(filtered_components).sum(axis=0))[::-1][:4].ravel()\n",
    "\n",
    "    for i in range(dataset.shape[-1]):\n",
    "        if i in most_important:\n",
    "            axes[ii].arrow(0,0,filtered_components[:,i].T[0]*PLOT_FACTOR,filtered_components[:,i].T[1]*PLOT_FACTOR,color='r',alpha=0.5)\n",
    "            axes[ii].text(filtered_components[:,i].T[0]*PLOT_FACTOR*1.15,filtered_components[:,i].T[1]*PLOT_FACTOR*1.15,s=list(heart_index.keys())[i],color = 'r', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            pass\n",
    "            #plt.arrow(0,0,ica_comp.components_[:2,i].T[0]*PLOT_FACTOR,ica_comp.components_[:2,i].T[1]*PLOT_FACTOR,color='g',alpha=0.5)\n",
    "            #plt.text(ica_comp.components_[:2,i].T[0]*PLOT_FACTOR*1.15,ica_comp.components_[:2,i].T[1]*PLOT_FACTOR*1.15,s=list(heart_index.keys())[i],color = 'g', ha = 'center', va = 'center')        \n",
    "    axes[ii].set_xlabel(f'ICA Component {ICA_COMP_1}')\n",
    "    axes[ii].set_ylabel(f'ICA Component {ICA_COMP_2}')\n",
    "    axes[ii].legend(*sc.legend_elements(), title='Label');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Projection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FMINST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sample data and calculations\n",
    "dataset=X_fm_scaled\n",
    "rgp_size=range(1,1000)\n",
    "repeats = 3\n",
    "original_distance = scipy.spatial.distance.pdist(dataset)\n",
    "reconstruction_error = np.zeros(len(rgp_size))\n",
    "reconstruction_vals = np.zeros(shape=(len(rgp_size), repeats))\n",
    "pairwise_distance_error = np.zeros(len(rgp_size))\n",
    "pairwise_distance_errors= np.zeros(shape=(len(rgp_size), repeats))\n",
    "\n",
    "for i_ind, i in tqdm(enumerate(rgp_size)):\n",
    "    for ix, r in enumerate([21 * (1 + i) for i in range(repeats)]):\n",
    "        rgp_comp = GaussianRandomProjection(n_components=i, random_state=r)\n",
    "        rgp_comp.fit(dataset)\n",
    "\n",
    "        dataset_transformed = rgp_comp.transform(dataset)\n",
    "        dataset_reconstructed = rgp_comp.inverse_transform(dataset_transformed)\n",
    "        error = mean_squared_error(dataset, dataset_reconstructed)\n",
    "        reconstruction_error[i_ind] += error / 3\n",
    "        reconstruction_vals[i_ind, ix] = error\n",
    "\n",
    "        pairwise_distance_error[i_ind]+=((original_distance-scipy.spatial.distance.pdist(dataset_transformed))**2).sum()/3\n",
    "        pairwise_distance_errors[i_ind, ix]=((original_distance-scipy.spatial.distance.pdist(dataset_transformed))**2).sum()\n",
    "\n",
    "# Plotting reconstruction error with filled space between error bars\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "plt.title('Reconstruction Error & Number of Random Gaussian Projection Components (FMINST)')\n",
    "plt.errorbar(rgp_size, reconstruction_error, yerr=np.std(reconstruction_vals, axis=1), fmt='o', capsize=5,color='orange')\n",
    "plt.fill_between(rgp_size, reconstruction_error - np.std(reconstruction_vals, axis=1), reconstruction_error + np.std(reconstruction_vals, axis=1), alpha=0.2,color='orange',label='error')\n",
    "plt.plot(rgp_size,reconstruction_error,color='orange')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "plt.xticks(rgp_size);\n",
    "\n",
    "# Plotting Pairwise Distance with filled space between error bars\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "plt.title('Pairwise Distance Error & Number of Random Gaussian Projection Components (FMINST)')\n",
    "plt.errorbar(rgp_size, pairwise_distance_error, yerr=np.std(pairwise_distance_errors, axis=1), fmt='o', capsize=5,color='orange')\n",
    "plt.fill_between(rgp_size, pairwise_distance_error - np.std(pairwise_distance_errors, axis=1), pairwise_distance_error + np.std(pairwise_distance_errors, axis=1), alpha=0.2, color='orange',label='error')\n",
    "plt.plot(rgp_size,pairwise_distance_error, color='orange')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Pairwise Distance Error')\n",
    "plt.xticks(rgp_size);\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Distance Metric \n",
    "rgp_comp=GaussianRandomProjection(n_components=johnson_lindenstrauss_min_dim(X_heart_scaled.shape[0],eps=0.323),random_state=r)\n",
    "dataset_transformed=rgp_comp.fit_transform(dataset)\n",
    "\n",
    "dataset_reconstructed=rgp_comp.inverse_transform(dataset_transformed)\n",
    "error=mean_squared_error(dataset,dataset_reconstructed)\n",
    "reconstruction_error=error\n",
    "\n",
    "#Calculating Pairwise Distance\n",
    "pairwise_distance_error=((original_distance-scipy.spatial.distance.pdist(dataset_transformed))**2).sum()/3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing Random Projection Results for FMINST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For selection random projection value, we use a reconstruction error of 0.15 to select the number of freatures for random projection. We chose this as this was the error that was produced by our ICA and PCA reduction techniques after selection.\n",
    "dataset=X_fm_scaled\n",
    "ERROR_CHOICE_D1=0.175\n",
    "#chosen_n_rgp_d1=np.argwhere(reconstruction_error<ERROR_CHOICE_D1)[0][0]\n",
    "chosen_n_rgp_d1=666\n",
    "\n",
    "#Creating the projection\n",
    "d1_rgp_ideal=GaussianRandomProjection(n_components=chosen_n_rgp_d1,random_state=21)\n",
    "d1_rgp_projection=d1_rgp_ideal.fit_transform(dataset)\n",
    "\n",
    "#Visualizing projection with distance \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "pairwise_distances_original = scipy.spatial.distance.pdist(dataset)\n",
    "pairwise_distances_rp = scipy.spatial.distance.pdist(d1_rgp_projection)\n",
    "\n",
    "plt.scatter(pairwise_distances_original, pairwise_distances_rp,s=5,color='orange')\n",
    "plt.plot((0,max(pairwise_distances_rp)),(0,max(pairwise_distances_rp)),color='red',label='Ideal',linestyle='--')\n",
    "plt.xlabel('Pairwise Distances Before Projection')\n",
    "plt.ylabel('Pairwise Distances After Projection')\n",
    "plt.title('Pairwise Distances Before and After Random Projection (FMINST)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Visualizing with Projection\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne_proj = tsne.fit_transform(d1_rgp_projection)\n",
    "X_tsne_original=tsne.fit_transform(dataset)\n",
    "\n",
    "plt.scatter(X_tsne_proj[:, 0], X_tsne_proj[:, 1],color='orange',s=5,label='transformed')\n",
    "plt.scatter(X_tsne_original[:, 0], X_tsne_original[:, 1],color='blue',s=5,label='original')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE Visualization of Random Projection (FMINST)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##Plotting the images generated by Random Projection \n",
    "titles=[f'Rand Proj Comp {i+1}' for i in range(chosen_n_rgp_d1)]\n",
    "plot_gallery(d1_rgp_ideal.components_,titles, 28, 28)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heart Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sample data and calculations\n",
    "dataset = X_heart_scaled\n",
    "rgp_size = range(1, 21)\n",
    "repeats = 3\n",
    "original_distance = scipy.spatial.distance.pdist(dataset)\n",
    "reconstruction_error = np.zeros(len(rgp_size))\n",
    "reconstruction_vals = np.zeros(shape=(len(rgp_size), repeats))\n",
    "pairwise_distance_error = np.zeros(len(rgp_size))\n",
    "pairwise_distance_errors= np.zeros(shape=(len(rgp_size), repeats))\n",
    "\n",
    "for i_ind, i in tqdm(enumerate(rgp_size)):\n",
    "    for ix, r in enumerate([21 * (1 + i) for i in range(repeats)]):\n",
    "        rgp_comp = GaussianRandomProjection(n_components=i, random_state=r)\n",
    "        rgp_comp.fit(dataset)\n",
    "\n",
    "        dataset_transformed = rgp_comp.transform(dataset)\n",
    "        dataset_reconstructed = rgp_comp.inverse_transform(dataset_transformed)\n",
    "        error = mean_squared_error(dataset, dataset_reconstructed)\n",
    "        reconstruction_error[i_ind] += error / 3\n",
    "        reconstruction_vals[i_ind, ix] = error\n",
    "\n",
    "        pairwise_distance_error[i_ind]+=((original_distance-scipy.spatial.distance.pdist(dataset_transformed))**2).sum()/3\n",
    "        pairwise_distance_errors[i_ind, ix]=((original_distance-scipy.spatial.distance.pdist(dataset_transformed))**2).sum()\n",
    "\n",
    "# Plotting reconstruction error with filled space between error bars\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "plt.title('Reconstruction Error & Number of Random Gaussian Projection Components (HEART)')\n",
    "plt.errorbar(rgp_size, reconstruction_error, yerr=np.std(reconstruction_vals, axis=1), fmt='o', capsize=5,color='orange')\n",
    "plt.fill_between(rgp_size, reconstruction_error - np.std(reconstruction_vals, axis=1), reconstruction_error + np.std(reconstruction_vals, axis=1), alpha=0.2,color='orange',label='error')\n",
    "plt.plot(rgp_size,reconstruction_error,color='orange')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "plt.xticks(rgp_size);\n",
    "\n",
    "# Plotting Pairwise Distance with filled space between error bars\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "plt.title('Pairwise Distance Error & Number of Random Gaussian Projection Components (HEART)')\n",
    "plt.errorbar(rgp_size, pairwise_distance_error, yerr=np.std(pairwise_distance_errors, axis=1), fmt='o', capsize=5,color='orange')\n",
    "plt.fill_between(rgp_size, pairwise_distance_error - np.std(pairwise_distance_errors, axis=1), pairwise_distance_error + np.std(pairwise_distance_errors, axis=1), alpha=0.2, color='orange',label='error')\n",
    "plt.plot(rgp_size,pairwise_distance_error, color='orange')\n",
    "plt.xlabel('N_components')\n",
    "plt.ylabel('Pairwise Distance Error')\n",
    "plt.xticks(rgp_size);\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing Random Projection Results for Heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For selection random projection value, we use a reconstruction error of 0.15 to select the number of freatures for random projection. We chose this as this was the error that was produced by our ICA and PCA reduction techniques after selection.\n",
    "dataset=X_heart_scaled\n",
    "ERROR_CHOICE_D2=0.15\n",
    "chosen_n_rgp_d2=np.argwhere(reconstruction_error<ERROR_CHOICE_D2)[0][0]\n",
    "\n",
    "#Creating the projection\n",
    "d2_rgp_ideal=GaussianRandomProjection(n_components=chosen_n_rgp_d2,random_state=21)\n",
    "d2_rgp_projection=d2_rgp_ideal.fit_transform(dataset)\n",
    "\n",
    "#Visualizing projection with distance \n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "pairwise_distances_original = scipy.spatial.distance.pdist(dataset)\n",
    "pairwise_distances_rp = scipy.spatial.distance.pdist(d2_rgp_projection)\n",
    "\n",
    "plt.scatter(pairwise_distances_original, pairwise_distances_rp,s=5,color='orange')\n",
    "plt.plot((0,max(pairwise_distances_rp)),(0,max(pairwise_distances_rp)),color='red',label='Ideal',linestyle='--')\n",
    "plt.xlabel('Pairwise Distances Before Projection')\n",
    "plt.ylabel('Pairwise Distances After Projection')\n",
    "plt.title('Pairwise Distances Before and After Random Projection (HEART)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Visualizing with Projection\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne_proj = tsne.fit_transform(d2_rgp_projection)\n",
    "X_tsne_original=tsne.fit_transform(dataset)\n",
    "\n",
    "plt.scatter(X_tsne_proj[:, 0], X_tsne_proj[:, 1],color='orange',s=5,label='transformed')\n",
    "plt.scatter(X_tsne_original[:, 0], X_tsne_original[:, 1],color='blue',s=5,label='original')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE Visualization of Random Projection (HEART)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FMINST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying UMAP Manifold Learning on the FMINST Dataset\n",
    "\n",
    "#Remembering that the number of features for FMINST are 784\n",
    "\n",
    "dataset=X_fm_scaled\n",
    "chosen_n_umap_d1=None\n",
    "chosen_neighbors_1=None\n",
    "\n",
    "n_comps=range(2,100)\n",
    "num_neighbors=[45]\n",
    "repeats=1\n",
    "ideal_comps=None \n",
    "\n",
    "trustworthiness_scores=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "reconstruction_errors=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "\n",
    "for i,n in tqdm(enumerate(n_comps)):\n",
    "\n",
    "    for ii,neighbors in enumerate(num_neighbors):\n",
    "\n",
    "        umap_comp=umap.UMAP(n_components=n,n_neighbors=neighbors,n_jobs=-1)\n",
    "        dataset_transformed=umap_comp.fit_transform(dataset)\n",
    "\n",
    "        trustworthiness_scores[i,ii]=trustworthiness(dataset,dataset_transformed,n_neighbors=neighbors)\n",
    "        reconstruction_errors[i,ii]=mean_squared_error(dataset,umap_comp.inverse_transform(dataset_transformed))\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Trustworthiness Score, Num Neighbors & N_Components (FMINST)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,trustworthiness_scores[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,trustworthiness_scores[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Trustworthiness Score')\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Reconstruction Error, Num Neighbors & N_Components (FMINST)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,reconstruction_errors[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,reconstruction_errors[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Reconstruction Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying ISOMAP Manifold Learning on the FMINST Dataset\n",
    "\n",
    "dataset=X_fm_scaled\n",
    "chosen_n_iso_map_d1=None\n",
    "chosen_neighbors_d1=None\n",
    "\n",
    "n_comps=range(2,100)\n",
    "num_neighbors=[30,45,60]\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "\n",
    "trustworthiness_scores=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "reconstruction_errors=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "\n",
    "for i,n in tqdm(enumerate(n_comps)):\n",
    "\n",
    "    for ii,neighbors in enumerate(num_neighbors):\n",
    "\n",
    "        iso_map_comp=Isomap(n_components=n,n_neighbors=neighbors,n_jobs=-1)\n",
    "        dataset_transformed=iso_map_comp.fit_transform(dataset)\n",
    "\n",
    "        trustworthiness_scores[i,ii]=trustworthiness(dataset,dataset_transformed,n_neighbors=neighbors,)\n",
    "        reconstruction_errors[i,ii]=iso_map_comp.reconstruction_error()\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Trustworthiness Score, Num Neighbors & N_Components (FMINST)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,trustworthiness_scores[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,trustworthiness_scores[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Trustworthiness Score')\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Reconstruction Error, Num Neighbors & N_Components (FMINST)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,reconstruction_errors[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,reconstruction_errors[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Reconstruction Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating ideal UMAP manifold \n",
    "chosen_n_umap_d1=11\n",
    "chosen_neighbors_d1=45\n",
    "dataset=X_fm_scaled\n",
    "labels=Y_fm\n",
    "\n",
    "#iso_map_d1=umap.UMAP(n_components=chosen_n_iso_map_d1,n_neighbors=chosen_neighbors_d1,n_jobs=-1)\n",
    "iso_map_d1=umap.UMAP(n_components=chosen_n_umap_d1,n_neighbors=chosen_neighbors_d1,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=iso_map_d1.fit_transform(dataset)\n",
    "\n",
    "#Visualizing transformed data\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,3,figsize=(25, 5))\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations of Select UMAP Components (FMINST)',fontsize=18)\n",
    "\n",
    "SELECTION=[[0,1],[0,2],[9,10]]\n",
    "\n",
    "for ax,dims in zip(axes,SELECTION):\n",
    "    artists = []\n",
    "    dim_1=dims[0]\n",
    "    dim_2=dims[1]\n",
    "\n",
    "    for lab in np.unique(Y_fm):\n",
    "        x,y=dataset_transformed[labels==lab][:,dim_1].mean(),dataset_transformed[labels==lab][:,dim_2].mean()\n",
    "        position_dict[lab]=[x,y]\n",
    "\n",
    "    sc=ax.scatter(dataset_transformed[:,dim_1],dataset_transformed[:,dim_2],c=Y_fm,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "    for lab in np.unique(Y_fm):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(dataset[labels==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "        ax.set_title(f'Dimension {dim_1} & Dimension {dim_2}')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating ideal ISOmap manifold \n",
    "chosen_n_iso_map_d1=20\n",
    "chosen_neighbors_d1=45\n",
    "dataset=X_fm_scaled\n",
    "labels=Y_fm\n",
    "\n",
    "#iso_map_d1=umap.UMAP(n_components=chosen_n_iso_map_d1,n_neighbors=chosen_neighbors_d1,n_jobs=-1)\n",
    "iso_map_d1=Isomap(n_components=chosen_n_iso_map_d1,n_neighbors=chosen_neighbors_d1,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=iso_map_d1.fit_transform(dataset)\n",
    "\n",
    "#Visualizing transformed data\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,3,figsize=(25, 5))\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations of Select ISOMAP Components (FMINST)',fontsize=18)\n",
    "\n",
    "SELECTION=[[0,1],[0,2],[9,10]]\n",
    "\n",
    "for ax,dims in zip(axes,SELECTION):\n",
    "    artists = []\n",
    "    dim_1=dims[0]\n",
    "    dim_2=dims[1]\n",
    "\n",
    "    for lab in np.unique(Y_fm):\n",
    "        x,y=dataset_transformed[labels==lab][:,dim_1].mean(),dataset_transformed[labels==lab][:,dim_2].mean()\n",
    "        position_dict[lab]=[x,y]\n",
    "\n",
    "    sc=ax.scatter(dataset_transformed[:,dim_1],dataset_transformed[:,dim_2],c=Y_fm,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "    for lab in np.unique(Y_fm):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(dataset[labels==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "        ax.set_title(f'Dimension {dim_1} & Dimension {dim_2}')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heart Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying UMAP Manifold Learning on the Heart Dataset\n",
    "\n",
    "dataset=X_heart_scaled\n",
    "chosen_n_umap_d1=None\n",
    "chosen_neighbors_1=None\n",
    "\n",
    "n_comps=range(1,20)\n",
    "num_neighbors=[30,45,60]\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "\n",
    "trustworthiness_scores=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "reconstruction_errors=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "\n",
    "for i,n in tqdm(enumerate(n_comps)):\n",
    "\n",
    "    for ii,neighbors in enumerate(num_neighbors):\n",
    "\n",
    "        umap_comp=umap.UMAP(n_components=n,n_neighbors=neighbors,n_jobs=-1)\n",
    "        dataset_transformed=umap_comp.fit_transform(dataset)\n",
    "\n",
    "        trustworthiness_scores[i,ii]=trustworthiness(dataset,dataset_transformed,n_neighbors=neighbors)\n",
    "        #reconstruction_errors[i,ii]=mean_squared_error(dataset,umap_comp.inverse_transform(dataset_transformed))\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Trustworthiness Score, Num Neighbors & N_Components (HEART)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,trustworthiness_scores[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,trustworthiness_scores[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Trustworthiness Score')\n",
    "\n",
    "# fig=plt.figure()\n",
    "# fig.set_size_inches(10,5)\n",
    "# plt.title('Reconstruction Error, Num Neighbors & N_Components (HEART)');\n",
    "# for ii,neighbors in enumerate(num_neighbors):\n",
    "#     plt.scatter(n_comps,reconstruction_errors[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "#     plt.plot(n_comps,reconstruction_errors[:,ii])\n",
    "#     plt.legend()\n",
    "# plt.xlabel('N_Components')\n",
    "# plt.ylabel('Reconstruction Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying ISOMAP Manifold Learning on the Heart Dataset\n",
    "\n",
    "dataset=X_heart_scaled\n",
    "chosen_n_umap_d2=None\n",
    "chosen_neighbors_d2=None\n",
    "\n",
    "n_comps=range(2,20)\n",
    "num_neighbors=[30,45,60]\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "\n",
    "trustworthiness_scores=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "reconstruction_errors=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "\n",
    "for i,n in tqdm(enumerate(n_comps)):\n",
    "\n",
    "    for ii,neighbors in enumerate(num_neighbors):\n",
    "\n",
    "        iso_map_comp=Isomap(n_components=n,n_neighbors=neighbors,n_jobs=-1,p=1)\n",
    "        dataset_transformed=iso_map_comp.fit_transform(dataset)\n",
    "\n",
    "        trustworthiness_scores[i,ii]=trustworthiness(dataset,dataset_transformed,n_neighbors=neighbors)\n",
    "        reconstruction_errors[i,ii]=iso_map_comp.reconstruction_error()\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Trustworthiness Score, Num Neighbors & N_Components (HEART)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,trustworthiness_scores[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,trustworthiness_scores[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Trustworthiness Score')\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Reconstruction Error, Num Neighbors & N_Components (HEART)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,reconstruction_errors[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,reconstruction_errors[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Reconstruction Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying ISOMAP Manifold Learning on the Heart Dataset\n",
    "\n",
    "dataset=X_heart_scaled\n",
    "chosen_n_umap_d2=None\n",
    "chosen_neighbors_d2=None\n",
    "\n",
    "n_comps=range(2,20)\n",
    "P=[1,2,3]\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "\n",
    "trustworthiness_scores=np.zeros(shape=(len(n_comps),len(P)))\n",
    "reconstruction_errors=np.zeros(shape=(len(n_comps),len(P)))\n",
    "\n",
    "for i,n in tqdm(enumerate(n_comps)):\n",
    "\n",
    "    for ii,p in enumerate(P):\n",
    "\n",
    "        iso_map_comp=Isomap(n_components=n,n_neighbors=45,n_jobs=-1,p=p)\n",
    "        dataset_transformed=iso_map_comp.fit_transform(dataset)\n",
    "\n",
    "        trustworthiness_scores[i,ii]=trustworthiness(dataset,dataset_transformed,n_neighbors=neighbors)\n",
    "        reconstruction_errors[i,ii]=iso_map_comp.reconstruction_error()\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Trustworthiness Score, Num Neighbors & N_Components (HEART)');\n",
    "for ii,p in enumerate(P):\n",
    "    plt.scatter(n_comps,trustworthiness_scores[:,ii],label=f'Distance Measure {p}')\n",
    "    plt.plot(n_comps,trustworthiness_scores[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Trustworthiness Score')\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Reconstruction Error, Num Neighbors & N_Components (HEART)');\n",
    "for ii,p in enumerate(P):\n",
    "    plt.scatter(n_comps,reconstruction_errors[:,ii],label=f'Distance Measure {p}')\n",
    "    plt.plot(n_comps,reconstruction_errors[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Reconstruction Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different values of p\n",
    "dataset=X_heart_scaled\n",
    "chosen_n_umap_d2=None\n",
    "chosen_neighbors_d2=None\n",
    "\n",
    "n_comps=range(2,20)\n",
    "num_neighbors=[30,45,60]\n",
    "repeats=3\n",
    "ideal_comps=None \n",
    "\n",
    "trustworthiness_scores=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "reconstruction_errors=np.zeros(shape=(len(n_comps),len(num_neighbors)))\n",
    "\n",
    "for i,n in tqdm(enumerate(n_comps)):\n",
    "\n",
    "    for ii,neighbors in enumerate(num_neighbors):\n",
    "\n",
    "        iso_map_comp=Isomap(n_components=n,n_neighbors=neighbors,n_jobs=-1)\n",
    "        dataset_transformed=iso_map_comp.fit_transform(dataset)\n",
    "\n",
    "        trustworthiness_scores[i,ii]=trustworthiness(dataset,dataset_transformed,n_neighbors=neighbors)\n",
    "        reconstruction_errors[i,ii]=iso_map_comp.reconstruction_error()\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Trustworthiness Score, Num Neighbors & N_Components (HEART)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,trustworthiness_scores[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,trustworthiness_scores[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Trustworthiness Score')\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.title('Reconstruction Error, Num Neighbors & N_Components (HEART)');\n",
    "for ii,neighbors in enumerate(num_neighbors):\n",
    "    plt.scatter(n_comps,reconstruction_errors[:,ii],label=f'Num Neighbours {neighbors}')\n",
    "    plt.plot(n_comps,reconstruction_errors[:,ii])\n",
    "    plt.legend()\n",
    "plt.xlabel('N_Components')\n",
    "plt.ylabel('Reconstruction Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating ideal UMAP manifold \n",
    "chosen_n_umap_d2=5\n",
    "chosen_neighbors_d2=45\n",
    "dataset=X_heart_scaled\n",
    "labels=Y_heart\n",
    "\n",
    "#iso_map_d1=umap.UMAP(n_components=chosen_n_iso_map_d1,n_neighbors=chosen_neighbors_d1,n_jobs=-1)\n",
    "umap_d2=umap.UMAP(n_components=chosen_n_umap_d2,n_neighbors=chosen_neighbors_d2,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=umap_d2.fit_transform(dataset)\n",
    "\n",
    "#Visualizing transformed data\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,3,figsize=(25, 5))\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations of Select UMAP Components (HEART)',fontsize=18)\n",
    "\n",
    "SELECTION=[[0,1],[0,2],[3,4]]\n",
    "\n",
    "for ax,dims in zip(axes,SELECTION):\n",
    "    artists = []\n",
    "    dim_1=dims[0]\n",
    "    dim_2=dims[1]\n",
    "\n",
    "    # for lab in np.unique(Y_fm):\n",
    "    #     x,y=dataset_transformed[labels==lab][:,dim_1].mean(),dataset_transformed[labels==lab][:,dim_2].mean()\n",
    "    #     position_dict[lab]=[x,y]\n",
    "\n",
    "    sc=ax.scatter(dataset_transformed[:,dim_1],dataset_transformed[:,dim_2],c=labels,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "    # for lab in np.unique(Y_fm):\n",
    "    #     # Positioning Images\n",
    "    #     image_x = position_dict[lab][0]\n",
    "    #     image_y = position_dict[lab][1]\n",
    "    #     im=OffsetImage(dataset[labels==lab][0].reshape(28,28),cmap='gray')\n",
    "    #     ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "    #     artists.append(ax.add_artist(ab))\n",
    "    ax.set_title(f'Dimension {dim_1} & Dimension {dim_2}')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating ideal ISOMAP manifold \n",
    "chosen_n_iso_map_d2=13\n",
    "chosen_neighbors_d2=45\n",
    "dataset=X_heart_scaled\n",
    "labels=Y_heart\n",
    "\n",
    "#iso_map_d1=umap.UMAP(n_components=chosen_n_iso_map_d1,n_neighbors=chosen_neighbors_d1,n_jobs=-1)\n",
    "iso_map_d2=Isomap(n_components=chosen_n_iso_map_d2,n_neighbors=chosen_neighbors_d2,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=iso_map_d2.fit_transform(dataset)\n",
    "\n",
    "#Visualizing transformed data\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,3,figsize=(25, 5))\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations of Select ISOMAP Components (HEART)',fontsize=18)\n",
    "\n",
    "SELECTION=[[0,1],[0,2],[11,12]]\n",
    "\n",
    "for ax,dims in zip(axes,SELECTION):\n",
    "    artists = []\n",
    "    dim_1=dims[0]\n",
    "    dim_2=dims[1]\n",
    "\n",
    "    # for lab in np.unique(Y_fm):\n",
    "    #     x,y=dataset_transformed[labels==lab][:,dim_1].mean(),dataset_transformed[labels==lab][:,dim_2].mean()\n",
    "    #     position_dict[lab]=[x,y]\n",
    "\n",
    "    sc=ax.scatter(dataset_transformed[:,dim_1],dataset_transformed[:,dim_2],c=labels,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "    # for lab in np.unique(Y_fm):\n",
    "    #     # Positioning Images\n",
    "    #     image_x = position_dict[lab][0]\n",
    "    #     image_y = position_dict[lab][1]\n",
    "    #     im=OffsetImage(dataset[labels==lab][0].reshape(28,28),cmap='gray')\n",
    "    #     ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "    #     artists.append(ax.add_artist(ab))\n",
    "    ax.set_title(f'Dimension {dim_1} & Dimension {dim_2}')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

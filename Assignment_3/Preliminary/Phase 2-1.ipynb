{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import operator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, fbeta_score,make_scorer, mutual_info_score, silhouette_score, normalized_mutual_info_score, classification_report, confusion_matrix,f1_score, mean_squared_error, adjusted_rand_score\n",
    "import time \n",
    "from sklearn.model_selection import LearningCurveDisplay\n",
    "import tensorflow_addons as tfa\n",
    "from NNnet_class import NNnet\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA,FastICA\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim,SparseRandomProjection,GaussianRandomProjection\n",
    "from sklearn.manifold import TSNE,trustworthiness,Isomap\n",
    "from A3_utils import calculate_wcss,plot_gallery, categorizer, create_elbow_plot,experiment_em_clusters,experiment_km_clusters\n",
    "from kmodes.kmodes import KModes\n",
    "import umap\n",
    "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
    "                                  TextArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "# Ignore SparseEfficiencyWarning\n",
    "warnings.simplefilter('ignore', category=SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeting seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Making accuracy scorrer\n",
    "accuracy_scorer=make_scorer(accuracy_score)\n",
    "\n",
    "#Global Beta (Used for Ftwo score - not used in assignment)\n",
    "BETA=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_curve(data,param,param_vals,ax,algorithim_name,algorithim,metric=accuracy_scorer,metric_name='Accuracy',graph=True,beta=BETA,cv=3):\n",
    "\n",
    "    '''Function to create parameter curves\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of np.arrays containing the features and labels  \n",
    "    param (str): Parameter to vary\n",
    "    param_vals (list): Parameter values\n",
    "    ax (matplotlib.axes): Axis for graph\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim to vary parameter in\n",
    "    metric (sklearn.metric): Metric to score algorotihim on\n",
    "    metric_name (str): Metric Name\n",
    "    graph (bool): Bool for validation graph\n",
    "    beta (int): Beta value for the Ftwo scored \n",
    "    cv (int): The number of cross validations folds\n",
    "    \n",
    "    Returns:\n",
    "    None'''\n",
    "\n",
    "\n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "    \n",
    "    for i in param_vals:\n",
    "        #Crossvalidating each parameter value\n",
    "        kf=KFold(n_splits=cv,shuffle=True)\n",
    "\n",
    "        internal_train_accuracy=0\n",
    "        internal_test_accuracy=0\n",
    "        \n",
    "        for train,test in kf.split(X=data[0]):\n",
    "            if i!='Default':\n",
    "                if 'net' in algorithim_name.lower():\n",
    "                    kwargs={param:i,'input_dims':data[0].shape[-1]}\n",
    "                    clf=algorithim(**kwargs)\n",
    "                else:\n",
    "                    clf=algorithim(**{param:i})\n",
    "            else:\n",
    "                clf=algorithim()\n",
    "\n",
    "            clf.fit(data[0][train],data[1][train])\n",
    "            internal_train_accuracy+=metric(y_pred=clf.predict(data[0][train]),y_true=data[1][train])\n",
    "            internal_test_accuracy+=metric(y_pred=clf.predict(data[0][test]),y_true=data[1][test])\n",
    "\n",
    "        train_acc.append(internal_train_accuracy/cv)\n",
    "        test_acc.append(internal_test_accuracy/cv)\n",
    "\n",
    "    best_val=param_vals[np.argmax(test_acc)]\n",
    "\n",
    "    if type(param_vals[0])==str:\n",
    "        ax.scatter(param_vals,train_acc,label='Training {}'.format(metric_name))\n",
    "        ax.scatter(param_vals,test_acc,label='Validation {}'.format(metric_name))\n",
    "    else:\n",
    "        ax.plot(param_vals,train_acc,label='Training {}'.format(metric_name))\n",
    "        ax.plot(param_vals,test_acc,label='Validation {}'.format(metric_name))\n",
    "    #plt.xscale('log')\n",
    "    ax.axvline(best_val,label='Best {} Value'.format(param),color='red',linestyle = '--')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('{}'.format(metric_name));\n",
    "    ax.set_xlabel(param);\n",
    "    ax.set_title('{} vs {}'.format(param,metric_name));\n",
    "    if type(best_val)!=str:\n",
    "        if best_val==max(param_vals):\n",
    "            ax.text(y=(max(test_acc)+min(test_acc))/2+np.abs(np.std(test_acc)),x=best_val-np.std(param_vals)/12,s=best_val,color='green',weight='bold')\n",
    "        else:\n",
    "            ax.text(y=(max(test_acc)+min(test_acc))/2+np.abs(np.std(test_acc)),x=best_val+np.std(param_vals)/12,s=best_val,color='green',weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_algorithim(data,param_dicts,dataset,algorithim_name,algorithim,metric=accuracy_score,metric_name='Accuracy',cv=3):\n",
    "    \n",
    "    '''Function to deal with algorithim\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of np.arrays containing the features and labels\n",
    "    param_dict (dict): Parameter dictionary to vary\n",
    "    dataset (str): Dataset name\n",
    "    algorithim_name (str): Algorithim name\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim to vary parameter in\n",
    "    metric (sklearn.metric): Metric to score algorotihim on\n",
    "    metric_name (str): Metric Name\n",
    "    cv (int): The number of cross validations folds \n",
    "\n",
    "    Returns:\n",
    "    None'''\n",
    "    \n",
    "    num_classes=len(param_dicts.keys())\n",
    "\n",
    "    #Getting Fig Size\n",
    "    fig,axes=plt.subplots(2,int(np.ceil(num_classes/2)))\n",
    "    fig.set_size_inches(15,15)\n",
    "    i=-1\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "        i+=1\n",
    "        plt.suptitle('Results for Algorithim: \"{}\" for \"{}\" Dataset'.format(algorithim_name,dataset),fontsize=18)\n",
    "        param=list(param_dicts.keys())[i]\n",
    "        param_vals=param_dicts[param]\n",
    "        create_param_curve(data,param,param_vals,ax,algorithim_name,algorithim,metric,metric_name)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Heart Disease Data\n",
    "def load_heart_data():\n",
    "\n",
    "    '''Load Heart Disease Dataset\n",
    "    \n",
    "    Returns:\n",
    "    X (np.array): X array\n",
    "    Y (np.array): Y array\n",
    "    col_index (dict): Dictionary containing the pairing for the column location and it's name'''\n",
    "\n",
    "    #PLEASE CHANGE TO LOCATION OF YOUR HEART DATA\n",
    "    df=pd.read_csv('Data/Heart_2/heart.csv')\n",
    "    Y=np.array(df['HeartDisease'])\n",
    "    df.drop('HeartDisease',axis=1,inplace=True)\n",
    "    \n",
    "    label_columns=['HeartDisease']\n",
    "    categorical_columns=['Sex', 'ChestPainType', 'RestingECG','ExerciseAngina','ST_Slope']\n",
    "\n",
    "    non_categorical_variables=list(set(df.columns).difference(set(categorical_columns+label_columns)))\n",
    "    X=np.array(df[non_categorical_variables])\n",
    "    columns_categorized=non_categorical_variables\n",
    "\n",
    "    #Now we need to one hot vectorize the type_of_meal_plan, room_type_reserved and market_segment_type\n",
    "    label_dict={}\n",
    "    for i in categorical_columns:\n",
    "        label_dict[i]=OneHotEncoder()\n",
    "        res=label_dict[i].fit_transform(np.array(df[i]).reshape(-1,1)).toarray()\n",
    "        X=np.c_[X,res]\n",
    "        columns_categorized=columns_categorized+[i+'%'+j for j in ['1','2','3','4','5','6','7'][:res.shape[-1]]]\n",
    "\n",
    "        col_index={}\n",
    "        results_corr={}\n",
    "        for label,col in zip(columns_categorized,range(X.shape[-1])):\n",
    "            corr=scipy.stats.pearsonr(X[:,col],Y)[0]\n",
    "            results_corr[label]=corr\n",
    "            col_index[label]=col\n",
    "    return X,Y,col_index\n",
    "\n",
    "#Fashion MNIST\n",
    "def load_fmnist():\n",
    "    #Loading dataset\n",
    "    data=pd.concat([pd.read_csv('Data/FMNIST/fashion-mnist_train.csv'),pd.read_csv('Data/FMNIST/fashion-mnist_test.csv')],axis=0)\n",
    "\n",
    "    X=data.iloc[:,1:].to_numpy()\n",
    "    Y=data.iloc[:,:1].to_numpy().ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1,random_state=42,stratify=Y)\n",
    "\n",
    "    return X_test,y_test\n",
    "\n",
    "\n",
    "#Load Pokemon Data\n",
    "def load_pokemon():\n",
    "    directory='Data/Pokemon/'\n",
    "    image_files = glob(os.path.join(directory, '**', '*.*'), recursive=True)\n",
    "\n",
    "    # Initialize a list to store the images as NumPy arrays\n",
    "    features=[]\n",
    "    labels=[]\n",
    "\n",
    "    # Iterate over the Image files\n",
    "    for file in image_files:\n",
    "\n",
    "        #Name of pokemon\n",
    "        label=file.split('\\\\')[1]\n",
    "\n",
    "        #Appending name\n",
    "        labels.append(label)\n",
    "\n",
    "        #Reading images\n",
    "        image = Image.open(file)\n",
    "\n",
    "        #Converting to RGB\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        #Removing color\n",
    "        image=image.convert('L')\n",
    "        \n",
    "        #Resizing images\n",
    "        image=image.resize((48, 48))\n",
    "\n",
    "        #Appending images\n",
    "        features.append(np.array(image).flatten())\n",
    "\n",
    "    #Converting to Numpy\n",
    "    labels=np.array(labels)\n",
    "    features=np.array(features)\n",
    "\n",
    "    pokemon_names=LabelEncoder()\n",
    "    labels_encoded=pokemon_names.fit_transform(labels)\n",
    "\n",
    "    return features,labels_encoded,pokemon_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_algorithim(X,Y,dataset_name,algorithim_name,algorithim,params,predictor_metric=accuracy_score,grid_search_metric=accuracy_scorer,predictor_metric_name='accuracy',return_needed=False):\n",
    "\n",
    "    '''Function to score algorithim\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.array): All features\n",
    "    Y (np.array): All labels\n",
    "    dataset_name (str): Dataset name\n",
    "    algorithim_name (str): Algorithim name\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim being experimented on\n",
    "    params (dict): Parameter dictionary for GridSearch\n",
    "    predictor_metric (sklearn.metric): Metric to score algorotihim on\n",
    "    grid_search_metric (sklearn.metric): Metric used by GridSearchCV\n",
    "    predictor_metric_name (str): Metric Name for predictor metric\n",
    "    return_needed (bool): Bool if fit model needed to be returned    \n",
    "\n",
    "    Returns:\n",
    "    glf (sklearn.model/NNnet): Fit model'''\n",
    "\n",
    "    standardize=False\n",
    "    if 'KNN' in algorithim_name.upper() or 'SVM' in algorithim_name.upper() or 'NET' in algorithim_name.upper():\n",
    "        standardize=True\n",
    "    \n",
    "    if 'NET' in algorithim_name.upper():\n",
    "        n_jobs=1\n",
    "    else:\n",
    "        n_jobs=-1\n",
    "\n",
    "\n",
    "    train,test=split_data(X,Y,valid=False,standardize=standardize)\n",
    "\n",
    "    glf_cv=GridSearchCV(algorithim,param_grid=params,verbose=0,n_jobs=n_jobs,cv=3,scoring=grid_search_metric,refit=True)\n",
    "\n",
    "    rep=10\n",
    "    start_time=time.time()\n",
    "    glf_cv.fit(train[0],train[1])\n",
    "\n",
    "    glf=glf_cv.best_estimator_\n",
    "    \n",
    "    for i in range(rep):\n",
    "        glf.fit(train[0],train[1])\n",
    "    time_delay_train=(time.time()-start_time)/rep\n",
    "\n",
    "    y_train_pred=glf.predict(train[0])\n",
    "    \n",
    "    if 'F' in predictor_metric_name.upper():\n",
    "        train_score = predictor_metric(y_pred=glf.predict(train[0]),y_true=train[1],beta=BETA)\n",
    "        test_score = predictor_metric(y_pred=glf.predict(test[0]),y_true=test[1],beta=BETA)\n",
    "    else:\n",
    "        train_score = predictor_metric(y_pred=glf.predict(train[0]),y_true=train[1])\n",
    "        test_score = predictor_metric(y_pred=glf.predict(test[0]),y_true=test[1])\n",
    "\n",
    "    start_time=time.time()\n",
    "    for i in range(rep):\n",
    "        y_test_pred=glf.predict(test[0])\n",
    "    time_delay_infer=(time.time()-start_time)/rep\n",
    "\n",
    "    print('{} Final Results'.format(dataset_name))\n",
    "    print('\\n')\n",
    "    print(glf_cv.best_params_)\n",
    "    print('\\n')\n",
    "    print('Average time to train the ideal {} was {:.3f} seconds'.format(algorithim_name,time_delay_train))\n",
    "    print('Average time to infer the ideal {} was {:.3f} seconds'.format(algorithim_name,time_delay_infer))\n",
    "    print('\\n')\n",
    "    print('The result on the training data for the ideal {} algorithim is a {} {} score'.format(algorithim_name,train_score,predictor_metric_name))\n",
    "    print('The result on the test data for the ideal {} algorithim is a {} {} score'.format(algorithim_name,test_score,predictor_metric_name))\n",
    "    \n",
    "    if return_needed:\n",
    "        return glf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,Y,valid=True,standardize=False):\n",
    "\n",
    "    '''\n",
    "    Split the data between train, test and optional validation dataset\n",
    "\n",
    "    Parameters:\n",
    "    X (np.array): X features\n",
    "    Y (np.rray): Labels\n",
    "    valid (bool): Split into validation dataset \n",
    "    standardize (bool): Whether to standardize the data (introduces bias as Sklearn Standard Scaler is trained only on the train data)\n",
    "\n",
    "    Returns:\n",
    "    train (list): np.array list of train\n",
    "    valid (list): optional np.array list of validation\n",
    "    test (list): np.array list of test\n",
    "    '''\n",
    "    \n",
    "    #Now let's split the data between test and train, we'll use the standard 80/20 split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=42)\n",
    "    \n",
    "    if valid:\n",
    "        #We'll also split the data between train and validation, we'll again use the standard 80/20 split\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2,random_state=42)\n",
    "        \n",
    "        if standardize:\n",
    "            sklr=StandardScaler()\n",
    "            X_train=sklr.fit_transform(X_train)\n",
    "            X_valid=sklr.transform(X_valid)\n",
    "            X_test=sklr.transform(X_test)\n",
    "        return [X_train,y_train],[X_valid,y_valid],[X_test,y_test]\n",
    "\n",
    "    if standardize:\n",
    "        sklr=StandardScaler()\n",
    "        X_train=sklr.fit_transform(X_train)\n",
    "        X_test=sklr.transform(X_test)\n",
    "    return [X_train,y_train],[X_test,y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pokemon\n",
    "X_poke,Y_poke,poke_encoder=load_pokemon()\n",
    "train_poke,test_poke=split_data(X_poke,Y_poke,valid=False)\n",
    "\n",
    "#Heart\n",
    "X_heart,Y_heart,heart_index=load_heart_data()\n",
    "train_heart,test_heart=split_data(X_heart,Y_heart,valid=False)\n",
    "\n",
    "#Fashion MNIST\n",
    "X_fm,Y_fm=load_fmnist()\n",
    "train_fm,test_fm=split_data(X_fm,Y_fm,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing\n",
    "sklr_poke=StandardScaler()\n",
    "sklr_heart=StandardScaler()\n",
    "sklr_fm=StandardScaler()\n",
    "\n",
    "X_poke_scaler=StandardScaler()\n",
    "X_heart_scaler=StandardScaler()\n",
    "X_fm_scaler=StandardScaler()\n",
    "\n",
    "#poke\n",
    "X_poke_scaled=X_poke_scaler.fit_transform(X_poke)\n",
    "\n",
    "train_poke_standardized=train_poke.copy()\n",
    "train_poke_unstandardized=test_poke.copy()\n",
    "\n",
    "test_poke_standardized=test_poke.copy()\n",
    "\n",
    "train_poke_standardized[0]=sklr_poke.fit_transform(train_poke[0])\n",
    "test_poke_standardized[0]=sklr_poke.transform(test_poke[0])\n",
    "\n",
    "#Heart\n",
    "X_heart_scaled=X_heart_scaler.fit_transform(X_heart)\n",
    "\n",
    "train_heart_standardized=train_heart.copy()\n",
    "\n",
    "test_heart_standardized=test_heart.copy()\n",
    "\n",
    "train_heart_standardized[0]=sklr_heart.fit_transform(train_heart[0])\n",
    "test_heart_standardized[0]=sklr_heart.transform(test_heart[0])\n",
    "\n",
    "#FMNIST\n",
    "X_fm_scaled=X_fm_scaler.fit_transform(X_fm)\n",
    "\n",
    "train_fm_standardized=train_fm.copy()\n",
    "train_fm_unstandardized=test_fm.copy()\n",
    "\n",
    "test_fm_standardized=test_fm.copy()\n",
    "\n",
    "train_fm_standardized[0]=sklr_fm.fit_transform(train_fm[0])\n",
    "test_fm_standardized[0]=sklr_fm.transform(test_fm[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering ALgorithms on Reduced Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMINST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET=X_fm_scaled\n",
    "LABELS=Y_fm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_pca_d1=95\n",
    "\n",
    "#Creating PCA\n",
    "d1_pca=PCA(n_components=chosen_n_pca_d1,random_state=21)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_PCA=d1_pca.fit_transform(DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "dataset_name='FMNIST PCA'\n",
    "dataset=dataset_transformed_PCA\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "\n",
    "experiment_em_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters\n",
    "#best_clusters_PCA_d1=n_list[np.argmin(BIC_scores)]\n",
    "dataset=dataset_transformed_PCA\n",
    "\n",
    "#Selecting N based on Plots\n",
    "best_clusters_PCA_d1=13\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "repeats=5\n",
    "\n",
    "for i in range(repeats):\n",
    "    best_em_pca_d1=GaussianMixture(n_components=best_clusters_PCA_d1,random_state=21*i)\n",
    "\n",
    "    best_em_pca_d1.fit(dataset)\n",
    "    mi_score+=normalized_mutual_info_score(LABELS,best_em_pca_d1.predict(dataset))/repeats\n",
    "    aj_score+=adjusted_rand_score(LABELS,best_em_pca_d1.predict(dataset))/repeats\n",
    "\n",
    "\n",
    "#Evaluation Normalized MI Score\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "#Evaluating Adjusted Rand Score Rand Score\n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(dataset_transformed_PCA)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "plt.suptitle('Visualizations EM Clustering Results with Labels (PCA FMINST)',fontsize=18)\n",
    "LABS=[LABELS,best_em_pca_d1.predict(dataset_transformed_PCA)]\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x,y=dataset_transformed[LABELS==lab][:,0].mean(),dataset_transformed[LABELS==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(DATASET[LABELS==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K MODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "dataset_name='FMNIST (PCA)'\n",
    "dataset=dataset_transformed_PCA\n",
    "labels_og=LABELS\n",
    "algorithm_name='K-MODES'\n",
    "categorical_cols=None\n",
    "\n",
    "experiment_km_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name,categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols=None\n",
    "data_categorized=categorizer(dataset_transformed_PCA,categorical_cols)\n",
    "\n",
    "#Best Number of Clusters\n",
    "best_kmodes_PCA_d1=9\n",
    "dataset=data_categorized\n",
    "\n",
    "print(\"DATASET 1 PCA KMODES RESULTS \\n\\n\")\n",
    "\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "repeats=5\n",
    "\n",
    "for i in range(repeats):\n",
    "    km = KModes(n_clusters=best_kmodes_PCA_d1, init='Huang', n_init=3, verbose=0)\n",
    "\n",
    "    km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    mi_score+=normalized_mutual_info_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))/repeats\n",
    "    aj_score+=adjusted_rand_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))/repeats\n",
    "\n",
    "\n",
    "#Evaluation Normalized MI Score\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "#Evaluating Adjusted Rand Score Rand Score\n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Mean Cluster Image from the ideal number of clusters \n",
    "#Visualizing\n",
    "categorical_cols=None\n",
    "data_categorized=categorizer(dataset_transformed_PCA,categorical_cols)\n",
    "km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(dataset_transformed_PCA)\n",
    "\n",
    "#fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (PCA FMINST)',fontsize=18)\n",
    "LABS=[LABELS,km.predict(data_categorized,categorical=data_categorized.shape[-1])]\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x,y=dataset_transformed[LABELS==lab][:,0].mean(),dataset_transformed[LABELS==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "\n",
    "    for l in np.unique(labels_):\n",
    "        # Choose color from a colormap\n",
    "        color = plt.cm.viridis(l / len(np.unique(labels_)))\n",
    "        ax.scatter(dataset_transformed[:, 0][labels_ == l], dataset_transformed[:, 1][labels_ == l],\n",
    "                   label=l, zorder=1, color=color)\n",
    "\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(X_fm_scaled[LABELS==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_ica_d1=66\n",
    "\n",
    "#Creating PCA\n",
    "d1_ica=FastICA(n_components=chosen_n_ica_d1)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_ICA=d1_ica.fit_transform(DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "dataset_name='FMNIST ICA'\n",
    "dataset=dataset_transformed_ICA\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "\n",
    "experiment_em_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters\n",
    "#best_clusters_ICA_d1=n_list[np.argmin(BIC_scores)]\n",
    "\n",
    "#Selecting Based on plots\n",
    "dataset=dataset_transformed_ICA\n",
    "\n",
    "REPEATS=5\n",
    "\n",
    "best_clusters_ICA_d1=18\n",
    "best_em_ica_d1=GaussianMixture(n_components=best_clusters_ICA_d1,random_state=42)\n",
    "best_em_ica_d1.fit(dataset)\n",
    "\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "\n",
    "for i in range(REPEATS):\n",
    "    mi_score+=normalized_mutual_info_score(labels_og,best_em_ica_d1.predict(dataset))/REPEATS\n",
    "    aj_score+=adjusted_rand_score(labels_og,best_em_ica_d1.predict(dataset))/REPEATS\n",
    "\n",
    "print(\"ICA RESULTS \\n\\n\")\n",
    "\n",
    "#Evaluation Normalized MI Score\n",
    "print(f'The Normalized Mutual Info Score for the best clustering is {mi_score}') \n",
    "\n",
    "#Evaluating Adjusted Rand Score Rand Score\n",
    "print(f'The Adjusted Rand Score for the best clustering is {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(DATASET)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "plt.suptitle('Visualizations EM Clustering Results with Labels (ICA FMINST)',fontsize=18)\n",
    "LABS=[LABELS,best_em_ica_d1.predict(dataset_transformed_ICA)]\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x,y=dataset_transformed[LABELS==lab][:,0].mean(),dataset_transformed[LABELS==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(DATASET[LABELS==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K MODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "n_list=[2,3,5,7,10,15,25]\n",
    "dataset_name='FMNIST (ICA)'\n",
    "dataset=dataset_transformed_ICA\n",
    "labels_og=LABELS\n",
    "algorithm_name='K-MODES'\n",
    "cluster_graphs=4\n",
    "categorical_cols=None\n",
    "\n",
    "data_categorized=categorizer(dataset,categorical_cols)\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2)\n",
    "dataset_2d=converter.fit_transform(data_categorized)\n",
    "silhouette_scores=[]\n",
    "all_labels=[]\n",
    "wcss_scores=[]\n",
    "KM_cost=[]\n",
    "KM_dict={}\n",
    "\n",
    "\n",
    "#Assigning Clusters\n",
    "for n in n_list:\n",
    "    #Creating Gaussian\n",
    "    km = KModes(n_clusters=n, init='Huang', n_init=5, verbose=0)\n",
    "\n",
    "    #Fitted labels dataset\n",
    "    km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "    #Predictions\n",
    "    labels=km.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    all_labels.append(labels)\n",
    "    \n",
    "    #Calculating Mutal Info Score\n",
    "    #silhouette_scores.append(silhouette_score(data_categorized,labels))\n",
    "\n",
    "    #WCSS Score\n",
    "    wcss_scores.append(calculate_wcss(data_categorized,labels,km.cluster_centroids_))\n",
    "\n",
    "    KM_dict[n]=km\n",
    "\n",
    "    KM_cost.append(km.cost_)\n",
    "\n",
    "#Plotting 2d color coded with labels \n",
    "fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "fig.set_size_inches(35,5)\n",
    "\n",
    "plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c>-1:\n",
    "        #Predictions\n",
    "        labels=all_labels[c]\n",
    "\n",
    "        #Plotting\n",
    "        sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "        #ax.legend()\n",
    "        ax.set_ylabel('Y');\n",
    "        ax.set_xlabel('X');\n",
    "        ax.set_title('{} : COMPONENTS'.format(n_list[c]));\n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    else:\n",
    "\n",
    "        sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "        #ax.legend()\n",
    "        ax.set_ylabel('Y');\n",
    "        ax.set_xlabel('X');\n",
    "        ax.set_title('Dataset');      \n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "ax_2=plt.gca().twinx()\n",
    "create_elbow_plot(n_list,wcss_scores,ax_2)\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting Cost\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Cost & Clusters')\n",
    "plt.plot(n_list,KM_cost)\n",
    "plt.scatter(n_list, KM_cost, s=100, c='blue', marker='o', edgecolors='black')\n",
    "ax_2=plt.gca().twinx()\n",
    "create_elbow_plot(n_list,KM_cost,ax_2)\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Cost')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "# fig=plt.figure()\n",
    "# fig.set_size_inches(10,5)\n",
    "\n",
    "# plt.title('Silhouette_scores & Clusters')\n",
    "# plt.plot(n_list,silhouette_scores)\n",
    "# plt.scatter(n_list, silhouette_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "# plt.xlabel('N_clusters')\n",
    "# plt.ylabel('Silhouette_scores')\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols=None\n",
    "data_categorized=categorizer(dataset_transformed_ICA,categorical_cols)\n",
    "\n",
    "#Best Number of Clusters\n",
    "best_kmodes_ICA_d1=15\n",
    "dataset=data_categorized\n",
    "\n",
    "#Evaluation\n",
    "\n",
    "#Creating Best KMODES\n",
    "km = KModes(n_clusters=best_kmodes_ICA_d1, init='Huang', n_init=3, verbose=0)\n",
    "\n",
    "#Fitted labels dataset\n",
    "km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "print(\"DATASET 1 KMODES RESULTS \\n\\n\")\n",
    "\n",
    "#Evaluation Normalized MI Score\n",
    "mi_score=normalized_mutual_info_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "#Evaluating Adjusted Rand Score Rand Score\n",
    "aj_score=adjusted_rand_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))\n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Mean Cluster Image from the ideal number of clusters \n",
    "#Visualizing\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(DATASET)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (ICA FMINST)',fontsize=18)\n",
    "LABS=[LABELS,km.predict(dataset,categorical=dataset.shape[-1])]\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x,y=dataset_transformed[LABELS==lab][:,0].mean(),dataset_transformed[LABELS==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(X_fm_scaled[LABELS==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_rgp_d1=666\n",
    "\n",
    "#Creating PCA\n",
    "d1_rgp=GaussianRandomProjection(n_components=chosen_n_rgp_d1,random_state=42)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_RGP=d1_rgp.fit_transform(DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "n_list=[2,3,5,7,9,10,15,25]\n",
    "dataset_name='FMNIST RGP'\n",
    "dataset=dataset_transformed_RGP\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "cluster_graphs=4\n",
    "repeats=3\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2,perplexity=45)\n",
    "dataset_2d=converter.fit_transform(dataset)\n",
    "\n",
    "#Average of 3 runs \n",
    "silhouette_scores=np.zeros(shape=len(n_list))\n",
    "wcss_scores=np.zeros(shape=len(n_list))\n",
    "BIC_scores=np.zeros(shape=len(n_list))\n",
    "AIC_scores=np.zeros(shape=len(n_list))\n",
    "EM_dict={}\n",
    "\n",
    "for r in [21*(1+i) for i in range(repeats)]:\n",
    "\n",
    "    #Will be calculated for each run for plots\n",
    "    all_labels=[]\n",
    "    \n",
    "    #Assigning Clusters\n",
    "    for i,n in enumerate(n_list):\n",
    "        #Creating Gaussian\n",
    "        em=GaussianMixture(n_components=n,random_state=r)\n",
    "\n",
    "        #Fitted labels dataset\n",
    "        em.fit(dataset)\n",
    "\n",
    "        #Predictions\n",
    "        labels=em.predict(dataset)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        #Calculating Mutal Info Score\n",
    "        silhouette_scores[i]=silhouette_score(dataset,labels)/repeats\n",
    "\n",
    "        #WCSS Score\n",
    "        wcss_scores[i]=calculate_wcss(dataset,labels,em.means_)/repeats\n",
    "\n",
    "        #BIC Scores\n",
    "        BIC_scores[i]=em.bic(dataset)/repeats\n",
    "\n",
    "        #AIC\n",
    "        AIC_scores[i]=em.aic(dataset)/repeats\n",
    "\n",
    "        #Saving EM models for final Run \n",
    "        EM_dict[n]=em\n",
    "\n",
    "    #Plotting 2d color coded with labels \n",
    "    fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "    fig.set_size_inches(35,5)\n",
    "\n",
    "    plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "\n",
    "        if c>-1:\n",
    "            #Predictions\n",
    "            labels=all_labels[c]\n",
    "\n",
    "            #Plotting\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('{} : COMPONENTS'.format(n_list[c]));\n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "        else:\n",
    "\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('Dataset');      \n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Silhouette_scores & Clusters')\n",
    "plt.plot(n_list,silhouette_scores)\n",
    "plt.scatter(n_list, silhouette_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Silhouette_scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('WCSS')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting BIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('BIC Score & Clusters')\n",
    "plt.plot(n_list,BIC_scores)\n",
    "plt.scatter(n_list, BIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "\n",
    "#Plotting AIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('AIC Score & Clusters')\n",
    "plt.plot(n_list,AIC_scores)\n",
    "plt.scatter(n_list, AIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('AIC Score')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters based on graphs\n",
    "dataset=dataset_transformed_RGP\n",
    "\n",
    "best_clusters_RGP_d1=5\n",
    "\n",
    "for i in range(5):\n",
    "    best_em_rgp_d1=GaussianMixture(n_components=best_clusters_RGP_d1,random_state=42)\n",
    "    best_em_rgp_d1.fit(dataset)\n",
    "    #best_em_igp_d1=EM_dict[best_clusters_RGP_d1]\n",
    "\n",
    "    print(\"RGP RESULTS \\n\\n\")\n",
    "\n",
    "    #Evaluation Normalized MI Score\n",
    "    mi_score=normalized_mutual_info_score(LABELS,best_em_rgp_d1.predict(dataset))\n",
    "    print(f'The Normalized Mutual Info Score for the best clustering is {mi_score}') \n",
    "\n",
    "    #Evaluating Adjusted Rand Score Rand Score\n",
    "    aj_score=adjusted_rand_score(LABELS,best_em_rgp_d1.predict(dataset))\n",
    "    print(f'The Adjusted Rand Score for the best clustering is {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(DATASET)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "plt.suptitle('Visualizations EM Clustering Results with Labels (RGP FMINST)',fontsize=18)\n",
    "LABS=[LABELS,best_em_rgp_d1.predict(dataset_transformed_RGP)]\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x,y=dataset_transformed[LABELS==lab][:,0].mean(),dataset_transformed[LABELS==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(DATASET[LABELS==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K MODES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISOMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_iso_d1=20\n",
    "choosen_n_iso_d1_neighbors=45\n",
    "\n",
    "#Creating PCA\n",
    "iso_map_d1=Isomap(n_components=chosen_n_iso_d1,n_neighbors=choosen_n_iso_d1_neighbors,n_jobs=-1)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_ISO=iso_map_d1.fit_transform(DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "dataset_name='FMNIST ISO'\n",
    "dataset=dataset_transformed_ISO\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "\n",
    "experiment_em_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters\n",
    "dataset=dataset_transformed_ISO\n",
    "\n",
    "#Selecting N based on Plots\n",
    "best_clusters_ISO_d1=14\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "repeats=5\n",
    "\n",
    "print(\"ISOMAP RESULTS \\n\\n\")\n",
    "\n",
    "for i in range(repeats):\n",
    "    best_em_iso_d1=GaussianMixture(n_components=best_clusters_ISO_d1,random_state=21*i)\n",
    "\n",
    "    best_em_iso_d1.fit(dataset)\n",
    "    mi_score+=normalized_mutual_info_score(LABELS,best_em_iso_d1.predict(dataset))/repeats\n",
    "    aj_score+=adjusted_rand_score(LABELS,best_em_iso_d1.predict(dataset))/repeats\n",
    "\n",
    "\n",
    "#Evaluation Normalized MI Score\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "#Evaluating Adjusted Rand Score Rand Score\n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(dataset_transformed_ISO)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "plt.suptitle('Visualizations EM Clustering Results with Labels (ISOMAP FMINST)',fontsize=18)\n",
    "LABS=[LABELS,best_em_iso_d1.predict(dataset_transformed_ISO)]\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x,y=dataset_transformed[LABELS==lab][:,0].mean(),dataset_transformed[LABELS==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(DATASET[LABELS==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K MODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "dataset_name='FMNIST ISO'\n",
    "dataset=dataset_transformed_ISO\n",
    "labels_og=LABELS\n",
    "algorithm_name='K-MODES'\n",
    "categorical_cols=None\n",
    "\n",
    "experiment_km_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name,categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols=None\n",
    "data_categorized=categorizer(dataset_transformed_ISO,categorical_cols)\n",
    "\n",
    "#Best Number of Clusters\n",
    "best_kmodes_ISO_d1=11\n",
    "dataset=data_categorized\n",
    "\n",
    "print(\"DATASET 1 ISO KMODES RESULTS \\n\\n\")\n",
    "\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "repeats=5\n",
    "\n",
    "for i in range(repeats):\n",
    "    km = KModes(n_clusters=best_kmodes_ISO_d1, init='Huang', n_init=3, verbose=0)\n",
    "\n",
    "    km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    mi_score+=normalized_mutual_info_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))/repeats\n",
    "    aj_score+=adjusted_rand_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))/repeats\n",
    "\n",
    "\n",
    "#Evaluation Normalized MI Score\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "#Evaluating Adjusted Rand Score Rand Score\n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Mean Cluster Image from the ideal number of clusters \n",
    "#Visualizing\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(dataset_transformed_ISO)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (ISOMAP FMINST)',fontsize=18)\n",
    "LABS=[LABELS,km.predict(dataset,categorical=dataset.shape[-1])]\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x,y=dataset_transformed[LABELS==lab][:,0].mean(),dataset_transformed[LABELS==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "\n",
    "    for l in np.unique(labels_):\n",
    "        # Choose color from a colormap\n",
    "        color = plt.cm.viridis(l / len(np.unique(labels_)))\n",
    "        ax.scatter(dataset_transformed[:, 0][labels_ == l], dataset_transformed[:, 1][labels_ == l],\n",
    "                   label=l, zorder=1, color=color)\n",
    "\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1)\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(X_fm_scaled[LABELS==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend()\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_umap_d1=11\n",
    "choosen_n_umap_d1_neighbors=45\n",
    "\n",
    "#Creating UMAP\n",
    "umap_map_d1=umap.UMAP(n_components=chosen_n_umap_d1,n_neighbors=choosen_n_umap_d1_neighbors,n_jobs=-1)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_UMAP=umap_map_d1.fit_transform(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "n_list=[2,3,5,7,9,10,15,25,35,45]\n",
    "dataset_name='FMNIST UMAP'\n",
    "dataset=dataset_transformed_UMAP\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "cluster_graphs=4\n",
    "repeats=3\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2,perplexity=45)\n",
    "dataset_2d=converter.fit_transform(dataset)\n",
    "\n",
    "#Average of 3 runs \n",
    "silhouette_scores=np.zeros(shape=len(n_list))\n",
    "wcss_scores=np.zeros(shape=len(n_list))\n",
    "BIC_scores=np.zeros(shape=len(n_list))\n",
    "AIC_scores=np.zeros(shape=len(n_list))\n",
    "EM_dict={}\n",
    "\n",
    "for r in [21*(1+i) for i in range(repeats)]:\n",
    "\n",
    "    #Will be calculated for each run for plots\n",
    "    all_labels=[]\n",
    "    \n",
    "    #Assigning Clusters\n",
    "    for i,n in enumerate(n_list):\n",
    "        #Creating Gaussian\n",
    "        em=GaussianMixture(n_components=n,random_state=r)\n",
    "\n",
    "        #Fitted labels dataset\n",
    "        em.fit(dataset)\n",
    "\n",
    "        #Predictions\n",
    "        labels=em.predict(dataset)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        #Calculating Mutal Info Score\n",
    "        silhouette_scores[i]=silhouette_score(dataset,labels)/repeats\n",
    "\n",
    "        #WCSS Score\n",
    "        wcss_scores[i]=calculate_wcss(dataset,labels,em.means_)/repeats\n",
    "\n",
    "        #BIC Scores\n",
    "        BIC_scores[i]=em.bic(dataset)/repeats\n",
    "\n",
    "        #AIC\n",
    "        AIC_scores[i]=em.aic(dataset)/repeats\n",
    "\n",
    "        #Saving EM models for final Run \n",
    "        EM_dict[n]=em\n",
    "\n",
    "    #Plotting 2d color coded with labels \n",
    "    fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "    fig.set_size_inches(35,5)\n",
    "\n",
    "    plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "\n",
    "        if c>-1:\n",
    "            #Predictions\n",
    "            labels=all_labels[c]\n",
    "\n",
    "            #Plotting\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('{} : COMPONENTS'.format(n_list[c]));\n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "        else:\n",
    "\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('Dataset');      \n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Silhouette_scores & Clusters')\n",
    "plt.plot(n_list,silhouette_scores)\n",
    "plt.scatter(n_list, silhouette_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Silhouette_scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('WCSS')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting BIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('BIC Score & Clusters')\n",
    "plt.plot(n_list,BIC_scores)\n",
    "plt.scatter(n_list, BIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "\n",
    "#Plotting AIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('AIC Score & Clusters')\n",
    "plt.plot(n_list,AIC_scores)\n",
    "plt.scatter(n_list, AIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('AIC Score')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K MODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "n_list=[2,3,5,7,10,15,25, 35, 45]\n",
    "dataset_name='FMNIST (UMAP)'\n",
    "dataset=dataset_transformed_UMAP\n",
    "labels_og=LABELS\n",
    "algorithm_name='K-MODES'\n",
    "cluster_graphs=4\n",
    "categorical_cols=None\n",
    "\n",
    "data_categorized=categorizer(dataset,categorical_cols)\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2)\n",
    "dataset_2d=converter.fit_transform(data_categorized)\n",
    "silhouette_scores=[]\n",
    "all_labels=[]\n",
    "wcss_scores=[]\n",
    "KM_dict={}\n",
    "\n",
    "#Assigning Clusters\n",
    "for n in tqdm(n_list):\n",
    "    #Creating KMODES\n",
    "    km = KModes(n_clusters=n, init='Huang', n_init=3, verbose=0)\n",
    "\n",
    "    #Fitted labels dataset\n",
    "    km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "    #Predictions\n",
    "    labels=km.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    all_labels.append(labels)\n",
    "    \n",
    "    #Calculating Mutal Info Score\n",
    "    silhouette_scores.append(silhouette_score(data_categorized,labels))\n",
    "\n",
    "    #WCSS Score\n",
    "    wcss_scores.append(calculate_wcss(data_categorized,labels,km.cluster_centroids_))\n",
    "\n",
    "    KM_dict[n]=km\n",
    "\n",
    "#Plotting 2d color coded with labels \n",
    "fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "fig.set_size_inches(35,5)\n",
    "\n",
    "plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c>-1:\n",
    "        #Predictions\n",
    "        labels=all_labels[c]\n",
    "\n",
    "        #Plotting\n",
    "        sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "        #ax.legend()\n",
    "        ax.set_ylabel('Y');\n",
    "        ax.set_xlabel('X');\n",
    "        ax.set_title('{} : COMPONENTS'.format(n_list[c]));\n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    else:\n",
    "\n",
    "        sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "        #ax.legend()\n",
    "        ax.set_ylabel('Y');\n",
    "        ax.set_xlabel('X');\n",
    "        ax.set_title('Dataset');      \n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('WCSS')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Silhouette_scores & Clusters')\n",
    "plt.plot(n_list,silhouette_scores)\n",
    "plt.scatter(n_list, silhouette_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Silhouette_scores')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Number of Clusters\n",
    "best_kmodes_UMAP_d1=15\n",
    "data_categorized=categorizer(dataset_transformed_UMAP,None)\n",
    "dataset=data_categorized\n",
    "\n",
    "#Evaluation\n",
    "\n",
    "#Creating Best KMODES\n",
    "km = KModes(n_clusters=best_kmodes_UMAP_d1, init='Huang', n_init=3, verbose=0)\n",
    "\n",
    "#Fitted labels dataset\n",
    "km.fit(dataset,categorical=np.arange(dataset.shape[-1]))\n",
    "\n",
    "print(\"DATASET 1 KMODES RESULTS \\n\\n\")\n",
    "\n",
    "#Evaluation Normalized MI Score\n",
    "mi_score=normalized_mutual_info_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))\n",
    "print(f'The Normalized Mutual Info Score for the best clustering is {mi_score}') \n",
    "\n",
    "#Evaluating Adjusted Rand Score Rand Score\n",
    "aj_score=adjusted_rand_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))\n",
    "print(f'The Adjusted Rand Score for the best clustering is {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Mean Cluster Image from the ideal number of clusters \n",
    "#Visualizing Results from KFOLDS\n",
    "tsne_vis=TSNE(n_components=2,perplexity=45,n_jobs=-1)\n",
    "\n",
    "#Transforming dataset 1 based on ideal \n",
    "dataset_transformed=tsne_vis.fit_transform(DATASET)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig, axes = plt.subplots(1,2,figsize=(25, 5))\n",
    "\n",
    "position_dict={}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (UMAP FMINST)',fontsize=18)\n",
    "LABS=[LABELS,km.predict(dataset,categorical=dataset.shape[-1])]\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x,y=dataset_transformed[LABELS==lab][:,0].mean(),dataset_transformed[LABELS==lab][:,1].mean()\n",
    "    position_dict[lab]=[x,y]\n",
    "\n",
    "axis_int=0\n",
    "for ax,labels_ in zip(axes,LABS):\n",
    "\n",
    "    artists=[]\n",
    "    sc=ax.scatter(dataset_transformed[:,0],dataset_transformed[:,1],c=labels_,cmap='viridis',zorder=1,label=np.unique(labels_))\n",
    "    # Overlay each image within the scatter plot\n",
    "\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Positioning Images\n",
    "        image_x = position_dict[lab][0]\n",
    "        image_y = position_dict[lab][1]\n",
    "        im=OffsetImage(X_fm_scaled[LABELS==lab][0].reshape(28,28),cmap='gray')\n",
    "        ab = AnnotationBbox(im, (image_x, image_y), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int==0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int+=1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "    ax.legend(*sc.legend_elements(), title='Label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(25, 5))\n",
    "\n",
    "position_dict = {}\n",
    "\n",
    "plt.suptitle('Visualizations K-MODES Clustering Results with Labels (UMAP FMINST)', fontsize=18)\n",
    "LABS = [LABELS, km.predict(dataset, categorical=dataset.shape[-1])]\n",
    "\n",
    "for lab in np.unique(LABELS):\n",
    "    x, y = dataset_transformed[labels == lab][:, 0].mean(), dataset_transformed[labels == lab][:, 1].mean()\n",
    "    position_dict[lab] = [x, y]\n",
    "\n",
    "axis_int = 0\n",
    "for ax, labels_ in zip(axes, LABS):\n",
    "    artists = []\n",
    "    for lab in np.unique(LABELS):\n",
    "        # Plot points\n",
    "        points = dataset_transformed[labels_ == lab]\n",
    "        sc = ax.scatter(points[:, 0], points[:, 1], label=lab, cmap='viridis')\n",
    "\n",
    "        # Overlay images\n",
    "        for i, (x, y) in enumerate(points):\n",
    "            im = OffsetImage(X_fm_scaled[labels_ == lab][i].reshape(28, 28), cmap='gray')\n",
    "            ab = AnnotationBbox(im, (x, y), xycoords='data', frameon=False)\n",
    "            artists.append(ax.add_artist(ab))\n",
    "\n",
    "    if axis_int == 0:\n",
    "        ax.set_title(f'Actual')\n",
    "        axis_int += 1\n",
    "    else:\n",
    "        ax.set_title(f'Clustered')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(*sc.legend_elements(), title='Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

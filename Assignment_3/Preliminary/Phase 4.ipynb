{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import operator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, fbeta_score,make_scorer, mutual_info_score, silhouette_score, normalized_mutual_info_score, classification_report, confusion_matrix,f1_score, mean_squared_error, adjusted_rand_score\n",
    "import time \n",
    "from sklearn.model_selection import LearningCurveDisplay\n",
    "import tensorflow_addons as tfa\n",
    "from NNnet_class import NNnet\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA,FastICA\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim,SparseRandomProjection,GaussianRandomProjection\n",
    "from sklearn.manifold import TSNE,trustworthiness,Isomap\n",
    "from A3_utils import calculate_wcss,plot_gallery, categorizer\n",
    "from kmodes.kmodes import KModes\n",
    "import umap\n",
    "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
    "                                  TextArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "# Ignore SparseEfficiencyWarning\n",
    "warnings.simplefilter('ignore', category=SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeting seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Making accuracy scorrer\n",
    "accuracy_scorer=make_scorer(accuracy_score)\n",
    "\n",
    "#Global Beta (Used for Ftwo score - not used in assignment)\n",
    "BETA=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_curve(data,param,param_vals,ax,algorithim_name,algorithim,metric=accuracy_scorer,metric_name='Accuracy',graph=True,beta=BETA,cv=3):\n",
    "\n",
    "    '''Function to create parameter curves\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of np.arrays containing the features and labels  \n",
    "    param (str): Parameter to vary\n",
    "    param_vals (list): Parameter values\n",
    "    ax (matplotlib.axes): Axis for graph\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim to vary parameter in\n",
    "    metric (sklearn.metric): Metric to score algorotihim on\n",
    "    metric_name (str): Metric Name\n",
    "    graph (bool): Bool for validation graph\n",
    "    beta (int): Beta value for the Ftwo scored \n",
    "    cv (int): The number of cross validations folds\n",
    "    \n",
    "    Returns:\n",
    "    None'''\n",
    "\n",
    "\n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "    \n",
    "    for i in param_vals:\n",
    "        #Crossvalidating each parameter value\n",
    "        kf=KFold(n_splits=cv,shuffle=True)\n",
    "\n",
    "        internal_train_accuracy=0\n",
    "        internal_test_accuracy=0\n",
    "        \n",
    "        for train,test in kf.split(X=data[0]):\n",
    "            if i!='Default':\n",
    "                if 'net' in algorithim_name.lower():\n",
    "                    kwargs={param:i,'input_dims':data[0].shape[-1]}\n",
    "                    clf=algorithim(**kwargs)\n",
    "                else:\n",
    "                    clf=algorithim(**{param:i})\n",
    "            else:\n",
    "                clf=algorithim()\n",
    "\n",
    "            clf.fit(data[0][train],data[1][train])\n",
    "            internal_train_accuracy+=metric(y_pred=clf.predict(data[0][train]),y_true=data[1][train])\n",
    "            internal_test_accuracy+=metric(y_pred=clf.predict(data[0][test]),y_true=data[1][test])\n",
    "\n",
    "        train_acc.append(internal_train_accuracy/cv)\n",
    "        test_acc.append(internal_test_accuracy/cv)\n",
    "\n",
    "    best_val=param_vals[np.argmax(test_acc)]\n",
    "\n",
    "    if type(param_vals[0])==str:\n",
    "        ax.scatter(param_vals,train_acc,label='Training {}'.format(metric_name))\n",
    "        ax.scatter(param_vals,test_acc,label='Validation {}'.format(metric_name))\n",
    "    else:\n",
    "        ax.plot(param_vals,train_acc,label='Training {}'.format(metric_name))\n",
    "        ax.plot(param_vals,test_acc,label='Validation {}'.format(metric_name))\n",
    "    #plt.xscale('log')\n",
    "    ax.axvline(best_val,label='Best {} Value'.format(param),color='red',linestyle = '--')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('{}'.format(metric_name));\n",
    "    ax.set_xlabel(param);\n",
    "    ax.set_title('{} vs {}'.format(param,metric_name));\n",
    "    if type(best_val)!=str:\n",
    "        if best_val==max(param_vals):\n",
    "            ax.text(y=(max(test_acc)+min(test_acc))/2+np.abs(np.std(test_acc)),x=best_val-np.std(param_vals)/12,s=best_val,color='green',weight='bold')\n",
    "        else:\n",
    "            ax.text(y=(max(test_acc)+min(test_acc))/2+np.abs(np.std(test_acc)),x=best_val+np.std(param_vals)/12,s=best_val,color='green',weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_algorithim(data,param_dicts,dataset,algorithim_name,algorithim,metric=accuracy_score,metric_name='Accuracy',cv=3):\n",
    "    \n",
    "    '''Function to deal with algorithim\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of np.arrays containing the features and labels\n",
    "    param_dict (dict): Parameter dictionary to vary\n",
    "    dataset (str): Dataset name\n",
    "    algorithim_name (str): Algorithim name\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim to vary parameter in\n",
    "    metric (sklearn.metric): Metric to score algorotihim on\n",
    "    metric_name (str): Metric Name\n",
    "    cv (int): The number of cross validations folds \n",
    "\n",
    "    Returns:\n",
    "    None'''\n",
    "    \n",
    "    num_classes=len(param_dicts.keys())\n",
    "\n",
    "    #Getting Fig Size\n",
    "    fig,axes=plt.subplots(2,int(np.ceil(num_classes/2)))\n",
    "    fig.set_size_inches(15,15)\n",
    "    i=-1\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "        i+=1\n",
    "        plt.suptitle('Results for Algorithim: \"{}\" for \"{}\" Dataset'.format(algorithim_name,dataset),fontsize=18)\n",
    "        param=list(param_dicts.keys())[i]\n",
    "        param_vals=param_dicts[param]\n",
    "        create_param_curve(data,param,param_vals,ax,algorithim_name,algorithim,metric,metric_name)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Heart Disease Data\n",
    "def load_heart_data():\n",
    "\n",
    "    '''Load Heart Disease Dataset\n",
    "    \n",
    "    Returns:\n",
    "    X (np.array): X array\n",
    "    Y (np.array): Y array\n",
    "    col_index (dict): Dictionary containing the pairing for the column location and it's name'''\n",
    "\n",
    "    #PLEASE CHANGE TO LOCATION OF YOUR HEART DATA\n",
    "    df=pd.read_csv('Data/Heart_2/heart.csv')\n",
    "    Y=np.array(df['HeartDisease'])\n",
    "    df.drop('HeartDisease',axis=1,inplace=True)\n",
    "    \n",
    "    label_columns=['HeartDisease']\n",
    "    categorical_columns=['Sex', 'ChestPainType', 'RestingECG','ExerciseAngina','ST_Slope']\n",
    "\n",
    "    non_categorical_variables=list(set(df.columns).difference(set(categorical_columns+label_columns)))\n",
    "    X=np.array(df[non_categorical_variables])\n",
    "    columns_categorized=non_categorical_variables\n",
    "\n",
    "    #Now we need to one hot vectorize the type_of_meal_plan, room_type_reserved and market_segment_type\n",
    "    label_dict={}\n",
    "    for i in categorical_columns:\n",
    "        label_dict[i]=OneHotEncoder()\n",
    "        res=label_dict[i].fit_transform(np.array(df[i]).reshape(-1,1)).toarray()\n",
    "        X=np.c_[X,res]\n",
    "        columns_categorized=columns_categorized+[i+'%'+j for j in ['1','2','3','4','5','6','7'][:res.shape[-1]]]\n",
    "\n",
    "        col_index={}\n",
    "        results_corr={}\n",
    "        for label,col in zip(columns_categorized,range(X.shape[-1])):\n",
    "            corr=scipy.stats.pearsonr(X[:,col],Y)[0]\n",
    "            results_corr[label]=corr\n",
    "            col_index[label]=col\n",
    "    return X,Y,col_index\n",
    "\n",
    "#Fashion MNIST\n",
    "def load_fmnist():\n",
    "    #Loading dataset\n",
    "    data=pd.concat([pd.read_csv('Data/FMNIST/fashion-mnist_train.csv'),pd.read_csv('Data/FMNIST/fashion-mnist_test.csv')],axis=0)\n",
    "\n",
    "    X=data.iloc[:,1:].to_numpy()\n",
    "    Y=data.iloc[:,:1].to_numpy().ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1,random_state=42,stratify=Y)\n",
    "\n",
    "    return X_test,y_test\n",
    "\n",
    "\n",
    "#Load Pokemon Data\n",
    "def load_pokemon():\n",
    "    directory='Data/Pokemon/'\n",
    "    image_files = glob(os.path.join(directory, '**', '*.*'), recursive=True)\n",
    "\n",
    "    # Initialize a list to store the images as NumPy arrays\n",
    "    features=[]\n",
    "    labels=[]\n",
    "\n",
    "    # Iterate over the Image files\n",
    "    for file in image_files:\n",
    "\n",
    "        #Name of pokemon\n",
    "        label=file.split('\\\\')[1]\n",
    "\n",
    "        #Appending name\n",
    "        labels.append(label)\n",
    "\n",
    "        #Reading images\n",
    "        image = Image.open(file)\n",
    "\n",
    "        #Converting to RGB\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        #Removing color\n",
    "        image=image.convert('L')\n",
    "        \n",
    "        #Resizing images\n",
    "        image=image.resize((48, 48))\n",
    "\n",
    "        #Appending images\n",
    "        features.append(np.array(image).flatten())\n",
    "\n",
    "    #Converting to Numpy\n",
    "    labels=np.array(labels)\n",
    "    features=np.array(features)\n",
    "\n",
    "    pokemon_names=LabelEncoder()\n",
    "    labels_encoded=pokemon_names.fit_transform(labels)\n",
    "\n",
    "    return features,labels_encoded,pokemon_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_algorithim(X,Y,dataset_name,algorithim_name,algorithim,params,predictor_metric=accuracy_score,grid_search_metric=accuracy_scorer,predictor_metric_name='accuracy',return_needed=False):\n",
    "\n",
    "    '''Function to score algorithim\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.array): All features\n",
    "    Y (np.array): All labels\n",
    "    dataset_name (str): Dataset name\n",
    "    algorithim_name (str): Algorithim name\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim being experimented on\n",
    "    params (dict): Parameter dictionary for GridSearch\n",
    "    predictor_metric (sklearn.metric): Metric to score algorotihim on\n",
    "    grid_search_metric (sklearn.metric): Metric used by GridSearchCV\n",
    "    predictor_metric_name (str): Metric Name for predictor metric\n",
    "    return_needed (bool): Bool if fit model needed to be returned    \n",
    "\n",
    "    Returns:\n",
    "    glf (sklearn.model/NNnet): Fit model'''\n",
    "\n",
    "    standardize=False\n",
    "    if 'KNN' in algorithim_name.upper() or 'SVM' in algorithim_name.upper() or 'NET' in algorithim_name.upper():\n",
    "        standardize=True\n",
    "    \n",
    "    if 'NET' in algorithim_name.upper():\n",
    "        n_jobs=1\n",
    "    else:\n",
    "        n_jobs=-1\n",
    "\n",
    "\n",
    "    train,test=split_data(X,Y,valid=False,standardize=standardize)\n",
    "\n",
    "    glf_cv=GridSearchCV(algorithim,param_grid=params,verbose=0,n_jobs=n_jobs,cv=3,scoring=grid_search_metric,refit=True)\n",
    "\n",
    "    rep=10\n",
    "    start_time=time.time()\n",
    "    glf_cv.fit(train[0],train[1])\n",
    "\n",
    "    glf=glf_cv.best_estimator_\n",
    "    \n",
    "    for i in range(rep):\n",
    "        glf.fit(train[0],train[1])\n",
    "    time_delay_train=(time.time()-start_time)/rep\n",
    "\n",
    "    y_train_pred=glf.predict(train[0])\n",
    "    \n",
    "    if 'F' in predictor_metric_name.upper():\n",
    "        train_score = predictor_metric(y_pred=glf.predict(train[0]),y_true=train[1],beta=BETA)\n",
    "        test_score = predictor_metric(y_pred=glf.predict(test[0]),y_true=test[1],beta=BETA)\n",
    "    else:\n",
    "        train_score = predictor_metric(y_pred=glf.predict(train[0]),y_true=train[1])\n",
    "        test_score = predictor_metric(y_pred=glf.predict(test[0]),y_true=test[1])\n",
    "\n",
    "    start_time=time.time()\n",
    "    for i in range(rep):\n",
    "        y_test_pred=glf.predict(test[0])\n",
    "    time_delay_infer=(time.time()-start_time)/rep\n",
    "\n",
    "    print('{} Final Results'.format(dataset_name))\n",
    "    print('\\n')\n",
    "    print(glf_cv.best_params_)\n",
    "    print('\\n')\n",
    "    print('Average time to train the ideal {} was {:.3f} seconds'.format(algorithim_name,time_delay_train))\n",
    "    print('Average time to infer the ideal {} was {:.3f} seconds'.format(algorithim_name,time_delay_infer))\n",
    "    print('\\n')\n",
    "    print('The result on the training data for the ideal {} algorithim is a {} {} score'.format(algorithim_name,train_score,predictor_metric_name))\n",
    "    print('The result on the test data for the ideal {} algorithim is a {} {} score'.format(algorithim_name,test_score,predictor_metric_name))\n",
    "    \n",
    "    if return_needed:\n",
    "        return glf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,Y,valid=True,standardize=False):\n",
    "\n",
    "    '''\n",
    "    Split the data between train, test and optional validation dataset\n",
    "\n",
    "    Parameters:\n",
    "    X (np.array): X features\n",
    "    Y (np.rray): Labels\n",
    "    valid (bool): Split into validation dataset \n",
    "    standardize (bool): Whether to standardize the data (introduces bias as Sklearn Standard Scaler is trained only on the train data)\n",
    "\n",
    "    Returns:\n",
    "    train (list): np.array list of train\n",
    "    valid (list): optional np.array list of validation\n",
    "    test (list): np.array list of test\n",
    "    '''\n",
    "    \n",
    "    #Now let's split the data between test and train, we'll use the standard 80/20 split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=42)\n",
    "    \n",
    "    if valid:\n",
    "        #We'll also split the data between train and validation, we'll again use the standard 80/20 split\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2,random_state=42)\n",
    "        \n",
    "        if standardize:\n",
    "            sklr=StandardScaler()\n",
    "            X_train=sklr.fit_transform(X_train)\n",
    "            X_valid=sklr.transform(X_valid)\n",
    "            X_test=sklr.transform(X_test)\n",
    "        return [X_train,y_train],[X_valid,y_valid],[X_test,y_test]\n",
    "\n",
    "    if standardize:\n",
    "        sklr=StandardScaler()\n",
    "        X_train=sklr.fit_transform(X_train)\n",
    "        X_test=sklr.transform(X_test)\n",
    "    return [X_train,y_train],[X_test,y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pokemon\n",
    "X_poke,Y_poke,poke_encoder=load_pokemon()\n",
    "train_poke,test_poke=split_data(X_poke,Y_poke,valid=False)\n",
    "\n",
    "#Heart\n",
    "X_heart,Y_heart,heart_index=load_heart_data()\n",
    "train_heart,test_heart=split_data(X_heart,Y_heart,valid=False)\n",
    "\n",
    "#Fashion MNIST\n",
    "X_fm,Y_fm=load_fmnist()\n",
    "train_fm,test_fm=split_data(X_fm,Y_fm,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing\n",
    "sklr_poke=StandardScaler()\n",
    "sklr_heart=StandardScaler()\n",
    "sklr_fm=StandardScaler()\n",
    "\n",
    "X_poke_scaler=StandardScaler()\n",
    "X_heart_scaler=StandardScaler()\n",
    "X_fm_scaler=StandardScaler()\n",
    "\n",
    "#poke\n",
    "X_poke_scaled=X_poke_scaler.fit_transform(X_poke)\n",
    "\n",
    "train_poke_standardized=train_poke.copy()\n",
    "train_poke_unstandardized=test_poke.copy()\n",
    "\n",
    "test_poke_standardized=test_poke.copy()\n",
    "\n",
    "train_poke_standardized[0]=sklr_poke.fit_transform(train_poke[0])\n",
    "test_poke_standardized[0]=sklr_poke.transform(test_poke[0])\n",
    "\n",
    "#Heart\n",
    "X_heart_scaled=X_heart_scaler.fit_transform(X_heart)\n",
    "\n",
    "train_heart_standardized=train_heart.copy()\n",
    "\n",
    "test_heart_standardized=test_heart.copy()\n",
    "\n",
    "train_heart_standardized[0]=sklr_heart.fit_transform(train_heart[0])\n",
    "test_heart_standardized[0]=sklr_heart.transform(test_heart[0])\n",
    "\n",
    "#FMNIST\n",
    "X_fm_scaled=X_fm_scaler.fit_transform(X_fm)\n",
    "\n",
    "train_fm_standardized=train_fm.copy()\n",
    "train_fm_unstandardized=test_fm.copy()\n",
    "\n",
    "test_fm_standardized=test_fm.copy()\n",
    "\n",
    "train_fm_standardized[0]=sklr_fm.fit_transform(train_fm[0])\n",
    "test_fm_standardized[0]=sklr_fm.transform(test_fm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET=X_fm_scaled\n",
    "LABELS=Y_fm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING - CLUSTERS ADDED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTER LABELS ADDED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best EM based on findings from Phase 1\n",
    "best_clusters_d1_em=8\n",
    "best_em_d1=GaussianMixture(n_components=best_clusters_d1_em,random_state=21)\n",
    "\n",
    "#Creating the clusters and One Hot Encoding the data\n",
    "best_em_d1.fit(DATASET)\n",
    "clusters_em=best_em_d1.predict(DATASET)\n",
    "#clusters_em_norm=scaler.fit_transform(clusters_em.reshape(-1,1))\n",
    "\n",
    "encoder=OneHotEncoder()\n",
    "clusters_em_one_hot=encoder.fit_transform(clusters_em.reshape(-1,1)).toarray()\n",
    "\n",
    "#Adding to the features\n",
    "DATASET_CLUSTERED=DATASET.copy()\n",
    "DATASET_EM_CLUSTERED=np.c_[DATASET_CLUSTERED,clusters_em_one_hot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final FMINST Dataset Results For EM Cluster Labels Added\n",
    "\n",
    "param_dicts={'internal_layers':[2,4,8],'units':[50,100,150,200],'learning_rate':[0.0001],'batch_size':[32,64],'dropout_rate':[0,0.005,0.05]}\n",
    "\n",
    "score_algorithim(X=DATASET_EM_CLUSTERED,Y=LABELS,dataset_name='FMINST Dataset (EM CLUSTERS)',algorithim_name='Neural Network',algorithim=NNnet(input_dims=DATASET_EM_CLUSTERED[0].shape[-1]),params=param_dicts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best KM based on findings from Phase 1\n",
    "best_clusters_d1_km=15\n",
    "\n",
    "#Categorizing\n",
    "data_categorized=categorizer(DATASET,None)\n",
    "\n",
    "#Fitted labels dataset\n",
    "km = KModes(n_clusters=best_clusters_d1_km, init='Huang', n_init=3, verbose=0)\n",
    "\n",
    "#Creating Predicted Labels\n",
    "km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "clusters_km=km.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "#ONE HOT\n",
    "encoder=OneHotEncoder()\n",
    "clusters_km_one_hot=encoder.fit_transform(clusters_km.reshape(-1,1)).toarray()\n",
    "\n",
    "#Adding to the features\n",
    "DATASET_CLUSTERED=DATASET.copy()\n",
    "DATASET_KM_CLUSTERED=np.c_[DATASET_CLUSTERED,clusters_km_one_hot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final FMINST Dataset Results For KM Cluster Labels Added\n",
    "\n",
    "param_dicts={'internal_layers':[2,4,8],'units':[50,100,150,200],'learning_rate':[0.0001],'batch_size':[32,64],'dropout_rate':[0,0.005,0.05]}\n",
    "\n",
    "score_algorithim(X=DATASET_KM_CLUSTERED,Y=LABELS,dataset_name='FMINST Dataset (KM CLUSTERS)',algorithim_name='Neural Network',algorithim=NNnet(input_dims=DATASET_KM_CLUSTERED[0].shape[-1]),params=param_dicts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNED CLUSTER LOCATION & DISTANCE FROM CLUSTER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best EM based on findings from Phase 1\n",
    "best_clusters_d1_em=5\n",
    "best_em_d1=GaussianMixture(n_components=best_clusters_d1_em,random_state=21)\n",
    "\n",
    "#Creating the clusters and One Hot Encoding the data\n",
    "best_em_d1.fit(DATASET)\n",
    "clusters_em=best_em_d1.predict(DATASET)\n",
    "#clusters_em_norm=scaler.fit_transform(clusters_em.reshape(-1,1))\n",
    "\n",
    "#Adding the distances between each cluster to each data point\n",
    "distances=[]\n",
    "for i in range(DATASET.shape[0]):\n",
    "    distances.append(((DATASET[i,:]-best_em_d1.means_)**2).sum(axis=1))\n",
    "distances=np.array(distances)\n",
    "\n",
    "#scaling\n",
    "scaler=StandardScaler()\n",
    "distances=scaler.fit_transform(distances)\n",
    "\n",
    "DATASET_DISTANCES_CLUSTERED=np.c_[DATASET,distances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances=[]\n",
    "for i in range(DATASET.shape[0]):\n",
    "    distances.append(((DATASET[i,:]-best_em_d1.means_)**2).sum(axis=1))\n",
    "distances=np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final FMINST Dataset Results For EM Cluster Labels Added\n",
    "\n",
    "param_dicts={'internal_layers':[2,4,8],'units':[50,100,150,200],'learning_rate':[0.0001],'batch_size':[32,64],'dropout_rate':[0,0.005,0.05]}\n",
    "\n",
    "score_algorithim(X=DATASET_DISTANCES_CLUSTERED,Y=LABELS,dataset_name='FMINST Dataset (EM CLUSTERS)',algorithim_name='Neural Network',algorithim=NNnet(input_dims=DATASET_DISTANCES_CLUSTERED[0].shape[-1]),params=param_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

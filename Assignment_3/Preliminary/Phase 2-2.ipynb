{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import operator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, fbeta_score,make_scorer, mutual_info_score, silhouette_score, normalized_mutual_info_score, classification_report, confusion_matrix,f1_score, mean_squared_error, adjusted_rand_score\n",
    "import time \n",
    "from sklearn.model_selection import LearningCurveDisplay\n",
    "import tensorflow_addons as tfa\n",
    "from NNnet_class import NNnet\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA,FastICA\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim,SparseRandomProjection,GaussianRandomProjection\n",
    "from sklearn.manifold import TSNE,trustworthiness,Isomap\n",
    "from A3_utils import calculate_wcss,plot_gallery,categorizer,create_elbow_plot,experiment_em_clusters,experiment_km_clusters\n",
    "from kmodes.kmodes import KModes\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "import umap\n",
    "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
    "                                  TextArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "# Ignore SparseEfficiencyWarning\n",
    "warnings.simplefilter('ignore', category=SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeting seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Making accuracy scorrer\n",
    "accuracy_scorer=make_scorer(accuracy_score)\n",
    "\n",
    "#Global Beta (Used for Ftwo score - not used in assignment)\n",
    "BETA=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_curve(data,param,param_vals,ax,algorithim_name,algorithim,metric=accuracy_scorer,metric_name='Accuracy',graph=True,beta=BETA,cv=3):\n",
    "\n",
    "    '''Function to create parameter curves\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of np.arrays containing the features and labels  \n",
    "    param (str): Parameter to vary\n",
    "    param_vals (list): Parameter values\n",
    "    ax (matplotlib.axes): Axis for graph\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim to vary parameter in\n",
    "    metric (sklearn.metric): Metric to score algorotihim on\n",
    "    metric_name (str): Metric Name\n",
    "    graph (bool): Bool for validation graph\n",
    "    beta (int): Beta value for the Ftwo scored \n",
    "    cv (int): The number of cross validations folds\n",
    "    \n",
    "    Returns:\n",
    "    None'''\n",
    "\n",
    "\n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "    \n",
    "    for i in param_vals:\n",
    "        #Crossvalidating each parameter value\n",
    "        kf=KFold(n_splits=cv,shuffle=True)\n",
    "\n",
    "        internal_train_accuracy=0\n",
    "        internal_test_accuracy=0\n",
    "        \n",
    "        for train,test in kf.split(X=data[0]):\n",
    "            if i!='Default':\n",
    "                if 'net' in algorithim_name.lower():\n",
    "                    kwargs={param:i,'input_dims':data[0].shape[-1]}\n",
    "                    clf=algorithim(**kwargs)\n",
    "                else:\n",
    "                    clf=algorithim(**{param:i})\n",
    "            else:\n",
    "                clf=algorithim()\n",
    "\n",
    "            clf.fit(data[0][train],data[1][train])\n",
    "            internal_train_accuracy+=metric(y_pred=clf.predict(data[0][train]),y_true=data[1][train])\n",
    "            internal_test_accuracy+=metric(y_pred=clf.predict(data[0][test]),y_true=data[1][test])\n",
    "\n",
    "        train_acc.append(internal_train_accuracy/cv)\n",
    "        test_acc.append(internal_test_accuracy/cv)\n",
    "\n",
    "    best_val=param_vals[np.argmax(test_acc)]\n",
    "\n",
    "    if type(param_vals[0])==str:\n",
    "        ax.scatter(param_vals,train_acc,label='Training {}'.format(metric_name))\n",
    "        ax.scatter(param_vals,test_acc,label='Validation {}'.format(metric_name))\n",
    "    else:\n",
    "        ax.plot(param_vals,train_acc,label='Training {}'.format(metric_name))\n",
    "        ax.plot(param_vals,test_acc,label='Validation {}'.format(metric_name))\n",
    "    #plt.xscale('log')\n",
    "    ax.axvline(best_val,label='Best {} Value'.format(param),color='red',linestyle = '--')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('{}'.format(metric_name));\n",
    "    ax.set_xlabel(param);\n",
    "    ax.set_title('{} vs {}'.format(param,metric_name));\n",
    "    if type(best_val)!=str:\n",
    "        if best_val==max(param_vals):\n",
    "            ax.text(y=(max(test_acc)+min(test_acc))/2+np.abs(np.std(test_acc)),x=best_val-np.std(param_vals)/12,s=best_val,color='green',weight='bold')\n",
    "        else:\n",
    "            ax.text(y=(max(test_acc)+min(test_acc))/2+np.abs(np.std(test_acc)),x=best_val+np.std(param_vals)/12,s=best_val,color='green',weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_algorithim(data,param_dicts,dataset,algorithim_name,algorithim,metric=accuracy_score,metric_name='Accuracy',cv=3):\n",
    "    \n",
    "    '''Function to deal with algorithim\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of np.arrays containing the features and labels\n",
    "    param_dict (dict): Parameter dictionary to vary\n",
    "    dataset (str): Dataset name\n",
    "    algorithim_name (str): Algorithim name\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim to vary parameter in\n",
    "    metric (sklearn.metric): Metric to score algorotihim on\n",
    "    metric_name (str): Metric Name\n",
    "    cv (int): The number of cross validations folds \n",
    "\n",
    "    Returns:\n",
    "    None'''\n",
    "    \n",
    "    num_classes=len(param_dicts.keys())\n",
    "\n",
    "    #Getting Fig Size\n",
    "    fig,axes=plt.subplots(2,int(np.ceil(num_classes/2)))\n",
    "    fig.set_size_inches(15,15)\n",
    "    i=-1\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "        i+=1\n",
    "        plt.suptitle('Results for Algorithim: \"{}\" for \"{}\" Dataset'.format(algorithim_name,dataset),fontsize=18)\n",
    "        param=list(param_dicts.keys())[i]\n",
    "        param_vals=param_dicts[param]\n",
    "        create_param_curve(data,param,param_vals,ax,algorithim_name,algorithim,metric,metric_name)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Heart Disease Data\n",
    "def load_heart_data():\n",
    "\n",
    "    '''Load Heart Disease Dataset\n",
    "    \n",
    "    Returns:\n",
    "    X (np.array): X array\n",
    "    Y (np.array): Y array\n",
    "    col_index (dict): Dictionary containing the pairing for the column location and it's name'''\n",
    "\n",
    "    #PLEASE CHANGE TO LOCATION OF YOUR HEART DATA\n",
    "    df=pd.read_csv('Data/Heart_2/heart.csv')\n",
    "    Y=np.array(df['HeartDisease'])\n",
    "    df.drop('HeartDisease',axis=1,inplace=True)\n",
    "    \n",
    "    label_columns=['HeartDisease']\n",
    "    categorical_columns=['Sex', 'ChestPainType', 'RestingECG','ExerciseAngina','ST_Slope']\n",
    "\n",
    "    non_categorical_variables=list(set(df.columns).difference(set(categorical_columns+label_columns)))\n",
    "    X=np.array(df[non_categorical_variables])\n",
    "    columns_categorized=non_categorical_variables\n",
    "\n",
    "    #Now we need to one hot vectorize the type_of_meal_plan, room_type_reserved and market_segment_type\n",
    "    label_dict={}\n",
    "    for i in categorical_columns:\n",
    "        label_dict[i]=OneHotEncoder()\n",
    "        res=label_dict[i].fit_transform(np.array(df[i]).reshape(-1,1)).toarray()\n",
    "        X=np.c_[X,res]\n",
    "        columns_categorized=columns_categorized+[i+'%'+j for j in ['1','2','3','4','5','6','7'][:res.shape[-1]]]\n",
    "\n",
    "        col_index={}\n",
    "        results_corr={}\n",
    "        for label,col in zip(columns_categorized,range(X.shape[-1])):\n",
    "            corr=scipy.stats.pearsonr(X[:,col],Y)[0]\n",
    "            results_corr[label]=corr\n",
    "            col_index[label]=col\n",
    "    return X,Y,col_index\n",
    "\n",
    "#Fashion MNIST\n",
    "def load_fmnist():\n",
    "    #Loading dataset\n",
    "    data=pd.concat([pd.read_csv('Data/FMNIST/fashion-mnist_train.csv'),pd.read_csv('Data/FMNIST/fashion-mnist_test.csv')],axis=0)\n",
    "\n",
    "    X=data.iloc[:,1:].to_numpy()\n",
    "    Y=data.iloc[:,:1].to_numpy().ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1,random_state=42,stratify=Y)\n",
    "\n",
    "    return X_test,y_test\n",
    "\n",
    "\n",
    "#Load Pokemon Data\n",
    "def load_pokemon():\n",
    "    directory='Data/Pokemon/'\n",
    "    image_files = glob(os.path.join(directory, '**', '*.*'), recursive=True)\n",
    "\n",
    "    # Initialize a list to store the images as NumPy arrays\n",
    "    features=[]\n",
    "    labels=[]\n",
    "\n",
    "    # Iterate over the Image files\n",
    "    for file in image_files:\n",
    "\n",
    "        #Name of pokemon\n",
    "        label=file.split('\\\\')[1]\n",
    "\n",
    "        #Appending name\n",
    "        labels.append(label)\n",
    "\n",
    "        #Reading images\n",
    "        image = Image.open(file)\n",
    "\n",
    "        #Converting to RGB\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        #Removing color\n",
    "        image=image.convert('L')\n",
    "        \n",
    "        #Resizing images\n",
    "        image=image.resize((48, 48))\n",
    "\n",
    "        #Appending images\n",
    "        features.append(np.array(image).flatten())\n",
    "\n",
    "    #Converting to Numpy\n",
    "    labels=np.array(labels)\n",
    "    features=np.array(features)\n",
    "\n",
    "    pokemon_names=LabelEncoder()\n",
    "    labels_encoded=pokemon_names.fit_transform(labels)\n",
    "\n",
    "    return features,labels_encoded,pokemon_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_algorithim(X,Y,dataset_name,algorithim_name,algorithim,params,predictor_metric=accuracy_score,grid_search_metric=accuracy_scorer,predictor_metric_name='accuracy',return_needed=False):\n",
    "\n",
    "    '''Function to score algorithim\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.array): All features\n",
    "    Y (np.array): All labels\n",
    "    dataset_name (str): Dataset name\n",
    "    algorithim_name (str): Algorithim name\n",
    "    algorithim (sklearn.algorithim/NNnet): Algorithim being experimented on\n",
    "    params (dict): Parameter dictionary for GridSearch\n",
    "    predictor_metric (sklearn.metric): Metric to score algorotihim on\n",
    "    grid_search_metric (sklearn.metric): Metric used by GridSearchCV\n",
    "    predictor_metric_name (str): Metric Name for predictor metric\n",
    "    return_needed (bool): Bool if fit model needed to be returned    \n",
    "\n",
    "    Returns:\n",
    "    glf (sklearn.model/NNnet): Fit model'''\n",
    "\n",
    "    standardize=False\n",
    "    if 'KNN' in algorithim_name.upper() or 'SVM' in algorithim_name.upper() or 'NET' in algorithim_name.upper():\n",
    "        standardize=True\n",
    "    \n",
    "    if 'NET' in algorithim_name.upper():\n",
    "        n_jobs=1\n",
    "    else:\n",
    "        n_jobs=-1\n",
    "\n",
    "\n",
    "    train,test=split_data(X,Y,valid=False,standardize=standardize)\n",
    "\n",
    "    glf_cv=GridSearchCV(algorithim,param_grid=params,verbose=0,n_jobs=n_jobs,cv=3,scoring=grid_search_metric,refit=True)\n",
    "\n",
    "    rep=10\n",
    "    start_time=time.time()\n",
    "    glf_cv.fit(train[0],train[1])\n",
    "\n",
    "    glf=glf_cv.best_estimator_\n",
    "    \n",
    "    for i in range(rep):\n",
    "        glf.fit(train[0],train[1])\n",
    "    time_delay_train=(time.time()-start_time)/rep\n",
    "\n",
    "    y_train_pred=glf.predict(train[0])\n",
    "    \n",
    "    if 'F' in predictor_metric_name.upper():\n",
    "        train_score = predictor_metric(y_pred=glf.predict(train[0]),y_true=train[1],beta=BETA)\n",
    "        test_score = predictor_metric(y_pred=glf.predict(test[0]),y_true=test[1],beta=BETA)\n",
    "    else:\n",
    "        train_score = predictor_metric(y_pred=glf.predict(train[0]),y_true=train[1])\n",
    "        test_score = predictor_metric(y_pred=glf.predict(test[0]),y_true=test[1])\n",
    "\n",
    "    start_time=time.time()\n",
    "    for i in range(rep):\n",
    "        y_test_pred=glf.predict(test[0])\n",
    "    time_delay_infer=(time.time()-start_time)/rep\n",
    "\n",
    "    print('{} Final Results'.format(dataset_name))\n",
    "    print('\\n')\n",
    "    print(glf_cv.best_params_)\n",
    "    print('\\n')\n",
    "    print('Average time to train the ideal {} was {:.3f} seconds'.format(algorithim_name,time_delay_train))\n",
    "    print('Average time to infer the ideal {} was {:.3f} seconds'.format(algorithim_name,time_delay_infer))\n",
    "    print('\\n')\n",
    "    print('The result on the training data for the ideal {} algorithim is a {} {} score'.format(algorithim_name,train_score,predictor_metric_name))\n",
    "    print('The result on the test data for the ideal {} algorithim is a {} {} score'.format(algorithim_name,test_score,predictor_metric_name))\n",
    "    \n",
    "    if return_needed:\n",
    "        return glf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,Y,valid=True,standardize=False):\n",
    "\n",
    "    '''\n",
    "    Split the data between train, test and optional validation dataset\n",
    "\n",
    "    Parameters:\n",
    "    X (np.array): X features\n",
    "    Y (np.rray): Labels\n",
    "    valid (bool): Split into validation dataset \n",
    "    standardize (bool): Whether to standardize the data (introduces bias as Sklearn Standard Scaler is trained only on the train data)\n",
    "\n",
    "    Returns:\n",
    "    train (list): np.array list of train\n",
    "    valid (list): optional np.array list of validation\n",
    "    test (list): np.array list of test\n",
    "    '''\n",
    "    \n",
    "    #Now let's split the data between test and train, we'll use the standard 80/20 split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=42)\n",
    "    \n",
    "    if valid:\n",
    "        #We'll also split the data between train and validation, we'll again use the standard 80/20 split\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2,random_state=42)\n",
    "        \n",
    "        if standardize:\n",
    "            sklr=StandardScaler()\n",
    "            X_train=sklr.fit_transform(X_train)\n",
    "            X_valid=sklr.transform(X_valid)\n",
    "            X_test=sklr.transform(X_test)\n",
    "        return [X_train,y_train],[X_valid,y_valid],[X_test,y_test]\n",
    "\n",
    "    if standardize:\n",
    "        sklr=StandardScaler()\n",
    "        X_train=sklr.fit_transform(X_train)\n",
    "        X_test=sklr.transform(X_test)\n",
    "    return [X_train,y_train],[X_test,y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pokemon\n",
    "X_poke,Y_poke,poke_encoder=load_pokemon()\n",
    "train_poke,test_poke=split_data(X_poke,Y_poke,valid=False)\n",
    "\n",
    "#Heart\n",
    "X_heart,Y_heart,heart_index=load_heart_data()\n",
    "train_heart,test_heart=split_data(X_heart,Y_heart,valid=False)\n",
    "\n",
    "#Fashion MNIST\n",
    "X_fm,Y_fm=load_fmnist()\n",
    "train_fm,test_fm=split_data(X_fm,Y_fm,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing\n",
    "sklr_poke=StandardScaler()\n",
    "sklr_heart=StandardScaler()\n",
    "sklr_fm=StandardScaler()\n",
    "\n",
    "X_poke_scaler=StandardScaler()\n",
    "X_heart_scaler=StandardScaler()\n",
    "X_fm_scaler=StandardScaler()\n",
    "\n",
    "#poke\n",
    "X_poke_scaled=X_poke_scaler.fit_transform(X_poke)\n",
    "\n",
    "train_poke_standardized=train_poke.copy()\n",
    "train_poke_unstandardized=test_poke.copy()\n",
    "\n",
    "test_poke_standardized=test_poke.copy()\n",
    "\n",
    "train_poke_standardized[0]=sklr_poke.fit_transform(train_poke[0])\n",
    "test_poke_standardized[0]=sklr_poke.transform(test_poke[0])\n",
    "\n",
    "#Heart\n",
    "X_heart_scaled=X_heart_scaler.fit_transform(X_heart)\n",
    "\n",
    "train_heart_standardized=train_heart.copy()\n",
    "\n",
    "test_heart_standardized=test_heart.copy()\n",
    "\n",
    "train_heart_standardized[0]=sklr_heart.fit_transform(train_heart[0])\n",
    "test_heart_standardized[0]=sklr_heart.transform(test_heart[0])\n",
    "\n",
    "#FMNIST\n",
    "X_fm_scaled=X_fm_scaler.fit_transform(X_fm)\n",
    "\n",
    "train_fm_standardized=train_fm.copy()\n",
    "train_fm_unstandardized=test_fm.copy()\n",
    "\n",
    "test_fm_standardized=test_fm.copy()\n",
    "\n",
    "train_fm_standardized[0]=sklr_fm.fit_transform(train_fm[0])\n",
    "test_fm_standardized[0]=sklr_fm.transform(test_fm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET=X_heart_scaled\n",
    "LABELS=Y_heart"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering ALgorithms on Reduced Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEART"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_pca_d2=11\n",
    "\n",
    "#Creating PCA\n",
    "d2_pca=PCA(n_components=chosen_n_pca_d2)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_PCA=d2_pca.fit_transform(DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "dataset_name='HEART PCA'\n",
    "dataset=dataset_transformed_PCA\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "\n",
    " \n",
    "experiment_em_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters\n",
    "dataset=dataset_transformed_PCA\n",
    "\n",
    "#Selecting N based on Plots\n",
    "best_clusters_PCA_d2=15\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "repeats=10\n",
    "\n",
    "for i in range(repeats):\n",
    "    best_em_pca_d2=GaussianMixture(n_components=best_clusters_PCA_d2,random_state=21*i)\n",
    "    best_em_pca_d2.fit(dataset)\n",
    "\n",
    "    #Evaluation Normalized MI Score\n",
    "    mi_score+=normalized_mutual_info_score(LABELS,best_em_pca_d2.predict(dataset))/repeats\n",
    "\n",
    "    #Evaluating Adjusted Rand Score Rand Score\n",
    "    aj_score+=adjusted_rand_score(LABELS,best_em_pca_d2.predict(dataset))/repeats\n",
    "\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWO_FIXED=['Oldpeak','MaxHR']\n",
    "ind_1=heart_index[TWO_FIXED[0]]\n",
    "ind_2=heart_index[TWO_FIXED[1]]\n",
    "pred_labels=best_em_pca_d2.predict(dataset)\n",
    "ROWS=2\n",
    "\n",
    "\n",
    "filter_cols=[i for i in range(len(heart_index)) if i not in (ind_1,ind_2)]\n",
    "filtered_heart=X_heart_scaled[:,filter_cols]\n",
    "reversed_dict={i:k for k,i in heart_index.items()}\n",
    "\n",
    "fig,axes=plt.subplots(ROWS,4,subplot_kw={'projection': '3d'})\n",
    "fig.set_size_inches(ROWS*(len(reversed_dict)-4)/ROWS,5*ROWS)\n",
    "plt.suptitle(f'Visualizing EM Clustering Results By Features (PCA HEART)',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c%2==0:\n",
    "        #Plot Actual With Binary\n",
    "        for lab in np.unique(LABELS):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(LABELS)))\n",
    "            ax.scatter(dataset[:,ind_1][LABELS==lab],dataset[:,ind_2][LABELS==lab],filtered_heart[:,c][LABELS==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        ax.set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        ax.set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        ax.set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        ax.set_title(f'Actual Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "        ax.grid(False)\n",
    "\n",
    "        #Plot Predicted Cluster\n",
    "        for lab in np.unique(pred_labels):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(pred_labels)))\n",
    "            fig.axes[c+1].scatter(dataset[:,ind_1][pred_labels==lab],dataset[:,ind_2][pred_labels==lab],filtered_heart[:,c][pred_labels==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        fig.axes[c+1].set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_title(f'Predicted Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        fig.axes[c+1].legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        fig.axes[c+1].xaxis.pane.fill = False\n",
    "        fig.axes[c+1].yaxis.pane.fill = False\n",
    "        fig.axes[c+1].zaxis.pane.fill = False\n",
    "        fig.axes[c+1].xaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].yaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].zaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].set_xticks([])\n",
    "        fig.axes[c+1].set_yticks([])\n",
    "        fig.axes[c+1].set_zticks([])\n",
    "        fig.axes[c+1].grid(False)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K MODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "dataset_name='Heart PCA KMODES'\n",
    "dataset=dataset_transformed_PCA\n",
    "labels_og=LABELS\n",
    "algorithm_name='K-Modes'\n",
    "categorical_cols=None\n",
    "\n",
    "experiment_km_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name,categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters\n",
    "data_categorized=categorizer(dataset_transformed_PCA,categorical_cols)\n",
    "dataset=data_categorized\n",
    "categorical_cols=None\n",
    "\n",
    "#Selecting N based on Plots\n",
    "best_clusters_KMODES_PCA_d2=10\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "repeats=5\n",
    "\n",
    "print(\"DATASET 2 PCA KMODES RESULTS \\n\\n\")\n",
    "\n",
    "for i in range(repeats):\n",
    "    km=KModes(n_clusters=best_clusters_KMODES_PCA_d2, init='Huang', n_init=3, verbose=0)\n",
    "    km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    mi_score+=normalized_mutual_info_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))/repeats\n",
    "    aj_score+=adjusted_rand_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))/repeats\n",
    "\n",
    "\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWO_FIXED=['Oldpeak','MaxHR']\n",
    "ind_1=heart_index[TWO_FIXED[0]]\n",
    "ind_2=heart_index[TWO_FIXED[1]]\n",
    "pred_labels=km.predict(dataset,categorical=np.arange(dataset.shape[-1]))\n",
    "\n",
    "\n",
    "filter_cols=[i for i in range(len(heart_index)) if i not in (ind_1,ind_2)]\n",
    "filtered_heart=X_heart_scaled[:,filter_cols]\n",
    "reversed_dict={i:k for k,i in heart_index.items()}\n",
    "\n",
    "fig,axes=plt.subplots(ROWS,4,subplot_kw={'projection': '3d'})\n",
    "fig.set_size_inches(ROWS*(len(reversed_dict)-4)/ROWS,5*ROWS)\n",
    "plt.suptitle(f'Visualizing K-Modes Clustering Results By Features (PCA HEART)',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c%2==0:\n",
    "        #Plot Actual With Binary\n",
    "        for lab in np.unique(LABELS):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(LABELS)))\n",
    "            ax.scatter(dataset[:,ind_1][LABELS==lab],dataset[:,ind_2][LABELS==lab],filtered_heart[:,c][LABELS==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        ax.set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        ax.set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        ax.set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        ax.set_title(f'Actual Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "        ax.grid(False)\n",
    "\n",
    "        #Plot Predicted Cluster\n",
    "        for lab in np.unique(pred_labels):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(pred_labels)))\n",
    "            fig.axes[c+1].scatter(dataset[:,ind_1][pred_labels==lab],dataset[:,ind_2][pred_labels==lab],filtered_heart[:,c][pred_labels==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        fig.axes[c+1].set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_title(f'Predicted Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        fig.axes[c+1].legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        fig.axes[c+1].xaxis.pane.fill = False\n",
    "        fig.axes[c+1].yaxis.pane.fill = False\n",
    "        fig.axes[c+1].zaxis.pane.fill = False\n",
    "        fig.axes[c+1].xaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].yaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].zaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].set_xticks([])\n",
    "        fig.axes[c+1].set_yticks([])\n",
    "        fig.axes[c+1].set_zticks([])\n",
    "        fig.axes[c+1].grid(False)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_ica_d2=14\n",
    "\n",
    "#Creating ICA\n",
    "d2_ica=FastICA(n_components=chosen_n_ica_d2,random_state=42,max_iter=100000)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_ICA=d2_ica.fit_transform(DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "dataset_name='HEART ICA'\n",
    "dataset=dataset_transformed_ICA\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "\n",
    "experiment_em_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters\n",
    "\n",
    "dataset=dataset_transformed_ICA\n",
    "\n",
    "#Selecting N based on Plots\n",
    "best_clusters_ICA_d2=9\n",
    "best_em_ica_d2=GaussianMixture(n_components=best_clusters_ICA_d2,random_state=42)\n",
    "\n",
    "for i in range(5):\n",
    "    best_em_ica_d2.fit(dataset)\n",
    "    #best_em_d1=EM_dict[best_clusters_PCA_d1]\n",
    "\n",
    "    print(\"ICA RESULTS \\n\\n\")\n",
    "\n",
    "    #Evaluation Normalized MI Score\n",
    "    mi_score=normalized_mutual_info_score(LABELS,best_em_ica_d2.predict(dataset))\n",
    "    print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "    #Evaluating Adjusted Rand Score Rand Score\n",
    "    aj_score=adjusted_rand_score(LABELS,best_em_ica_d2.predict(dataset))\n",
    "    print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the best number of cluster and plotting pair plots\n",
    "#Getting labels for best cluster\n",
    "labels=best_em_ica_d2.predict(dataset)\n",
    "\n",
    "ROWS=5\n",
    "#Plotting with actual labels\n",
    "fig,axes=plt.subplots(ROWS,len(heart_index)//ROWS,subplot_kw={'projection': '3d'})\n",
    "\n",
    "fig.set_size_inches(5*len(heart_index)/ROWS,5*ROWS,)\n",
    "\n",
    "plt.suptitle(f'Visualizing Best Clusters By Feature (ICA HEART)',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c%2==0:\n",
    "\n",
    "        #Plot Actual With Binary\n",
    "        sc=ax.scatter(X_heart_scaled[:,0],X_heart_scaled[:,2],X_heart_scaled[:,c],c=Y_heart,cmap='viridis',alpha=0.75,s=10)\n",
    "        ax.set_xlabel(f'{list(heart_index.keys())[0]}',labelpad=-13)\n",
    "        ax.set_ylabel(f'{list(heart_index.keys())[2]}',labelpad=-13)\n",
    "        ax.set_zlabel(f'{list(heart_index.keys())[c]}',labelpad=-13)\n",
    "        ax.set_title(f'Actual Clusters for {list(heart_index.keys())[c]}')\n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "        \n",
    "        # Remove fill\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "        ax.grid(False)\n",
    "\n",
    "        #Plot Predicted Cluster\n",
    "        sc=fig.axes[c+1].scatter(X_heart_scaled[:,0],X_heart_scaled[:,2],X_heart_scaled[:,c],c=labels,cmap='viridis',alpha=0.75,s=10)\n",
    "        fig.axes[c+1].set_xlabel(f'{list(heart_index.keys())[0]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_ylabel(f'{list(heart_index.keys())[2]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_zlabel(f'{list(heart_index.keys())[c]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_title(f'Predicted Clusters for {list(heart_index.keys())[c]}')\n",
    "        fig.axes[c+1].legend(*sc.legend_elements(), title='clusters')\n",
    "        \n",
    "        # Remove fill\n",
    "        fig.axes[c+1].xaxis.pane.fill = False\n",
    "        fig.axes[c+1].yaxis.pane.fill = False\n",
    "        fig.axes[c+1].zaxis.pane.fill = False\n",
    "        fig.axes[c+1].xaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].yaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].zaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].set_xticks([])\n",
    "        fig.axes[c+1].set_yticks([])\n",
    "        fig.axes[c+1].set_zticks([])\n",
    "        fig.axes[c+1].grid(False)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K MODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "dataset_name='Heart ICA KMODES'\n",
    "dataset=dataset_transformed_ICA\n",
    "labels_og=LABELS\n",
    "algorithm_name='K-Modes'\n",
    "\n",
    "experiment_km_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name,categorical_cols=categorical_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_umap_d2=5\n",
    "chosen_neighbors_umap_d2=45\n",
    "\n",
    "#Creating UMAP\n",
    "umap_d2=umap.UMAP(n_components=chosen_n_umap_d2,n_neighbors=chosen_neighbors_umap_d2,n_jobs=-1)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_UMAP=umap_d2.fit_transform(DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "n_list=[2,3,5,7,9,10,15,25,35,45,55,65,75]\n",
    "dataset_name='HEART UMAP'\n",
    "dataset=dataset_transformed_UMAP\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "cluster_graphs=4\n",
    "repeats=3\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2,perplexity=45)\n",
    "dataset_2d=converter.fit_transform(dataset)\n",
    "\n",
    "#Average of 3 runs \n",
    "silhouette_scores=np.zeros(shape=len(n_list))\n",
    "wcss_scores=np.zeros(shape=len(n_list))\n",
    "BIC_scores=np.zeros(shape=len(n_list))\n",
    "AIC_scores=np.zeros(shape=len(n_list))\n",
    "EM_dict={}\n",
    "\n",
    "for r in [21*(1+i) for i in range(repeats)]:\n",
    "\n",
    "    #Will be calculated for each run for plots\n",
    "    all_labels=[]\n",
    "    \n",
    "    #Assigning Clusters\n",
    "    for i,n in enumerate(n_list):\n",
    "        #Creating Gaussian\n",
    "        em=GaussianMixture(n_components=n,random_state=r)\n",
    "\n",
    "        #Fitted labels dataset\n",
    "        em.fit(dataset)\n",
    "\n",
    "        #Predictions\n",
    "        labels=em.predict(dataset)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        #Calculating Mutal Info Score\n",
    "        silhouette_scores[i]=silhouette_score(dataset,labels)/repeats\n",
    "\n",
    "        #WCSS Score\n",
    "        wcss_scores[i]=calculate_wcss(dataset,labels,em.means_)/repeats\n",
    "\n",
    "        #BIC Scores\n",
    "        BIC_scores[i]=em.bic(dataset)/repeats\n",
    "\n",
    "        #AIC\n",
    "        AIC_scores[i]=em.aic(dataset)/repeats\n",
    "\n",
    "        #Saving EM models for final Run \n",
    "        EM_dict[n]=em\n",
    "\n",
    "    #Plotting 2d color coded with labels \n",
    "    fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "    fig.set_size_inches(35,5)\n",
    "\n",
    "    plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "    for c,ax in enumerate(fig.axes):\n",
    "\n",
    "        if c>-1:\n",
    "            #Predictions\n",
    "            labels=all_labels[c]\n",
    "\n",
    "            #Plotting\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('{} : COMPONENTS'.format(n_list[c]));\n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "        else:\n",
    "\n",
    "            sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "            #ax.legend()\n",
    "            ax.set_ylabel('Y');\n",
    "            ax.set_xlabel('X');\n",
    "            ax.set_title('Dataset');      \n",
    "            ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Silhouette_scores & Clusters')\n",
    "plt.plot(n_list,silhouette_scores)\n",
    "plt.scatter(n_list, silhouette_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Silhouette_scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('WCSS')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting BIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('BIC Score & Clusters')\n",
    "plt.plot(n_list,BIC_scores)\n",
    "plt.scatter(n_list, BIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "\n",
    "#Plotting AIC\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('AIC Score & Clusters')\n",
    "plt.plot(n_list,AIC_scores)\n",
    "plt.scatter(n_list, AIC_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('AIC Score')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K MODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "n_list=[2,3,5,7,10,15,25,35,45,55]\n",
    "dataset_name='Heart UMAP KMODES'\n",
    "dataset=dataset_transformed_UMAP\n",
    "labels_og=LABELS\n",
    "algorithm_name='K-Modes'\n",
    "cluster_graphs=4\n",
    "#categorical=[i for i,k in enumerate(heart_index.keys()) if '%' in k]\n",
    "\n",
    "#Categorizing Data\n",
    "data_categorized=categorizer(dataset,None)\n",
    "\n",
    "#Converting dataset to 2D so it's plottable\n",
    "converter=TSNE(n_components=2)\n",
    "dataset_2d=converter.fit_transform(data_categorized)\n",
    "silhouette_scores=[]\n",
    "all_labels=[]\n",
    "wcss_scores=[]\n",
    "KM_cost=[]\n",
    "KM_dict={}\n",
    "\n",
    "#Assigning Clusters\n",
    "for n in n_list:\n",
    "    #Creating Gaussian\n",
    "    km = KModes(n_clusters=n, init='Huang', n_init=5, verbose=0)\n",
    "\n",
    "    #Fitted labels dataset\n",
    "    km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "    #Predictions\n",
    "    labels=km.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    all_labels.append(labels)\n",
    "    \n",
    "    #Calculating Mutal Info Score\n",
    "    silhouette_scores.append(silhouette_score(data_categorized,labels))\n",
    "\n",
    "    #WCSS Score\n",
    "    wcss_scores.append(calculate_wcss(data_categorized,labels,km.cluster_centroids_))\n",
    "\n",
    "    KM_dict[n]=km\n",
    "\n",
    "    KM_cost.append(km.cost_)\n",
    "\n",
    "#Plotting 2d color coded with labels \n",
    "fig,axes=plt.subplots(1,len(n_list[:cluster_graphs])+1)\n",
    "fig.set_size_inches(35,5)\n",
    "\n",
    "plt.suptitle(f'{dataset_name} dataset clusters using {algorithm_name}',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c>0:\n",
    "        #Predictions\n",
    "        labels=all_labels[c-1]\n",
    "\n",
    "        #Plotting\n",
    "        sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels,cmap='plasma')\n",
    "        #ax.legend()\n",
    "        ax.set_ylabel('Y');\n",
    "        ax.set_xlabel('X');\n",
    "        ax.set_title('{} : COMPONENTS'.format(n_list[c-1]));\n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "    else:\n",
    "\n",
    "        sc=ax.scatter(dataset_2d[:,0],dataset_2d[:,1],c=labels_og,cmap='plasma')\n",
    "        #ax.legend()\n",
    "        ax.set_ylabel('Y');\n",
    "        ax.set_xlabel('X');\n",
    "        ax.set_title('Dataset');      \n",
    "        ax.legend(*sc.legend_elements(), title='clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting WCSS\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('WCSS & Clusters')\n",
    "plt.plot(n_list,wcss_scores)\n",
    "plt.scatter(n_list, wcss_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('WCSS')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting Cost\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Cost & Clusters')\n",
    "plt.plot(n_list,KM_cost)\n",
    "plt.scatter(n_list, KM_cost, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Cost')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#Plotting Silhouette score\n",
    "fig=plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.title('Silhouette_scores & Clusters')\n",
    "plt.plot(n_list,silhouette_scores)\n",
    "plt.scatter(n_list, silhouette_scores, s=100, c='blue', marker='o', edgecolors='black')\n",
    "plt.xlabel('N_clusters')\n",
    "plt.ylabel('Silhouette_scores')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISOMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_iso_d2=12\n",
    "choosen_n_iso_d2_neighbors=45\n",
    "\n",
    "#Creating PCA\n",
    "iso_map_d2=Isomap(n_components=chosen_n_iso_d2,n_neighbors=choosen_n_iso_d2_neighbors,n_jobs=-1,p=1)\n",
    "\n",
    "#Transforming Data\n",
    "dataset_transformed_ISO=iso_map_d2.fit_transform(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing EM algorithm with different number of components\n",
    "dataset_name='HEART ISOMAP'\n",
    "dataset=dataset_transformed_ISO\n",
    "labels_og=LABELS\n",
    "algorithm_name='EM'\n",
    "\n",
    " \n",
    "experiment_em_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters\n",
    "dataset=dataset_transformed_ISO\n",
    "\n",
    "#Selecting N based on Plots\n",
    "best_clusters_ISO_d2=4\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "repeats=5\n",
    "\n",
    "print(\"ISOMAP RESULTS \\n\\n\")\n",
    "\n",
    "for i in range(repeats):\n",
    "    best_em_iso_d2=GaussianMixture(n_components=best_clusters_ISO_d2,random_state=21*i)\n",
    "\n",
    "    best_em_iso_d2.fit(dataset)\n",
    "    mi_score+=normalized_mutual_info_score(LABELS,best_em_iso_d2.predict(dataset))/repeats\n",
    "    aj_score+=adjusted_rand_score(LABELS,best_em_iso_d2.predict(dataset))/repeats\n",
    "\n",
    "\n",
    "#Evaluation Normalized MI Score\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "\n",
    "#Evaluating Adjusted Rand Score Rand Score\n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWO_FIXED=['Oldpeak','MaxHR']\n",
    "dataset=dataset_transformed_ISO\n",
    "ind_1=heart_index[TWO_FIXED[0]]\n",
    "ind_2=heart_index[TWO_FIXED[1]]\n",
    "pred_labels=best_em_iso_d2.predict(dataset)\n",
    "\n",
    "\n",
    "\n",
    "filter_cols=[i for i in range(len(heart_index)) if i not in (ind_1,ind_2)]\n",
    "filtered_heart=X_heart_scaled[:,filter_cols]\n",
    "reversed_dict={i:k for k,i in heart_index.items()}\n",
    "\n",
    "fig,axes=plt.subplots(ROWS,4,subplot_kw={'projection': '3d'})\n",
    "fig.set_size_inches(ROWS*(len(reversed_dict)-4)/ROWS,5*ROWS)\n",
    "plt.suptitle(f'Visualizing EM Clustering Results By Features (ISOMAP HEART)',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c%2==0:\n",
    "        #Plot Actual With Binary\n",
    "        for lab in np.unique(LABELS):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(LABELS)))\n",
    "            ax.scatter(dataset[:,ind_1][LABELS==lab],dataset[:,ind_2][LABELS==lab],filtered_heart[:,c][LABELS==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        ax.set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        ax.set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        ax.set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        ax.set_title(f'Actual Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "        ax.grid(False)\n",
    "\n",
    "        #Plot Predicted Cluster\n",
    "        for lab in np.unique(pred_labels):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(pred_labels)))\n",
    "            fig.axes[c+1].scatter(dataset[:,ind_1][pred_labels==lab],dataset[:,ind_2][pred_labels==lab],filtered_heart[:,c][pred_labels==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        fig.axes[c+1].set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_title(f'Predicted Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        fig.axes[c+1].legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        fig.axes[c+1].xaxis.pane.fill = False\n",
    "        fig.axes[c+1].yaxis.pane.fill = False\n",
    "        fig.axes[c+1].zaxis.pane.fill = False\n",
    "        fig.axes[c+1].xaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].yaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].zaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].set_xticks([])\n",
    "        fig.axes[c+1].set_yticks([])\n",
    "        fig.axes[c+1].set_zticks([])\n",
    "        fig.axes[c+1].grid(False)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing K_Modes algorithm with different number of components\n",
    "dataset_name='Heart ISOMAP KMODES'\n",
    "dataset=dataset_transformed_ISO\n",
    "labels_og=LABELS\n",
    "algorithm_name='K-Modes'\n",
    "categorical_cols=None\n",
    "\n",
    "experiment_km_clusters(dataset=dataset,dataset_name=dataset_name,labels_og=labels_og,algorithm_name=algorithm_name,categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Best Clusters\n",
    "categorical_cols=None\n",
    "data_categorized=categorizer(dataset_transformed_ISO,categorical_cols)\n",
    "dataset=data_categorized\n",
    "\n",
    "\n",
    "#Selecting N based on Plots\n",
    "best_clusters_KMODES_ISO_d2=7\n",
    "mi_score=0\n",
    "aj_score=0\n",
    "repeats=5\n",
    "\n",
    "print(\"DATASET 2 ISOMAP KMODES RESULTS \\n\\n\")\n",
    "\n",
    "for i in range(repeats):\n",
    "    km=KModes(n_clusters=best_clusters_KMODES_ISO_d2, init='Huang', n_init=3, verbose=0)\n",
    "    km.fit(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "    mi_score+=normalized_mutual_info_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))/repeats\n",
    "    aj_score+=adjusted_rand_score(LABELS,km.predict(dataset,categorical=np.arange(dataset.shape[-1])))/repeats\n",
    "\n",
    "\n",
    "print(f'The Normalized Mutual Info Score for the best clustering iS {mi_score}') \n",
    "print(f'The Adjusted Rand Score for the best clustering iS {aj_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWO_FIXED=['Oldpeak','MaxHR']\n",
    "dataset=data_categorized\n",
    "ind_1=heart_index[TWO_FIXED[0]]\n",
    "ind_2=heart_index[TWO_FIXED[1]]\n",
    "pred_labels=km.predict(data_categorized,categorical=np.arange(data_categorized.shape[-1]))\n",
    "\n",
    "\n",
    "filter_cols=[i for i in range(len(heart_index)) if i not in (ind_1,ind_2)]\n",
    "filtered_heart=X_heart_scaled[:,filter_cols]\n",
    "reversed_dict={i:k for k,i in heart_index.items()}\n",
    "\n",
    "fig,axes=plt.subplots(ROWS,4,subplot_kw={'projection': '3d'})\n",
    "fig.set_size_inches(ROWS*(len(reversed_dict)-4)/ROWS,5*ROWS)\n",
    "plt.suptitle(f'Visualizing K-MODES Clustering Results By Features (ISOMAP HEART)',fontsize=18)\n",
    "\n",
    "for c,ax in enumerate(fig.axes):\n",
    "\n",
    "    if c%2==0:\n",
    "        #Plot Actual With Binary\n",
    "        for lab in np.unique(LABELS):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(LABELS)))\n",
    "            ax.scatter(dataset[:,ind_1][LABELS==lab],dataset[:,ind_2][LABELS==lab],filtered_heart[:,c][LABELS==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        ax.set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        ax.set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        ax.set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        ax.set_title(f'Actual Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "        ax.grid(False)\n",
    "\n",
    "        #Plot Predicted Cluster\n",
    "        for lab in np.unique(pred_labels):\n",
    "            color = plt.cm.viridis(lab / len(np.unique(pred_labels)))\n",
    "            fig.axes[c+1].scatter(dataset[:,ind_1][pred_labels==lab],dataset[:,ind_2][pred_labels==lab],filtered_heart[:,c][pred_labels==lab],label=lab,zorder=1,color=color,alpha=0.75,s=10)\n",
    "        fig.axes[c+1].set_xlabel(f'{TWO_FIXED[0]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_ylabel(f'{TWO_FIXED[1]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_zlabel(f'{reversed_dict[filter_cols[c]]}',labelpad=-13)\n",
    "        fig.axes[c+1].set_title(f'Predicted Clusters for {list(heart_index.keys())[filter_cols[c]]}')\n",
    "        fig.axes[c+1].legend()\n",
    "        \n",
    "        # Remove fill\n",
    "        fig.axes[c+1].xaxis.pane.fill = False\n",
    "        fig.axes[c+1].yaxis.pane.fill = False\n",
    "        fig.axes[c+1].zaxis.pane.fill = False\n",
    "        fig.axes[c+1].xaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].yaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].zaxis.pane.set_edgecolor('w')\n",
    "        fig.axes[c+1].set_xticks([])\n",
    "        fig.axes[c+1].set_yticks([])\n",
    "        fig.axes[c+1].set_zticks([])\n",
    "        fig.axes[c+1].grid(False)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
